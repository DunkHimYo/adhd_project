{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb의 사본의 사본의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v2Z_0CIqrhM"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "control=[]\n",
        "adhd=[]\n",
        "for i in os.listdir('./drive/MyDrive/bp/ADHD/'):\n",
        "    b=[]\n",
        "    for j in os.listdir(f'./drive/MyDrive/bp/ADHD/{i}'):\n",
        "        b.append(pd.read_csv(f'./drive/MyDrive/bp/ADHD/{i}/'+j,index_col='Chan').loc[:,:'TotalAbsPow'].to_numpy())\n",
        "    adhd.append(b)\n",
        "    \n",
        "for i in os.listdir('./drive/MyDrive/bp/Control/'):\n",
        "    b=[]    \n",
        "    for j in os.listdir(f'./drive/MyDrive/bp/Control/{i}'):\n",
        "        b.append(pd.read_csv(f'./drive/MyDrive/bp/Control/{i}/'+j,index_col='Chan').loc[:,:'TotalAbsPow'].to_numpy())\n",
        "    control.append(b)\n",
        "    \n",
        "\n",
        "control= np.array(control).transpose(1,0,2,3)\n",
        "adhd=np.array(adhd).transpose(1,0,2,3)\n",
        "\n",
        "x=np.append(adhd,control,axis=0)\n",
        "y=np.array([1]*61+[0]*60)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNGrhl7tyjkZ"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "cc=[]\n",
        "for i in range(7):\n",
        "    cc.append(x[:,:,:,i].flatten())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnAvzkDcyKJ-"
      },
      "source": [
        "std=RobustScaler()\n",
        "x=std.fit_transform(np.array(cc).transpose()).reshape(121,19,19,7)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbREeoOqI4YY",
        "outputId": "0ff2811d-e874-40f5-bb8b-d06948c4c228"
      },
      "source": [
        "import joblib\n",
        "# 객체를 pickled binary file 형태로 저장한다\n",
        "file_name = 'scale.pkl'\n",
        "joblib.dump(std, file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scale.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex8uYJ5FHvvZ"
      },
      "source": [
        "def crop(dimension, start, end):\n",
        "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
        "    # example : to crop tensor x[:, :, 5:10]\n",
        "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
        "    def func(x):\n",
        "        if dimension == 0:\n",
        "            return x[start: end]\n",
        "        if dimension == 1:\n",
        "            return x[:, start: end]\n",
        "        if dimension == 2:\n",
        "            return x[:, :, start: end]\n",
        "        if dimension == 3:\n",
        "            return x[:, :, :, start: end]\n",
        "        if dimension == 4:\n",
        "            return x[:, :, :, :, start: end]\n",
        "    return Lambda(func)\n",
        "import math\n",
        "def slice_model(model_input,unit,row_num,col_num,term):\n",
        "  remain=math.ceil(unit/2)\n",
        "  return [crop(3,col_num-(j+unit),col_num-j)(crop(2,row_num-(i+unit),row_num-i)(model_input)) for i in range(0,row_num-unit+1,term) for j in range(0,col_num-unit+1,term)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlP81GCoqlnc"
      },
      "source": [
        "import sys\n",
        "import sys\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, GlobalMaxPooling3D,Lambda,concatenate,Conv3D, MaxPooling3D,GlobalAveragePooling3D\n",
        "from tensorflow.keras.regularizers import l1,l2\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "def mk_model(filepath=None):\n",
        "    \n",
        "    FILTER_SIZE=3\n",
        "    NUM_FILTERS=8\n",
        "    INPUT_SIZE=19\n",
        "    MAXPOOL_SIZE=2\n",
        "\n",
        "    densors=[]\n",
        "    model_input=Input(shape=(19,INPUT_SIZE,7,1))\n",
        "    for idx in slice_model(model_input,3,19,7,2):\n",
        "            \n",
        "            model_output=Conv3D(NUM_FILTERS, (FILTER_SIZE,FILTER_SIZE,FILTER_SIZE),activation='relu')(idx)\n",
        "            model_output=Dropout(0.5)(model_output)                        \n",
        "            model_output=GlobalAveragePooling3D()(model_output)\n",
        "            densors.append(model_output)\n",
        "\n",
        "    model_output=concatenate(densors)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "\n",
        "\n",
        "    model_output=Dense(units=1,activation='sigmoid',kernel_regularizer=l1(0.01))(model_output)\n",
        "    model = Model(inputs = model_input, outputs = model_output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_Wxk0KWV7Bq",
        "outputId": "dfee0660-3bd8-48f6-aa12-d88a37a65b4f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "x_tra,x_test,y_tra,y_test=train_test_split(x,y,train_size=0.8,stratify=y,random_state=128)\n",
        "\n",
        "kf=KFold(7,True )\n",
        "train_score=[]\n",
        "test_score=[]\n",
        "val_score=[]\n",
        "test_list=[]\n",
        "\n",
        "idx=0\n",
        "\n",
        "val_list=[]\n",
        "\n",
        "for train_index, test_index in kf.split(x_tra):\n",
        "    callback_list = [\n",
        "    EarlyStopping( #성능 향상이 멈추면 훈련을 중지\n",
        "    monitor='val_loss',  #모델 검증 정확도를 모니터링\n",
        "    patience=50        #1 에포크 보다 더 길게(즉, 2에포크 동안 정확도가 향상되지 않으면 훈련 중지\n",
        "),\n",
        "    ModelCheckpoint( #에포크마다 현재 가중치를 저장\n",
        "    filepath=f'./mod{idx}.h5', #모델 파일 경로\n",
        "    monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\n",
        "    save_best_only=True,\n",
        "    mode='auto',\n",
        "    verbose=1\n",
        ")\n",
        "]\n",
        "    x_train,x_val=x_tra[train_index],x_tra[test_index]\n",
        "    y_train,y_val=y_tra[train_index],y_tra[test_index]\n",
        "    \n",
        "    #with strategy.scope():\n",
        "    model=mk_model()\n",
        "    print(model.summary())\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    hist=model.fit(x_train, y_train, epochs=200, validation_data=(x_val, y_val),batch_size=36,callbacks=callback_list)\n",
        "    \n",
        "    \n",
        "    plt.plot(hist.history['loss'],label='train'+str(idx))\n",
        "    plt.plot(hist.history['val_loss'],label='train'+str(idx))\n",
        "    plt.title('loss',fontsize=15)\n",
        "    plt.legend(['train','val'])\n",
        "    plt.show()\n",
        "    plt.plot(hist.history['accuracy'],label='train'+str(idx))\n",
        "    plt.plot(hist.history['val_accuracy'],label='train'+str(idx))\n",
        "    plt.legend(['train','val'])\n",
        "    plt.title('acc',fontsize=15)\n",
        "    plt.show()\n",
        "    model=load_model(f'./mod{idx}.h5')\n",
        "    \n",
        "    train_score.append(model.evaluate(x_train,y_train))\n",
        "    test_score.append(model.evaluate(x_test,y_test))\n",
        "    val_score.append(model.evaluate(x_val,y_val))\n",
        "    idx+=1\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_21 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_23 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_27 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_29 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_31 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_33 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_35 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_37 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_39 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_41 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_43 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_45 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_47 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_49 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_51 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_53 (Lambda)              (None, 19, 3, 7, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 19, 3, 3, 1)  0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_20 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_22 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_24 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_26 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_28 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_30 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_32 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_34 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_36 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_38 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_40 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_42 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_44 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_46 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_48 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_50 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_52 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d (Conv3D)                 (None, 17, 1, 1, 8)  224         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_1 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_2 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_3 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_4 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_5 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_6 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_7 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_8 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_9 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_10 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_11 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_12 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_13 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_14 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_15 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_16 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_17 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_18 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_19 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_20 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_21 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_22 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_23 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_24 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_25 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_26 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 17, 1, 1, 8)  0           conv3d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d (Globa (None, 8)            0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_1 (Glo (None, 8)            0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_2 (Glo (None, 8)            0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_3 (Glo (None, 8)            0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_4 (Glo (None, 8)            0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_5 (Glo (None, 8)            0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_6 (Glo (None, 8)            0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_7 (Glo (None, 8)            0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_8 (Glo (None, 8)            0           dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_9 (Glo (None, 8)            0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_10 (Gl (None, 8)            0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_11 (Gl (None, 8)            0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_12 (Gl (None, 8)            0           dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_13 (Gl (None, 8)            0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_14 (Gl (None, 8)            0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_15 (Gl (None, 8)            0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_16 (Gl (None, 8)            0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_17 (Gl (None, 8)            0           dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_18 (Gl (None, 8)            0           dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_19 (Gl (None, 8)            0           dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_20 (Gl (None, 8)            0           dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_21 (Gl (None, 8)            0           dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_22 (Gl (None, 8)            0           dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_23 (Gl (None, 8)            0           dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_24 (Gl (None, 8)            0           dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_25 (Gl (None, 8)            0           dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_26 (Gl (None, 8)            0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 216)          0           global_average_pooling3d[0][0]   \n",
            "                                                                 global_average_pooling3d_1[0][0] \n",
            "                                                                 global_average_pooling3d_2[0][0] \n",
            "                                                                 global_average_pooling3d_3[0][0] \n",
            "                                                                 global_average_pooling3d_4[0][0] \n",
            "                                                                 global_average_pooling3d_5[0][0] \n",
            "                                                                 global_average_pooling3d_6[0][0] \n",
            "                                                                 global_average_pooling3d_7[0][0] \n",
            "                                                                 global_average_pooling3d_8[0][0] \n",
            "                                                                 global_average_pooling3d_9[0][0] \n",
            "                                                                 global_average_pooling3d_10[0][0]\n",
            "                                                                 global_average_pooling3d_11[0][0]\n",
            "                                                                 global_average_pooling3d_12[0][0]\n",
            "                                                                 global_average_pooling3d_13[0][0]\n",
            "                                                                 global_average_pooling3d_14[0][0]\n",
            "                                                                 global_average_pooling3d_15[0][0]\n",
            "                                                                 global_average_pooling3d_16[0][0]\n",
            "                                                                 global_average_pooling3d_17[0][0]\n",
            "                                                                 global_average_pooling3d_18[0][0]\n",
            "                                                                 global_average_pooling3d_19[0][0]\n",
            "                                                                 global_average_pooling3d_20[0][0]\n",
            "                                                                 global_average_pooling3d_21[0][0]\n",
            "                                                                 global_average_pooling3d_22[0][0]\n",
            "                                                                 global_average_pooling3d_23[0][0]\n",
            "                                                                 global_average_pooling3d_24[0][0]\n",
            "                                                                 global_average_pooling3d_25[0][0]\n",
            "                                                                 global_average_pooling3d_26[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          111104      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          262656      dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          262656      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            513         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 376ms/step - loss: 0.9778 - accuracy: 0.5000 - val_loss: 0.8123 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.81229, saving model to ./mod0.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.8471 - accuracy: 0.7927 - val_loss: 0.6703 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.81229 to 0.67028, saving model to ./mod0.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.7085 - accuracy: 0.8537 - val_loss: 0.5462 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.67028 to 0.54625, saving model to ./mod0.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.6028 - accuracy: 0.8415 - val_loss: 0.4745 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.54625 to 0.47447, saving model to ./mod0.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.5480 - accuracy: 0.8659 - val_loss: 0.3938 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.47447 to 0.39384, saving model to ./mod0.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4175 - accuracy: 0.9146 - val_loss: 0.3860 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.39384 to 0.38596, saving model to ./mod0.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3753 - accuracy: 0.9512 - val_loss: 0.4233 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.38596\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3378 - accuracy: 0.9634 - val_loss: 0.3191 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.38596 to 0.31907, saving model to ./mod0.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.2903 - accuracy: 0.9634 - val_loss: 0.3517 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.31907\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.2678 - accuracy: 1.0000 - val_loss: 0.2421 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.31907 to 0.24214, saving model to ./mod0.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.2287 - accuracy: 1.0000 - val_loss: 0.2334 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.24214 to 0.23341, saving model to ./mod0.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.2174 - accuracy: 0.9756 - val_loss: 0.2325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.23341 to 0.23248, saving model to ./mod0.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2145 - accuracy: 0.9878 - val_loss: 0.1899 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.23248 to 0.18994, saving model to ./mod0.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.1743 - accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.18994\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1975 - accuracy: 1.0000 - val_loss: 0.1870 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.18994 to 0.18697, saving model to ./mod0.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.1612 - accuracy: 1.0000 - val_loss: 0.5498 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.18697\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.1783 - accuracy: 0.9878 - val_loss: 0.2675 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.18697\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.1540 - accuracy: 1.0000 - val_loss: 0.1723 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.18697 to 0.17232, saving model to ./mod0.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.2600 - accuracy: 0.9512 - val_loss: 0.1369 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.17232 to 0.13685, saving model to ./mod0.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1489 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.13685\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.1289 - accuracy: 1.0000 - val_loss: 0.1335 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.13685 to 0.13352, saving model to ./mod0.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.1199 - accuracy: 1.0000 - val_loss: 0.1226 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.13352 to 0.12260, saving model to ./mod0.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1143 - accuracy: 1.0000 - val_loss: 0.1214 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.12260 to 0.12137, saving model to ./mod0.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.1163 - accuracy: 1.0000 - val_loss: 0.1227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.12137\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.1041 - accuracy: 1.0000 - val_loss: 0.1599 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.12137\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.1070 - accuracy: 1.0000 - val_loss: 0.1229 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.12137\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0927 - accuracy: 1.0000 - val_loss: 0.1044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.12137 to 0.10440, saving model to ./mod0.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0900 - accuracy: 1.0000 - val_loss: 0.1004 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.10440 to 0.10035, saving model to ./mod0.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0893 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.10035 to 0.09573, saving model to ./mod0.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0796 - accuracy: 1.0000 - val_loss: 0.1172 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.09573\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0755 - accuracy: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.09573\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0771 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.09573 to 0.09497, saving model to ./mod0.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0734 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.09497 to 0.08571, saving model to ./mod0.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.08571 to 0.08217, saving model to ./mod0.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 0.0801 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.08217 to 0.08006, saving model to ./mod0.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0597 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.08006 to 0.07596, saving model to ./mod0.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0607 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.07596 to 0.07124, saving model to ./mod0.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.07124 to 0.06595, saving model to ./mod0.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.06595 to 0.06470, saving model to ./mod0.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0532 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.06470\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.06470\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.06470\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 0.0616 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.06470 to 0.06157, saving model to ./mod0.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.06157\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.0715 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.06157\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.06157\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.06157 to 0.05922, saving model to ./mod0.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.05922 to 0.05573, saving model to ./mod0.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.05573 to 0.05218, saving model to ./mod0.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.0657 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.05218\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.0602 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.05218\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.05218\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.05218\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.0568 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.05218\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.05218\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.05218\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.05218\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.05218 to 0.04195, saving model to ./mod0.h5\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.04195 to 0.04056, saving model to ./mod0.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.04056\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.0900 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.04056\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.1343 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.04056\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.0902 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.04056\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.04056\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.04056 to 0.03768, saving model to ./mod0.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.03768\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.03768\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.03768\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.03768\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.03768\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.03768\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.03768\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.03768\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.0694 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.03768\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.03768\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.03768 to 0.03341, saving model to ./mod0.h5\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.03341\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.03341\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.03341\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.03341\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.03341\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.03341 to 0.02438, saving model to ./mod0.h5\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.02438 to 0.02172, saving model to ./mod0.h5\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.02172\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.02172\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.02172\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.02172\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.02172\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.02172\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.02172\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.02172\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.02172\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.02172\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.02172\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.02172\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.02172\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.02172\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.02172\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.02172 to 0.01956, saving model to ./mod0.h5\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01956\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.01956\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.01956\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.01956\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.01956\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.01956\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0470 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.01956\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.01956\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.01956\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.01956\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.01956\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.01956\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.01956\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.01956\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.01956\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.01956\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.01956\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.01956\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.01956\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.01956\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.01956\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.01956\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.01956\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.01956 to 0.01487, saving model to ./mod0.h5\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.01487\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.01487\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.01487\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0898 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.01487\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1037 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.01487\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1005 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.01487\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0775 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.01487\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.01487\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.01487\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.01487\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.01487\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.01487\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.01487\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.01487\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1066 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.01487\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0792 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.01487\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.01487\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.01487\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0660 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.01487\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.01487\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.1212 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.01487\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.01487\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.01487\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.01487\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.01487\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.01487\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.01487\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.01487\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.01487\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.01487\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.01487\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0587 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.01487\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0701 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.01487\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.01487\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.01487\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.01487\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.01487\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.01487\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.01487\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.01487\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0677 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.01487\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.01487\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.01487\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.01487\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.01487\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.01487\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.01487\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.01487\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.01487\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.01487\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 12ms/step - loss: 0.0079 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0507 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0149 - accuracy: 1.0000\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_55 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_57 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_59 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_61 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_63 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_65 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_67 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_69 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_71 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_73 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_75 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_77 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_79 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_81 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_83 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_85 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_87 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_89 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_91 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_93 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_95 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_97 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_99 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_101 (Lambda)             (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_103 (Lambda)             (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_105 (Lambda)             (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_107 (Lambda)             (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_54 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_56 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_58 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_60 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_62 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_64 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_66 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_68 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_70 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_72 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_74 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_76 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_78 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_80 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_82 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_84 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_86 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_88 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_90 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_92 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_94 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_96 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_98 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_100 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_102 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_104 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_106 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_27 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_28 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_29 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_30 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_31 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_32 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_33 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_34 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_35 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_36 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_37 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_38 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_39 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_40 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_41 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_42 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_43 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_44 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_45 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_46 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_47 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_48 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_49 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_50 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_51 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_52 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_53 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_27 (Gl (None, 8)            0           dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_28 (Gl (None, 8)            0           dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_29 (Gl (None, 8)            0           dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_30 (Gl (None, 8)            0           dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_31 (Gl (None, 8)            0           dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_32 (Gl (None, 8)            0           dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_33 (Gl (None, 8)            0           dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_34 (Gl (None, 8)            0           dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_35 (Gl (None, 8)            0           dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_36 (Gl (None, 8)            0           dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_37 (Gl (None, 8)            0           dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_38 (Gl (None, 8)            0           dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_39 (Gl (None, 8)            0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_40 (Gl (None, 8)            0           dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_41 (Gl (None, 8)            0           dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_42 (Gl (None, 8)            0           dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_43 (Gl (None, 8)            0           dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_44 (Gl (None, 8)            0           dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_45 (Gl (None, 8)            0           dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_46 (Gl (None, 8)            0           dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_47 (Gl (None, 8)            0           dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_48 (Gl (None, 8)            0           dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_49 (Gl (None, 8)            0           dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_50 (Gl (None, 8)            0           dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_51 (Gl (None, 8)            0           dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_52 (Gl (None, 8)            0           dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_53 (Gl (None, 8)            0           dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 216)          0           global_average_pooling3d_27[0][0]\n",
            "                                                                 global_average_pooling3d_28[0][0]\n",
            "                                                                 global_average_pooling3d_29[0][0]\n",
            "                                                                 global_average_pooling3d_30[0][0]\n",
            "                                                                 global_average_pooling3d_31[0][0]\n",
            "                                                                 global_average_pooling3d_32[0][0]\n",
            "                                                                 global_average_pooling3d_33[0][0]\n",
            "                                                                 global_average_pooling3d_34[0][0]\n",
            "                                                                 global_average_pooling3d_35[0][0]\n",
            "                                                                 global_average_pooling3d_36[0][0]\n",
            "                                                                 global_average_pooling3d_37[0][0]\n",
            "                                                                 global_average_pooling3d_38[0][0]\n",
            "                                                                 global_average_pooling3d_39[0][0]\n",
            "                                                                 global_average_pooling3d_40[0][0]\n",
            "                                                                 global_average_pooling3d_41[0][0]\n",
            "                                                                 global_average_pooling3d_42[0][0]\n",
            "                                                                 global_average_pooling3d_43[0][0]\n",
            "                                                                 global_average_pooling3d_44[0][0]\n",
            "                                                                 global_average_pooling3d_45[0][0]\n",
            "                                                                 global_average_pooling3d_46[0][0]\n",
            "                                                                 global_average_pooling3d_47[0][0]\n",
            "                                                                 global_average_pooling3d_48[0][0]\n",
            "                                                                 global_average_pooling3d_49[0][0]\n",
            "                                                                 global_average_pooling3d_50[0][0]\n",
            "                                                                 global_average_pooling3d_51[0][0]\n",
            "                                                                 global_average_pooling3d_52[0][0]\n",
            "                                                                 global_average_pooling3d_53[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          111104      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 512)          262656      dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 512)          262656      dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            513         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 347ms/step - loss: 1.0085 - accuracy: 0.4268 - val_loss: 0.8787 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.87871, saving model to ./mod1.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.8439 - accuracy: 0.7439 - val_loss: 0.7328 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.87871 to 0.73277, saving model to ./mod1.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.7159 - accuracy: 0.8537 - val_loss: 0.7305 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.73277 to 0.73053, saving model to ./mod1.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.6657 - accuracy: 0.7927 - val_loss: 0.5368 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.73053 to 0.53679, saving model to ./mod1.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.6896 - accuracy: 0.7561 - val_loss: 0.5907 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.53679\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.5134 - accuracy: 0.8780 - val_loss: 0.7296 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.53679\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4806 - accuracy: 0.9024 - val_loss: 0.4237 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.53679 to 0.42365, saving model to ./mod1.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4490 - accuracy: 0.8902 - val_loss: 0.4326 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.42365\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3572 - accuracy: 0.9756 - val_loss: 0.5715 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.42365\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3800 - accuracy: 0.9390 - val_loss: 0.2999 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.42365 to 0.29989, saving model to ./mod1.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.2913 - accuracy: 0.9756 - val_loss: 0.2835 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.29989 to 0.28346, saving model to ./mod1.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2721 - accuracy: 0.9878 - val_loss: 0.2551 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.28346 to 0.25511, saving model to ./mod1.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.2325 - accuracy: 0.9878 - val_loss: 0.2425 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.25511 to 0.24247, saving model to ./mod1.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.2279 - accuracy: 0.9878 - val_loss: 0.2147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.24247 to 0.21465, saving model to ./mod1.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.2359 - accuracy: 0.9756 - val_loss: 0.2248 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.21465\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.2038 - accuracy: 0.9878 - val_loss: 0.2227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.21465\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1985 - accuracy: 0.9878 - val_loss: 0.2160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.21465\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.1716 - accuracy: 1.0000 - val_loss: 0.1805 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.21465 to 0.18051, saving model to ./mod1.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1702 - accuracy: 1.0000 - val_loss: 0.1656 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.18051 to 0.16561, saving model to ./mod1.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1552 - accuracy: 1.0000 - val_loss: 0.1630 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.16561 to 0.16299, saving model to ./mod1.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.1542 - accuracy: 1.0000 - val_loss: 0.1541 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.16299 to 0.15410, saving model to ./mod1.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.1442 - accuracy: 1.0000 - val_loss: 0.1446 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.15410 to 0.14456, saving model to ./mod1.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.1417 - accuracy: 1.0000 - val_loss: 0.1399 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.14456 to 0.13986, saving model to ./mod1.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1306 - accuracy: 1.0000 - val_loss: 0.1393 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.13986 to 0.13932, saving model to ./mod1.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1256 - accuracy: 1.0000 - val_loss: 0.1411 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.13932\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.1228 - accuracy: 1.0000 - val_loss: 0.1290 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.13932 to 0.12898, saving model to ./mod1.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1181 - accuracy: 1.0000 - val_loss: 0.1158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.12898 to 0.11577, saving model to ./mod1.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1149 - accuracy: 1.0000 - val_loss: 0.1158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.11577 to 0.11575, saving model to ./mod1.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1123 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.11575 to 0.10512, saving model to ./mod1.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.1009 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.10512\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.1015 - accuracy: 1.0000 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.10512 to 0.09644, saving model to ./mod1.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0930 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.09644 to 0.09061, saving model to ./mod1.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0894 - accuracy: 1.0000 - val_loss: 0.0870 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.09061 to 0.08699, saving model to ./mod1.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.0859 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.08699 to 0.08374, saving model to ./mod1.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0828 - accuracy: 1.0000 - val_loss: 0.0805 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.08374 to 0.08047, saving model to ./mod1.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0808 - accuracy: 1.0000 - val_loss: 0.0775 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.08047 to 0.07746, saving model to ./mod1.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0765 - accuracy: 1.0000 - val_loss: 0.0749 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.07746 to 0.07493, saving model to ./mod1.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0723 - accuracy: 1.0000 - val_loss: 0.0723 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.07493 to 0.07227, saving model to ./mod1.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0704 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.07227 to 0.06963, saving model to ./mod1.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0677 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.06963 to 0.06752, saving model to ./mod1.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.06752 to 0.06392, saving model to ./mod1.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0637 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.06392 to 0.06354, saving model to ./mod1.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0670 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.06354 to 0.05978, saving model to ./mod1.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0574 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.05978 to 0.05808, saving model to ./mod1.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0553 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.05808 to 0.05655, saving model to ./mod1.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.05655 to 0.05453, saving model to ./mod1.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0525 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.05453 to 0.05259, saving model to ./mod1.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0480 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.05259 to 0.05103, saving model to ./mod1.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.05103 to 0.04916, saving model to ./mod1.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.04916 to 0.04768, saving model to ./mod1.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.04768 to 0.04637, saving model to ./mod1.h5\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.04637 to 0.04619, saving model to ./mod1.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0407 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.04619 to 0.04359, saving model to ./mod1.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.04359 to 0.04220, saving model to ./mod1.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.04220\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.04220 to 0.04061, saving model to ./mod1.h5\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.04061 to 0.03876, saving model to ./mod1.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.03876 to 0.03722, saving model to ./mod1.h5\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.03722\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.03722\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.03722 to 0.03460, saving model to ./mod1.h5\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.03460 to 0.03220, saving model to ./mod1.h5\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.03220 to 0.03113, saving model to ./mod1.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.03113 to 0.03017, saving model to ./mod1.h5\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.03017 to 0.02968, saving model to ./mod1.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.02968 to 0.02861, saving model to ./mod1.h5\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.02861 to 0.02814, saving model to ./mod1.h5\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.02814 to 0.02776, saving model to ./mod1.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.02776\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.02776\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.02776 to 0.02546, saving model to ./mod1.h5\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.02546 to 0.02450, saving model to ./mod1.h5\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.02450 to 0.02422, saving model to ./mod1.h5\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.02422\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.02422 to 0.02267, saving model to ./mod1.h5\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.02267 to 0.02245, saving model to ./mod1.h5\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.02245 to 0.02170, saving model to ./mod1.h5\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.02170 to 0.02076, saving model to ./mod1.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.02076 to 0.02048, saving model to ./mod1.h5\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.02048\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.02048 to 0.01954, saving model to ./mod1.h5\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01954\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01954\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.01954 to 0.01852, saving model to ./mod1.h5\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01852\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01852\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01852\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.01852 to 0.01827, saving model to ./mod1.h5\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01827\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.01827 to 0.01712, saving model to ./mod1.h5\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1476 - accuracy: 0.9268 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01712\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1751 - accuracy: 0.9634 - val_loss: 1.6717 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01712\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3830 - accuracy: 0.8780 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01712\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1018 - accuracy: 0.9512 - val_loss: 0.7970 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01712\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.2289 - accuracy: 0.9146 - val_loss: 0.0542 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01712\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.1373 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01712\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0496 - accuracy: 0.9878 - val_loss: 0.3178 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01712\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 0.1728 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01712\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0668 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01712\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01712\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.01712\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.01712\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.01712\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.01712\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.01712\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.01712\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.01712\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.01712\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.01712\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.01712\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.01712\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.01712\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.01712 to 0.01701, saving model to ./mod1.h5\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.01701 to 0.01673, saving model to ./mod1.h5\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.01673 to 0.01644, saving model to ./mod1.h5\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.01644 to 0.01628, saving model to ./mod1.h5\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.01628 to 0.01611, saving model to ./mod1.h5\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.01611 to 0.01601, saving model to ./mod1.h5\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.01601 to 0.01586, saving model to ./mod1.h5\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.01586 to 0.01575, saving model to ./mod1.h5\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.01575\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.01575\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.01575\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.01575\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.01575 to 0.01502, saving model to ./mod1.h5\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.01502 to 0.01465, saving model to ./mod1.h5\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.01465 to 0.01447, saving model to ./mod1.h5\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.01447 to 0.01439, saving model to ./mod1.h5\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.01439 to 0.01431, saving model to ./mod1.h5\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.01431 to 0.01431, saving model to ./mod1.h5\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.01431\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.01431\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.01431\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.01431\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.01431\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.01431\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.01431\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.01431\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.01431\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.01431\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.01431\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.01431\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.01431\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.01431\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.01431\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.01431\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.01431\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.01431\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.01431\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.01431\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.01431\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.01431\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.01431\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.01431\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.01431\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.01431\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.01431\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.01431\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.01431 to 0.01425, saving model to ./mod1.h5\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.01425 to 0.01414, saving model to ./mod1.h5\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss improved from 0.01414 to 0.01409, saving model to ./mod1.h5\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss improved from 0.01409 to 0.01400, saving model to ./mod1.h5\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss improved from 0.01400 to 0.01386, saving model to ./mod1.h5\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss improved from 0.01386 to 0.01382, saving model to ./mod1.h5\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.01382 to 0.01370, saving model to ./mod1.h5\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.01370 to 0.01357, saving model to ./mod1.h5\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.01357 to 0.01349, saving model to ./mod1.h5\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.01349\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.01349\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.01349\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.01349\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.01349\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.01349\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.01349\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.01349\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.01349 to 0.01326, saving model to ./mod1.h5\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss improved from 0.01326 to 0.01310, saving model to ./mod1.h5\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.01310 to 0.01296, saving model to ./mod1.h5\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.01296\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.01296\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.01296\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.01296\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.01296\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.01296\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss improved from 0.01296 to 0.01292, saving model to ./mod1.h5\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss improved from 0.01292 to 0.01261, saving model to ./mod1.h5\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.01261 to 0.01241, saving model to ./mod1.h5\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss improved from 0.01241 to 0.01219, saving model to ./mod1.h5\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.01219 to 0.01208, saving model to ./mod1.h5\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss improved from 0.01208 to 0.01192, saving model to ./mod1.h5\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.01192 to 0.01183, saving model to ./mod1.h5\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.01183 to 0.01183, saving model to ./mod1.h5\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.01183\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.01183\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.01183\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.01183\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.01183\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.01183\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.01183\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.01183\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 14ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1035 - accuracy: 0.9200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_109 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_111 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_113 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_115 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_117 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_119 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_121 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_123 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_125 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_127 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_129 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_131 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_133 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_135 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_137 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_139 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_141 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_143 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_145 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_147 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_149 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_151 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_153 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_155 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_157 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_159 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_161 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_108 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_110 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_112 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_114 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_116 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_118 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_120 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_122 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_124 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_126 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_128 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_130 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_132 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_134 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_136 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_138 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_140 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_142 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_144 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_146 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_148 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_150 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_152 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_154 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_156 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_158 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_160 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_54 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_55 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_56 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_57 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_58 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_59 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_60 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_61 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_62 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_63 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_64 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_65 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_66 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_67 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_68 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_69 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_70 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_71 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_72 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_73 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_74 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_75 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_76 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_77 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_78 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_79 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_80 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_65 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_67 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_71 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_72 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_73 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_76 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_77 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_79 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_80 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_54 (Gl (None, 8)            0           dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_55 (Gl (None, 8)            0           dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_56 (Gl (None, 8)            0           dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_57 (Gl (None, 8)            0           dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_58 (Gl (None, 8)            0           dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_59 (Gl (None, 8)            0           dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_60 (Gl (None, 8)            0           dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_61 (Gl (None, 8)            0           dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_62 (Gl (None, 8)            0           dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_63 (Gl (None, 8)            0           dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_64 (Gl (None, 8)            0           dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_65 (Gl (None, 8)            0           dropout_65[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_66 (Gl (None, 8)            0           dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_67 (Gl (None, 8)            0           dropout_67[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_68 (Gl (None, 8)            0           dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_69 (Gl (None, 8)            0           dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_70 (Gl (None, 8)            0           dropout_70[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_71 (Gl (None, 8)            0           dropout_71[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_72 (Gl (None, 8)            0           dropout_72[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_73 (Gl (None, 8)            0           dropout_73[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_74 (Gl (None, 8)            0           dropout_74[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_75 (Gl (None, 8)            0           dropout_75[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_76 (Gl (None, 8)            0           dropout_76[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_77 (Gl (None, 8)            0           dropout_77[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_78 (Gl (None, 8)            0           dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_79 (Gl (None, 8)            0           dropout_79[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_80 (Gl (None, 8)            0           dropout_80[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 216)          0           global_average_pooling3d_54[0][0]\n",
            "                                                                 global_average_pooling3d_55[0][0]\n",
            "                                                                 global_average_pooling3d_56[0][0]\n",
            "                                                                 global_average_pooling3d_57[0][0]\n",
            "                                                                 global_average_pooling3d_58[0][0]\n",
            "                                                                 global_average_pooling3d_59[0][0]\n",
            "                                                                 global_average_pooling3d_60[0][0]\n",
            "                                                                 global_average_pooling3d_61[0][0]\n",
            "                                                                 global_average_pooling3d_62[0][0]\n",
            "                                                                 global_average_pooling3d_63[0][0]\n",
            "                                                                 global_average_pooling3d_64[0][0]\n",
            "                                                                 global_average_pooling3d_65[0][0]\n",
            "                                                                 global_average_pooling3d_66[0][0]\n",
            "                                                                 global_average_pooling3d_67[0][0]\n",
            "                                                                 global_average_pooling3d_68[0][0]\n",
            "                                                                 global_average_pooling3d_69[0][0]\n",
            "                                                                 global_average_pooling3d_70[0][0]\n",
            "                                                                 global_average_pooling3d_71[0][0]\n",
            "                                                                 global_average_pooling3d_72[0][0]\n",
            "                                                                 global_average_pooling3d_73[0][0]\n",
            "                                                                 global_average_pooling3d_74[0][0]\n",
            "                                                                 global_average_pooling3d_75[0][0]\n",
            "                                                                 global_average_pooling3d_76[0][0]\n",
            "                                                                 global_average_pooling3d_77[0][0]\n",
            "                                                                 global_average_pooling3d_78[0][0]\n",
            "                                                                 global_average_pooling3d_79[0][0]\n",
            "                                                                 global_average_pooling3d_80[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          111104      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 512)          262656      dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          262656      dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            513         dense_10[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 362ms/step - loss: 0.9672 - accuracy: 0.5000 - val_loss: 0.8532 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.85321, saving model to ./mod2.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.8547 - accuracy: 0.5976 - val_loss: 0.8151 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.85321 to 0.81510, saving model to ./mod2.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.7618 - accuracy: 0.7805 - val_loss: 0.6761 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.81510 to 0.67608, saving model to ./mod2.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.6271 - accuracy: 0.8659 - val_loss: 0.6232 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.67608 to 0.62321, saving model to ./mod2.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.5294 - accuracy: 0.8780 - val_loss: 0.6018 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.62321 to 0.60176, saving model to ./mod2.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4433 - accuracy: 0.9146 - val_loss: 0.5567 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.60176 to 0.55666, saving model to ./mod2.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3913 - accuracy: 0.9512 - val_loss: 0.7128 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.55666\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3193 - accuracy: 0.9756 - val_loss: 0.4813 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.55666 to 0.48132, saving model to ./mod2.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3284 - accuracy: 0.9756 - val_loss: 0.4696 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.48132 to 0.46962, saving model to ./mod2.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.2533 - accuracy: 0.9878 - val_loss: 0.5950 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.46962\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.2993 - accuracy: 0.9512 - val_loss: 0.5072 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.46962\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.2317 - accuracy: 0.9878 - val_loss: 0.4187 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.46962 to 0.41872, saving model to ./mod2.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.2427 - accuracy: 0.9756 - val_loss: 0.4633 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.41872\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.2265 - accuracy: 0.9756 - val_loss: 0.4911 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.41872\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1985 - accuracy: 0.9878 - val_loss: 1.0392 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.41872\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.2922 - accuracy: 0.9634 - val_loss: 0.5847 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.41872\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.2014 - accuracy: 0.9878 - val_loss: 0.6535 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.41872\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1766 - accuracy: 1.0000 - val_loss: 0.3333 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.41872 to 0.33334, saving model to ./mod2.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1658 - accuracy: 1.0000 - val_loss: 0.3918 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.33334\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.1550 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.33334\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.1447 - accuracy: 1.0000 - val_loss: 0.4978 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.33334\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1381 - accuracy: 1.0000 - val_loss: 0.5290 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.33334\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1330 - accuracy: 1.0000 - val_loss: 0.5034 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.33334\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1257 - accuracy: 1.0000 - val_loss: 0.4637 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.33334\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1246 - accuracy: 1.0000 - val_loss: 0.4189 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.33334\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1156 - accuracy: 1.0000 - val_loss: 0.4201 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.33334\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1127 - accuracy: 1.0000 - val_loss: 0.3834 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.33334\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1061 - accuracy: 1.0000 - val_loss: 0.3474 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.33334\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1050 - accuracy: 1.0000 - val_loss: 0.2802 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.33334 to 0.28023, saving model to ./mod2.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.1017 - accuracy: 1.0000 - val_loss: 0.2820 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.28023\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0983 - accuracy: 1.0000 - val_loss: 0.3018 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.28023\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0907 - accuracy: 1.0000 - val_loss: 0.3635 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.28023\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0867 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.28023\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0833 - accuracy: 1.0000 - val_loss: 0.3923 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.28023\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0825 - accuracy: 1.0000 - val_loss: 0.3606 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.28023\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0805 - accuracy: 1.0000 - val_loss: 0.2309 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.28023 to 0.23094, saving model to ./mod2.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0770 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.23094 to 0.20475, saving model to ./mod2.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0737 - accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.20475\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0705 - accuracy: 1.0000 - val_loss: 0.3271 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.20475\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 0.3493 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.20475\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0686 - accuracy: 1.0000 - val_loss: 0.3072 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.20475\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 0.2811 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.20475\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0598 - accuracy: 1.0000 - val_loss: 0.2581 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.20475\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.2366 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.20475\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0546 - accuracy: 1.0000 - val_loss: 0.2316 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.20475\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.2352 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.20475\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0506 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.20475\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.20475\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0476 - accuracy: 1.0000 - val_loss: 0.2216 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.20475\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0469 - accuracy: 1.0000 - val_loss: 0.2380 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.20475\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.20475\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.2903 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.20475\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.2551 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.20475\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 0.2462 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.20475\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.2798 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.20475\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.2344 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.20475\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.1970 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.20475 to 0.19703, saving model to ./mod2.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.19703 to 0.18761, saving model to ./mod2.h5\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.1924 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.18761\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.4143 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.18761\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.3786 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.18761\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 0.2228 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.18761\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.1542 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.18761 to 0.15420, saving model to ./mod2.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.1706 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.15420\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.2273 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.15420\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.2898 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.15420\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.15420\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.1997 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.15420\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.1975 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.15420\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.2500 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.15420\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.3208 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.15420\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.3229 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.15420\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.2389 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.15420\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.1929 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.15420\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.2010 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.15420\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.2239 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.15420\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.2379 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.15420\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.2590 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.15420\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.2727 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.15420\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.2402 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.15420\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.2641 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.15420\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.2895 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.15420\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.15420\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.2268 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.15420\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.1681 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.15420\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.1410 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.15420 to 0.14103, saving model to ./mod2.h5\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.14103\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.2169 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.14103\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.1772 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.14103\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.2041 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.14103\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.2716 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.14103\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.3033 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.14103\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.14103\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.2084 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.14103\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.1924 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.14103\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.1793 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.14103\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.1652 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.14103\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.14103\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.1903 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.14103\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.2154 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.14103\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.2147 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.14103\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.2143 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.14103\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.2484 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.14103\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.2820 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.14103\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.2846 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.14103\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.2658 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.14103\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.1905 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.14103\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.14103\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.2259 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.14103\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.1923 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.14103\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.5423 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.14103\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.3938 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.14103\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.2556 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.14103\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.5494 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.14103\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.2452 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.14103\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.3063 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.14103\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.3848 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.14103\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.4260 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.14103\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.3236 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.14103\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.14103\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.2768 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.14103\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.2907 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.14103\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.3700 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.14103\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.4120 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.14103\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.4222 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.14103\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.3982 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.14103\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.3645 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.14103\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.14103\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3027 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.14103\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.3750 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.14103\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.14103\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.4500 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.14103\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.4228 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.14103\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3771 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.14103\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3374 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.14103\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3093 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.14103\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 15ms/step - loss: 0.0176 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.1117 - accuracy: 0.8000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1410 - accuracy: 0.9286\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_163 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_165 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_167 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_169 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_171 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_173 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_175 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_177 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_179 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_181 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_183 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_185 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_187 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_189 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_191 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_193 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_195 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_197 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_199 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_201 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_203 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_205 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_207 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_209 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_211 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_213 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_215 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_162 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_164 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_166 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_168 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_170 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_172 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_174 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_176 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_178 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_180 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_182 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_184 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_186 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_188 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_190 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_192 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_194 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_196 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_198 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_200 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_202 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_204 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_206 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_208 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_210 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_212 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_214 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_81 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_82 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_83 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_84 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_85 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_86 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_87 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_88 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_89 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_90 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_91 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_92 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_93 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_94 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_95 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_96 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_97 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_98 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_99 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_100 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_101 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_102 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_103 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_104 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_105 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_106 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_107 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_81 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_82 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_83 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_84 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_85 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_86 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_87 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_88 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_89 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_90 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_91 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_92 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_93 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_94 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_95 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_96 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_97 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_98 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_99 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_100 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_101 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_102 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_103 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_104 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_105 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_106 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_107 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_81 (Gl (None, 8)            0           dropout_81[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_82 (Gl (None, 8)            0           dropout_82[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_83 (Gl (None, 8)            0           dropout_83[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_84 (Gl (None, 8)            0           dropout_84[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_85 (Gl (None, 8)            0           dropout_85[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_86 (Gl (None, 8)            0           dropout_86[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_87 (Gl (None, 8)            0           dropout_87[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_88 (Gl (None, 8)            0           dropout_88[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_89 (Gl (None, 8)            0           dropout_89[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_90 (Gl (None, 8)            0           dropout_90[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_91 (Gl (None, 8)            0           dropout_91[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_92 (Gl (None, 8)            0           dropout_92[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_93 (Gl (None, 8)            0           dropout_93[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_94 (Gl (None, 8)            0           dropout_94[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_95 (Gl (None, 8)            0           dropout_95[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_96 (Gl (None, 8)            0           dropout_96[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_97 (Gl (None, 8)            0           dropout_97[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_98 (Gl (None, 8)            0           dropout_98[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_99 (Gl (None, 8)            0           dropout_99[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_100 (G (None, 8)            0           dropout_100[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_101 (G (None, 8)            0           dropout_101[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_102 (G (None, 8)            0           dropout_102[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_103 (G (None, 8)            0           dropout_103[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_104 (G (None, 8)            0           dropout_104[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_105 (G (None, 8)            0           dropout_105[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_106 (G (None, 8)            0           dropout_106[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_107 (G (None, 8)            0           dropout_107[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 216)          0           global_average_pooling3d_81[0][0]\n",
            "                                                                 global_average_pooling3d_82[0][0]\n",
            "                                                                 global_average_pooling3d_83[0][0]\n",
            "                                                                 global_average_pooling3d_84[0][0]\n",
            "                                                                 global_average_pooling3d_85[0][0]\n",
            "                                                                 global_average_pooling3d_86[0][0]\n",
            "                                                                 global_average_pooling3d_87[0][0]\n",
            "                                                                 global_average_pooling3d_88[0][0]\n",
            "                                                                 global_average_pooling3d_89[0][0]\n",
            "                                                                 global_average_pooling3d_90[0][0]\n",
            "                                                                 global_average_pooling3d_91[0][0]\n",
            "                                                                 global_average_pooling3d_92[0][0]\n",
            "                                                                 global_average_pooling3d_93[0][0]\n",
            "                                                                 global_average_pooling3d_94[0][0]\n",
            "                                                                 global_average_pooling3d_95[0][0]\n",
            "                                                                 global_average_pooling3d_96[0][0]\n",
            "                                                                 global_average_pooling3d_97[0][0]\n",
            "                                                                 global_average_pooling3d_98[0][0]\n",
            "                                                                 global_average_pooling3d_99[0][0]\n",
            "                                                                 global_average_pooling3d_100[0][0\n",
            "                                                                 global_average_pooling3d_101[0][0\n",
            "                                                                 global_average_pooling3d_102[0][0\n",
            "                                                                 global_average_pooling3d_103[0][0\n",
            "                                                                 global_average_pooling3d_104[0][0\n",
            "                                                                 global_average_pooling3d_105[0][0\n",
            "                                                                 global_average_pooling3d_106[0][0\n",
            "                                                                 global_average_pooling3d_107[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 512)          111104      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 512)          262656      dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 512)          262656      dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1)            513         dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 362ms/step - loss: 0.9746 - accuracy: 0.4390 - val_loss: 0.8488 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.84880, saving model to ./mod3.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.8379 - accuracy: 0.7195 - val_loss: 0.8281 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.84880 to 0.82808, saving model to ./mod3.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.7060 - accuracy: 0.8415 - val_loss: 0.6756 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.82808 to 0.67557, saving model to ./mod3.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.6053 - accuracy: 0.8293 - val_loss: 0.6937 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.67557\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4752 - accuracy: 0.8902 - val_loss: 0.6121 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.67557 to 0.61207, saving model to ./mod3.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4345 - accuracy: 0.9146 - val_loss: 0.4595 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.61207 to 0.45948, saving model to ./mod3.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3606 - accuracy: 0.9512 - val_loss: 0.3892 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.45948 to 0.38923, saving model to ./mod3.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3230 - accuracy: 0.9512 - val_loss: 0.5940 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.38923\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3167 - accuracy: 0.9756 - val_loss: 0.3044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.38923 to 0.30437, saving model to ./mod3.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.2677 - accuracy: 0.9756 - val_loss: 0.2770 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.30437 to 0.27705, saving model to ./mod3.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.2449 - accuracy: 0.9756 - val_loss: 0.2783 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.27705\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.2313 - accuracy: 0.9878 - val_loss: 0.1985 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.27705 to 0.19849, saving model to ./mod3.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.2091 - accuracy: 0.9878 - val_loss: 0.2062 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.19849\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1889 - accuracy: 1.0000 - val_loss: 0.2199 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.19849\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1740 - accuracy: 1.0000 - val_loss: 0.1702 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.19849 to 0.17022, saving model to ./mod3.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.1904 - accuracy: 1.0000 - val_loss: 0.2463 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.17022\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1746 - accuracy: 0.9878 - val_loss: 0.2355 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.17022\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1554 - accuracy: 1.0000 - val_loss: 0.1508 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.17022 to 0.15080, saving model to ./mod3.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.1460 - accuracy: 1.0000 - val_loss: 0.1387 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.15080 to 0.13875, saving model to ./mod3.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1375 - accuracy: 1.0000 - val_loss: 0.1373 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.13875 to 0.13730, saving model to ./mod3.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.1286 - accuracy: 1.0000 - val_loss: 0.1452 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.13730\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1268 - accuracy: 1.0000 - val_loss: 0.1285 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.13730 to 0.12849, saving model to ./mod3.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1171 - accuracy: 1.0000 - val_loss: 0.1181 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.12849 to 0.11805, saving model to ./mod3.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1124 - accuracy: 1.0000 - val_loss: 0.1269 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.11805\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1063 - accuracy: 1.0000 - val_loss: 0.1373 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.11805\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0978 - accuracy: 1.0000 - val_loss: 0.1382 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.11805\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.1031 - accuracy: 0.9878 - val_loss: 0.1044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.11805 to 0.10444, saving model to ./mod3.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0898 - accuracy: 1.0000 - val_loss: 0.0926 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.10444 to 0.09256, saving model to ./mod3.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0808 - accuracy: 1.0000 - val_loss: 0.0866 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.09256 to 0.08656, saving model to ./mod3.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0784 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.08656\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0741 - accuracy: 1.0000 - val_loss: 0.0932 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.08656\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0754 - accuracy: 1.0000 - val_loss: 0.0746 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.08656 to 0.07459, saving model to ./mod3.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0689 - accuracy: 1.0000 - val_loss: 0.0747 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.07459\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0620 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.07459\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 0.0750 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.07459\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0569 - accuracy: 1.0000 - val_loss: 0.0680 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.07459 to 0.06798, saving model to ./mod3.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0565 - accuracy: 1.0000 - val_loss: 0.0672 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.06798 to 0.06723, saving model to ./mod3.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0523 - accuracy: 1.0000 - val_loss: 0.0754 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.06723\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.06723 to 0.06610, saving model to ./mod3.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0506 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.06610 to 0.05959, saving model to ./mod3.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0489 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.05959\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.1093 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.05959\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.1162 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.05959\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.05959\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.05959 to 0.04749, saving model to ./mod3.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.04749 to 0.04419, saving model to ./mod3.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.04419\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.04419\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.04419\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.04419 to 0.03842, saving model to ./mod3.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.03842 to 0.03617, saving model to ./mod3.h5\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.03617\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.03617\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.03617\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.0470 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.03617\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.03617 to 0.03519, saving model to ./mod3.h5\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.03519 to 0.02970, saving model to ./mod3.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.02970 to 0.02963, saving model to ./mod3.h5\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.02963\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.02963\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.02963\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.02963\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.02963 to 0.02562, saving model to ./mod3.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.02562\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.02562\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.02562\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.02562\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.02562 to 0.02509, saving model to ./mod3.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.02509\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.02509\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.02509\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.02509\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.02509\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.02509 to 0.02350, saving model to ./mod3.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.02350\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.02350\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.02350\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.02350 to 0.01904, saving model to ./mod3.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.01904 to 0.01778, saving model to ./mod3.h5\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01778\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01778\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01778\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01778\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01778\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01778\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01778\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01778\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01778\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01778\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01778\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01778\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01778\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01778\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01778\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01778\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01778\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01778\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01778\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01778\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01778\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.01778\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.01778 to 0.01589, saving model to ./mod3.h5\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.01589\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0200 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.01589\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.01589\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.01589\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.01589\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.01589\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.01589\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.01589\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.01589\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.01589\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.01589\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.01589\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.01589\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.01589\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.01589\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.01589\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.01589\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.01589 to 0.01324, saving model to ./mod3.h5\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.01324 to 0.01285, saving model to ./mod3.h5\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.01285\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.01285\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.01285\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0200 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.01285\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.01285\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.01285\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.01285\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.01285\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.01285\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.01285\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.01285\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.01285\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.01285\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.01285 to 0.01040, saving model to ./mod3.h5\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.01040\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.01040\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.01040\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.01040\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.01040\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.01040\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.01040\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.01040 to 0.01019, saving model to ./mod3.h5\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.01019\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.01019\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.01019\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.01019\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.01019 to 0.00958, saving model to ./mod3.h5\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.00958\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.00958\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.2078 - accuracy: 0.9634 - val_loss: 0.0647 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.00958\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0203 - accuracy: 0.9878 - val_loss: 0.8004 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.00958\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0901 - accuracy: 0.9756 - val_loss: 0.1964 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.00958\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.00958\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.00958\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.00958\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.00958\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.00958\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0538 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.00958\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.00958\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.00958\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.00958\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.00958\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss improved from 0.00958 to 0.00884, saving model to ./mod3.h5\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.00884 to 0.00836, saving model to ./mod3.h5\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.00836 to 0.00805, saving model to ./mod3.h5\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.00805\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.00805\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.00805\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.00805\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.00805\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.00805\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.00805\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.00805\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.00805\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.00805\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00805\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00805\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00805\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00805\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00805\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00805\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00805\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00805\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00805\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00805\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00805\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00805\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00805\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00805\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00805\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00805\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.00805\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00805\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00805\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.00805\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.00805\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00805\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00805\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00805\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 13ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0809 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0081 - accuracy: 1.0000\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_217 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_219 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_221 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_223 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_225 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_227 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_229 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_231 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_233 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_235 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_237 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_239 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_241 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_243 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_245 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_247 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_249 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_251 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_253 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_255 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_257 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_259 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_261 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_263 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_265 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_267 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_269 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_216 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_218 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_220 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_222 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_224 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_226 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_228 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_230 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_232 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_234 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_236 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_238 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_240 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_242 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_244 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_246 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_248 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_250 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_252 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_254 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_256 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_258 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_260 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_262 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_264 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_266 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_268 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_108 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_109 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_110 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_111 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_112 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_113 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_114 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_115 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_116 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_117 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_118 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_119 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_120 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_121 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_122 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_123 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_124 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_125 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_126 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_127 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_128 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_129 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_130 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_131 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_132 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_133 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_134 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_108 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_109 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_110 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_111 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_114 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_115 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_116 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_117 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_118 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_119 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_120 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_121 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_122 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_123 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_124 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_125 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_126 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_127 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_128 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_129 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_130 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_131 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_132 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_133 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_134 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_108 (G (None, 8)            0           dropout_108[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_109 (G (None, 8)            0           dropout_109[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_110 (G (None, 8)            0           dropout_110[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_111 (G (None, 8)            0           dropout_111[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_112 (G (None, 8)            0           dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_113 (G (None, 8)            0           dropout_113[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_114 (G (None, 8)            0           dropout_114[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_115 (G (None, 8)            0           dropout_115[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_116 (G (None, 8)            0           dropout_116[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_117 (G (None, 8)            0           dropout_117[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_118 (G (None, 8)            0           dropout_118[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_119 (G (None, 8)            0           dropout_119[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_120 (G (None, 8)            0           dropout_120[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_121 (G (None, 8)            0           dropout_121[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_122 (G (None, 8)            0           dropout_122[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_123 (G (None, 8)            0           dropout_123[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_124 (G (None, 8)            0           dropout_124[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_125 (G (None, 8)            0           dropout_125[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_126 (G (None, 8)            0           dropout_126[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_127 (G (None, 8)            0           dropout_127[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_128 (G (None, 8)            0           dropout_128[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_129 (G (None, 8)            0           dropout_129[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_130 (G (None, 8)            0           dropout_130[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_131 (G (None, 8)            0           dropout_131[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_132 (G (None, 8)            0           dropout_132[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_133 (G (None, 8)            0           dropout_133[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_134 (G (None, 8)            0           dropout_134[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 216)          0           global_average_pooling3d_108[0][0\n",
            "                                                                 global_average_pooling3d_109[0][0\n",
            "                                                                 global_average_pooling3d_110[0][0\n",
            "                                                                 global_average_pooling3d_111[0][0\n",
            "                                                                 global_average_pooling3d_112[0][0\n",
            "                                                                 global_average_pooling3d_113[0][0\n",
            "                                                                 global_average_pooling3d_114[0][0\n",
            "                                                                 global_average_pooling3d_115[0][0\n",
            "                                                                 global_average_pooling3d_116[0][0\n",
            "                                                                 global_average_pooling3d_117[0][0\n",
            "                                                                 global_average_pooling3d_118[0][0\n",
            "                                                                 global_average_pooling3d_119[0][0\n",
            "                                                                 global_average_pooling3d_120[0][0\n",
            "                                                                 global_average_pooling3d_121[0][0\n",
            "                                                                 global_average_pooling3d_122[0][0\n",
            "                                                                 global_average_pooling3d_123[0][0\n",
            "                                                                 global_average_pooling3d_124[0][0\n",
            "                                                                 global_average_pooling3d_125[0][0\n",
            "                                                                 global_average_pooling3d_126[0][0\n",
            "                                                                 global_average_pooling3d_127[0][0\n",
            "                                                                 global_average_pooling3d_128[0][0\n",
            "                                                                 global_average_pooling3d_129[0][0\n",
            "                                                                 global_average_pooling3d_130[0][0\n",
            "                                                                 global_average_pooling3d_131[0][0\n",
            "                                                                 global_average_pooling3d_132[0][0\n",
            "                                                                 global_average_pooling3d_133[0][0\n",
            "                                                                 global_average_pooling3d_134[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 512)          111104      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 512)          262656      dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 512)          262656      dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 1)            513         dense_18[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 370ms/step - loss: 0.9275 - accuracy: 0.6098 - val_loss: 0.7634 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.76336, saving model to ./mod4.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.7900 - accuracy: 0.7805 - val_loss: 0.6700 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.76336 to 0.67001, saving model to ./mod4.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.6357 - accuracy: 0.8780 - val_loss: 0.5713 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.67001 to 0.57130, saving model to ./mod4.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.5338 - accuracy: 0.9268 - val_loss: 0.4167 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.57130 to 0.41668, saving model to ./mod4.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4129 - accuracy: 0.9512 - val_loss: 0.6344 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.41668\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3790 - accuracy: 0.9146 - val_loss: 0.4257 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.41668\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3237 - accuracy: 0.9634 - val_loss: 0.5210 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.41668\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3360 - accuracy: 0.9390 - val_loss: 0.3828 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.41668 to 0.38281, saving model to ./mod4.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.2761 - accuracy: 0.9756 - val_loss: 0.2272 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.38281 to 0.22716, saving model to ./mod4.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.2601 - accuracy: 0.9756 - val_loss: 0.3681 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.22716\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.2343 - accuracy: 0.9878 - val_loss: 0.2808 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.22716\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.2040 - accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.22716 to 0.19547, saving model to ./mod4.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.2053 - accuracy: 1.0000 - val_loss: 0.2140 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.19547\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1959 - accuracy: 1.0000 - val_loss: 0.2084 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.19547\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.1932 - accuracy: 0.9878 - val_loss: 0.1922 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.19547 to 0.19219, saving model to ./mod4.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1743 - accuracy: 1.0000 - val_loss: 0.1655 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.19219 to 0.16546, saving model to ./mod4.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1688 - accuracy: 1.0000 - val_loss: 0.1650 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.16546 to 0.16505, saving model to ./mod4.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.1699 - accuracy: 0.9878 - val_loss: 0.3344 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.16505\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1689 - accuracy: 0.9878 - val_loss: 0.2469 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.16505\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1460 - accuracy: 1.0000 - val_loss: 0.1454 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.16505 to 0.14538, saving model to ./mod4.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1358 - accuracy: 1.0000 - val_loss: 0.1319 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.14538 to 0.13194, saving model to ./mod4.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.1348 - accuracy: 1.0000 - val_loss: 0.1302 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.13194 to 0.13020, saving model to ./mod4.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1311 - accuracy: 1.0000 - val_loss: 0.2097 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.13020\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.1355 - accuracy: 0.9878 - val_loss: 0.1348 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.13020\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.1269 - accuracy: 0.9878 - val_loss: 0.1537 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.13020\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1076 - accuracy: 1.0000 - val_loss: 0.2079 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.13020\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.1031 - accuracy: 1.0000 - val_loss: 0.2220 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.13020\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.1019 - accuracy: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.13020\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0986 - accuracy: 1.0000 - val_loss: 0.1198 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.13020 to 0.11984, saving model to ./mod4.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0883 - accuracy: 1.0000 - val_loss: 0.1400 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.11984\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0855 - accuracy: 1.0000 - val_loss: 0.1654 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.11984\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0889 - accuracy: 1.0000 - val_loss: 0.1348 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.11984\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0786 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.11984 to 0.10339, saving model to ./mod4.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0762 - accuracy: 1.0000 - val_loss: 0.0913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.10339 to 0.09134, saving model to ./mod4.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0746 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.09134 to 0.08626, saving model to ./mod4.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0698 - accuracy: 1.0000 - val_loss: 0.0956 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.08626\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0671 - accuracy: 1.0000 - val_loss: 0.1362 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.08626\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0653 - accuracy: 1.0000 - val_loss: 0.1427 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.08626\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0623 - accuracy: 1.0000 - val_loss: 0.1012 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.08626\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0600 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.08626 to 0.07903, saving model to ./mod4.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.07903 to 0.07768, saving model to ./mod4.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0539 - accuracy: 1.0000 - val_loss: 0.0852 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.07768\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0524 - accuracy: 1.0000 - val_loss: 0.0873 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.07768\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0489 - accuracy: 1.0000 - val_loss: 0.0848 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.07768\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0469 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.07768\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.07768 to 0.07240, saving model to ./mod4.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.07240 to 0.06479, saving model to ./mod4.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0439 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.06479\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0945 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.06479\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.06479\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.06479\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.06479 to 0.06122, saving model to ./mod4.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.06122\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.0852 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.06122\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.06122\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.06122\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.06122 to 0.05802, saving model to ./mod4.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.05802\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.0761 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.05802\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0949 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.05802\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.05802\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.05802 to 0.05553, saving model to ./mod4.h5\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.05553 to 0.04055, saving model to ./mod4.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.04055\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.04055\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.1207 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.04055\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.0996 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.04055\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.04055\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.04055 to 0.03484, saving model to ./mod4.h5\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.03484 to 0.03372, saving model to ./mod4.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.03372\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.03372\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.03372\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.03372\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0518 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.03372\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.03372\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.03372 to 0.02703, saving model to ./mod4.h5\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.02703 to 0.02582, saving model to ./mod4.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.02582\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.02582\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.02582\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.02582 to 0.02304, saving model to ./mod4.h5\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.02304\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0615 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.02304\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0693 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.02304\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.02304\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.02304\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.02304\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0700 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.02304\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.2439 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.02304\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.02304 to 0.02116, saving model to ./mod4.h5\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.02116\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.02116 to 0.01897, saving model to ./mod4.h5\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01897\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.1163 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01897\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01897\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01897\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.01897 to 0.01591, saving model to ./mod4.h5\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01591\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01591\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.01591\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.01591\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.01591\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.01591\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.01591 to 0.01483, saving model to ./mod4.h5\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.01483 to 0.01469, saving model to ./mod4.h5\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.01469 to 0.01327, saving model to ./mod4.h5\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.01327\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.01327\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.01327\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.01327\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0339 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.01327\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.01327\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.01327\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.01327\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.01327\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.01327\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.01327\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.01327\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.01327\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.01327\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.01327\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.01327\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.01327\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.01327\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.01327\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.01327\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.01327\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.01327\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.01327\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.01327\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.01327\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.01327\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.01327\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.01327\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.01327\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.01327\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.01327\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.01327\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.01327\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.01327\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.01327\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.01327 to 0.01106, saving model to ./mod4.h5\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.01106\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.01106\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.2202 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.01106\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.3393 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.01106\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0254 - accuracy: 0.9878 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.01106 to 0.01071, saving model to ./mod4.h5\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.01071\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0209 - accuracy: 0.9878 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.01071\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.5765 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.01071\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.0340 - accuracy: 0.9878 - val_loss: 0.0796 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.01071\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.01071\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.01071\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss improved from 0.01071 to 0.00977, saving model to ./mod4.h5\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.00977\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.00977\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.1134 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.00977\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.1231 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.00977\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.00977\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.00977\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.00977\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.00977\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.00977\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.00977 to 0.00944, saving model to ./mod4.h5\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.00944\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0795 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.00944\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.00944\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.00944 to 0.00829, saving model to ./mod4.h5\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.00829 to 0.00790, saving model to ./mod4.h5\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.00790 to 0.00778, saving model to ./mod4.h5\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.00778\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.00778\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.00778\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.00778\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.00778\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00778\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00778\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00778\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00778\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00778\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00778\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00778\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00778\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00778\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00778\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00778\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00778\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00778\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00778\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00778\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00778\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss improved from 0.00778 to 0.00765, saving model to ./mod4.h5\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss improved from 0.00765 to 0.00716, saving model to ./mod4.h5\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss improved from 0.00716 to 0.00714, saving model to ./mod4.h5\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.00714\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.00714\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00714\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00714\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00714\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 16ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1209 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_271 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_273 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_275 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_277 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_279 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_281 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_283 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_285 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_287 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_289 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_291 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_293 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_295 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_297 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_299 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_301 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_303 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_305 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_307 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_309 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_311 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_313 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_315 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_317 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_319 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_321 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_323 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_270 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_272 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_274 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_276 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_278 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_280 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_282 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_283[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_284 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_285[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_286 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_287[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_288 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_289[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_290 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_291[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_292 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_293[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_294 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_295[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_296 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_297[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_298 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_299[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_300 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_301[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_302 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_303[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_304 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_305[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_306 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_307[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_308 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_309[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_310 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_311[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_312 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_313[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_314 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_315[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_316 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_317[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_318 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_319[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_320 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_321[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_322 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_323[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_135 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_136 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_137 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_138 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_139 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_140 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_141 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_282[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_142 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_284[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_143 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_286[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_144 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_288[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_145 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_290[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_146 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_292[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_147 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_294[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_148 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_296[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_149 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_298[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_150 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_300[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_151 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_302[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_152 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_304[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_153 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_306[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_154 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_308[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_155 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_310[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_156 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_312[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_157 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_314[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_158 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_316[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_159 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_318[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_160 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_320[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_161 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_322[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_135 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_136 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_137 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_138 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_139 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_140 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_141 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_142 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_143 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_144 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_145 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_146 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_147 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_148 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_149 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_150 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_151 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_152 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_153 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_154 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_155 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_156 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_157 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_158 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_159 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_160 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_161 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_135 (G (None, 8)            0           dropout_135[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_136 (G (None, 8)            0           dropout_136[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_137 (G (None, 8)            0           dropout_137[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_138 (G (None, 8)            0           dropout_138[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_139 (G (None, 8)            0           dropout_139[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_140 (G (None, 8)            0           dropout_140[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_141 (G (None, 8)            0           dropout_141[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_142 (G (None, 8)            0           dropout_142[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_143 (G (None, 8)            0           dropout_143[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_144 (G (None, 8)            0           dropout_144[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_145 (G (None, 8)            0           dropout_145[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_146 (G (None, 8)            0           dropout_146[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_147 (G (None, 8)            0           dropout_147[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_148 (G (None, 8)            0           dropout_148[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_149 (G (None, 8)            0           dropout_149[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_150 (G (None, 8)            0           dropout_150[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_151 (G (None, 8)            0           dropout_151[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_152 (G (None, 8)            0           dropout_152[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_153 (G (None, 8)            0           dropout_153[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_154 (G (None, 8)            0           dropout_154[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_155 (G (None, 8)            0           dropout_155[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_156 (G (None, 8)            0           dropout_156[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_157 (G (None, 8)            0           dropout_157[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_158 (G (None, 8)            0           dropout_158[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_159 (G (None, 8)            0           dropout_159[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_160 (G (None, 8)            0           dropout_160[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_161 (G (None, 8)            0           dropout_161[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 216)          0           global_average_pooling3d_135[0][0\n",
            "                                                                 global_average_pooling3d_136[0][0\n",
            "                                                                 global_average_pooling3d_137[0][0\n",
            "                                                                 global_average_pooling3d_138[0][0\n",
            "                                                                 global_average_pooling3d_139[0][0\n",
            "                                                                 global_average_pooling3d_140[0][0\n",
            "                                                                 global_average_pooling3d_141[0][0\n",
            "                                                                 global_average_pooling3d_142[0][0\n",
            "                                                                 global_average_pooling3d_143[0][0\n",
            "                                                                 global_average_pooling3d_144[0][0\n",
            "                                                                 global_average_pooling3d_145[0][0\n",
            "                                                                 global_average_pooling3d_146[0][0\n",
            "                                                                 global_average_pooling3d_147[0][0\n",
            "                                                                 global_average_pooling3d_148[0][0\n",
            "                                                                 global_average_pooling3d_149[0][0\n",
            "                                                                 global_average_pooling3d_150[0][0\n",
            "                                                                 global_average_pooling3d_151[0][0\n",
            "                                                                 global_average_pooling3d_152[0][0\n",
            "                                                                 global_average_pooling3d_153[0][0\n",
            "                                                                 global_average_pooling3d_154[0][0\n",
            "                                                                 global_average_pooling3d_155[0][0\n",
            "                                                                 global_average_pooling3d_156[0][0\n",
            "                                                                 global_average_pooling3d_157[0][0\n",
            "                                                                 global_average_pooling3d_158[0][0\n",
            "                                                                 global_average_pooling3d_159[0][0\n",
            "                                                                 global_average_pooling3d_160[0][0\n",
            "                                                                 global_average_pooling3d_161[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 512)          111104      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 512)          262656      dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 512)          262656      dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 1)            513         dense_22[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 351ms/step - loss: 0.9465 - accuracy: 0.4940 - val_loss: 1.0269 - val_accuracy: 0.3846\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.02693, saving model to ./mod5.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.8175 - accuracy: 0.6867 - val_loss: 0.9071 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.02693 to 0.90710, saving model to ./mod5.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.6804 - accuracy: 0.8675 - val_loss: 0.7312 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.90710 to 0.73116, saving model to ./mod5.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.5705 - accuracy: 0.8916 - val_loss: 0.5856 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.73116 to 0.58563, saving model to ./mod5.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4586 - accuracy: 0.9036 - val_loss: 0.8398 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.58563\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4174 - accuracy: 0.8916 - val_loss: 0.3922 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.58563 to 0.39223, saving model to ./mod5.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3924 - accuracy: 0.9277 - val_loss: 0.4565 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.39223\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3243 - accuracy: 0.9518 - val_loss: 0.2872 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.39223 to 0.28717, saving model to ./mod5.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2664 - accuracy: 1.0000 - val_loss: 0.2452 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.28717 to 0.24524, saving model to ./mod5.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.2693 - accuracy: 0.9880 - val_loss: 0.3243 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.24524\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2539 - accuracy: 0.9759 - val_loss: 0.2124 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.24524 to 0.21242, saving model to ./mod5.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.2207 - accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.21242 to 0.20283, saving model to ./mod5.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.2148 - accuracy: 1.0000 - val_loss: 0.7256 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.20283\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.2563 - accuracy: 0.9639 - val_loss: 0.1946 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.20283 to 0.19459, saving model to ./mod5.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1973 - accuracy: 0.9880 - val_loss: 0.2216 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.19459\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.2020 - accuracy: 0.9759 - val_loss: 0.1783 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.19459 to 0.17834, saving model to ./mod5.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1597 - accuracy: 1.0000 - val_loss: 0.2962 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.17834\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1632 - accuracy: 1.0000 - val_loss: 0.2660 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.17834\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1563 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.17834 to 0.15195, saving model to ./mod5.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1481 - accuracy: 1.0000 - val_loss: 0.1419 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.15195 to 0.14188, saving model to ./mod5.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1337 - accuracy: 1.0000 - val_loss: 0.1346 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.14188 to 0.13463, saving model to ./mod5.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1315 - accuracy: 1.0000 - val_loss: 0.1278 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.13463 to 0.12778, saving model to ./mod5.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.1229 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.12778 to 0.12572, saving model to ./mod5.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1155 - accuracy: 1.0000 - val_loss: 0.1256 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.12572 to 0.12561, saving model to ./mod5.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1113 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.12561 to 0.11745, saving model to ./mod5.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1128 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.11745 to 0.10345, saving model to ./mod5.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1029 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.10345 to 0.09868, saving model to ./mod5.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1075 - accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.09868\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1217 - accuracy: 0.9759 - val_loss: 0.2412 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.09868\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1047 - accuracy: 0.9880 - val_loss: 0.0999 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.09868\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0827 - accuracy: 1.0000 - val_loss: 0.0952 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.09868 to 0.09516, saving model to ./mod5.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0982 - accuracy: 0.9880 - val_loss: 0.0900 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.09516 to 0.08997, saving model to ./mod5.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0873 - accuracy: 0.9880 - val_loss: 0.1057 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.08997\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0764 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.08997\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0779 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.08997\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0686 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.08997 to 0.07604, saving model to ./mod5.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0666 - accuracy: 1.0000 - val_loss: 0.0711 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.07604 to 0.07111, saving model to ./mod5.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0664 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.07111 to 0.06815, saving model to ./mod5.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0629 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.06815\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 0.0880 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.06815\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.06815\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0543 - accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.06815\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0536 - accuracy: 1.0000 - val_loss: 0.0616 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.06815 to 0.06158, saving model to ./mod5.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0514 - accuracy: 1.0000 - val_loss: 0.0578 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.06158 to 0.05782, saving model to ./mod5.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.05782\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.05782 to 0.05662, saving model to ./mod5.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.05662 to 0.05441, saving model to ./mod5.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0444 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.05441 to 0.05281, saving model to ./mod5.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0426 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.05281 to 0.05243, saving model to ./mod5.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.05243 to 0.05032, saving model to ./mod5.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.05032 to 0.04852, saving model to ./mod5.h5\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.04852\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.04852\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.04852\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.04852\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.04852\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.04852\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.04852\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.04852\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.04852 to 0.03894, saving model to ./mod5.h5\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.03894 to 0.03600, saving model to ./mod5.h5\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.03600\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.03600\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.03600 to 0.03424, saving model to ./mod5.h5\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.03424 to 0.03310, saving model to ./mod5.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.03310\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.03310\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.03310\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.03310\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.03310 to 0.03231, saving model to ./mod5.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.03231 to 0.02884, saving model to ./mod5.h5\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.0307 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.02884\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.02884 to 0.02702, saving model to ./mod5.h5\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.02702 to 0.02379, saving model to ./mod5.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.02379\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.02379\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.02379\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.02379\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.02379 to 0.02311, saving model to ./mod5.h5\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.02311\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.02311\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.02311\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.02311\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.0546 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.02311\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.02311\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.02311\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.02311\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.02311\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.02311\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.02311 to 0.01759, saving model to ./mod5.h5\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01759\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.1618 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01759\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0272 - accuracy: 0.9880 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01759\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0491 - accuracy: 0.9759 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.01759 to 0.01697, saving model to ./mod5.h5\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.9748 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01697\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1147 - accuracy: 0.9639 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01697\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01697\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0769 - accuracy: 0.9759 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01697\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.7483 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01697\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1173 - accuracy: 0.9518 - val_loss: 0.2821 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01697\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.01697\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.01697\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.01697\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.01697\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.01697\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.2365 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.01697\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.01697\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.2256 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.01697\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0297 - accuracy: 0.9880 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.01697\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.01697 to 0.01681, saving model to ./mod5.h5\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.01681 to 0.01532, saving model to ./mod5.h5\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.01532 to 0.01478, saving model to ./mod5.h5\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.01478 to 0.01445, saving model to ./mod5.h5\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.01445 to 0.01427, saving model to ./mod5.h5\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.01427 to 0.01409, saving model to ./mod5.h5\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.01409 to 0.01392, saving model to ./mod5.h5\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.01392 to 0.01374, saving model to ./mod5.h5\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.01374 to 0.01358, saving model to ./mod5.h5\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.01358 to 0.01349, saving model to ./mod5.h5\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.01349 to 0.01340, saving model to ./mod5.h5\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.01340 to 0.01334, saving model to ./mod5.h5\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.01334 to 0.01334, saving model to ./mod5.h5\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.01334 to 0.01331, saving model to ./mod5.h5\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.01331 to 0.01329, saving model to ./mod5.h5\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.01329 to 0.01328, saving model to ./mod5.h5\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.01328 to 0.01327, saving model to ./mod5.h5\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.01327 to 0.01326, saving model to ./mod5.h5\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.01326 to 0.01323, saving model to ./mod5.h5\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.01323 to 0.01318, saving model to ./mod5.h5\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.01318 to 0.01317, saving model to ./mod5.h5\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.01317 to 0.01309, saving model to ./mod5.h5\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.01309 to 0.01296, saving model to ./mod5.h5\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.01296 to 0.01287, saving model to ./mod5.h5\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.01287 to 0.01273, saving model to ./mod5.h5\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.01273 to 0.01259, saving model to ./mod5.h5\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.01259 to 0.01247, saving model to ./mod5.h5\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.01247 to 0.01240, saving model to ./mod5.h5\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.01240 to 0.01232, saving model to ./mod5.h5\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.01232 to 0.01224, saving model to ./mod5.h5\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.01224 to 0.01209, saving model to ./mod5.h5\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.01209 to 0.01197, saving model to ./mod5.h5\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.01197 to 0.01181, saving model to ./mod5.h5\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.01181 to 0.01171, saving model to ./mod5.h5\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.01171 to 0.01155, saving model to ./mod5.h5\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss improved from 0.01155 to 0.01149, saving model to ./mod5.h5\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.01149 to 0.01147, saving model to ./mod5.h5\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.01147 to 0.01141, saving model to ./mod5.h5\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.01141 to 0.01127, saving model to ./mod5.h5\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.01127 to 0.01115, saving model to ./mod5.h5\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.01115 to 0.01098, saving model to ./mod5.h5\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.01098\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.01098\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.01098\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.01098\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.01098\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.01098\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.01098\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.01098\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.01098\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.01098\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.01098\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.01098\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss improved from 0.01098 to 0.01065, saving model to ./mod5.h5\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.01065\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.01065 to 0.01040, saving model to ./mod5.h5\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.01040 to 0.01034, saving model to ./mod5.h5\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.01034\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.01034\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.01034\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.01034\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.01034\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.01034\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.01034\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.01034\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.01034\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.01034\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.01034\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.01034\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.01034\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.01034\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.01034\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.01034\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.01034\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.01034\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.01034\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.01034\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.01034\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.01034\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.01034 to 0.01032, saving model to ./mod5.h5\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss improved from 0.01032 to 0.00999, saving model to ./mod5.h5\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.00999 to 0.00964, saving model to ./mod5.h5\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.00964 to 0.00938, saving model to ./mod5.h5\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss improved from 0.00938 to 0.00886, saving model to ./mod5.h5\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00886\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00886\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss improved from 0.00886 to 0.00874, saving model to ./mod5.h5\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss improved from 0.00874 to 0.00846, saving model to ./mod5.h5\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00846\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00846\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00846\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 17ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0169 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_325 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_327 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_329 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_331 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_333 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_335 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_337 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_339 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_341 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_343 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_345 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_347 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_349 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_351 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_353 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_355 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_357 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_359 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_361 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_363 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_365 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_367 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_369 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_371 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_373 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_375 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_377 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_324 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_325[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_326 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_327[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_328 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_329[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_330 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_331[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_332 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_333[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_334 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_335[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_336 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_337[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_338 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_339[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_340 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_341[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_342 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_343[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_344 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_345[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_346 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_347[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_348 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_349[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_350 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_351[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_352 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_353[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_354 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_355[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_356 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_357[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_358 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_359[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_360 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_361[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_362 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_363[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_364 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_365[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_366 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_368 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_369[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_370 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_372 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_374 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_376 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_377[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_162 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_324[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_163 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_326[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_164 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_328[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_165 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_330[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_166 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_332[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_167 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_334[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_168 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_336[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_169 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_338[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_170 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_340[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_171 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_342[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_172 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_344[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_173 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_346[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_174 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_348[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_175 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_350[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_176 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_352[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_177 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_354[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_178 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_356[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_179 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_358[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_180 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_360[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_181 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_362[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_182 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_364[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_183 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_366[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_184 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_185 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_186 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_187 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_188 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_162 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_163 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_164 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_165 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_166 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_167 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_168 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_169 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_170 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_171 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_172 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_173 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_174 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_175 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_176 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_177 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_178 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_179 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_180 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_181 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_182 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_183 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_184 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_185 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_186 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_187 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_188 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_162 (G (None, 8)            0           dropout_162[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_163 (G (None, 8)            0           dropout_163[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_164 (G (None, 8)            0           dropout_164[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_165 (G (None, 8)            0           dropout_165[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_166 (G (None, 8)            0           dropout_166[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_167 (G (None, 8)            0           dropout_167[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_168 (G (None, 8)            0           dropout_168[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_169 (G (None, 8)            0           dropout_169[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_170 (G (None, 8)            0           dropout_170[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_171 (G (None, 8)            0           dropout_171[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_172 (G (None, 8)            0           dropout_172[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_173 (G (None, 8)            0           dropout_173[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_174 (G (None, 8)            0           dropout_174[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_175 (G (None, 8)            0           dropout_175[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_176 (G (None, 8)            0           dropout_176[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_177 (G (None, 8)            0           dropout_177[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_178 (G (None, 8)            0           dropout_178[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_179 (G (None, 8)            0           dropout_179[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_180 (G (None, 8)            0           dropout_180[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_181 (G (None, 8)            0           dropout_181[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_182 (G (None, 8)            0           dropout_182[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_183 (G (None, 8)            0           dropout_183[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_184 (G (None, 8)            0           dropout_184[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_185 (G (None, 8)            0           dropout_185[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_186 (G (None, 8)            0           dropout_186[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_187 (G (None, 8)            0           dropout_187[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_188 (G (None, 8)            0           dropout_188[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 216)          0           global_average_pooling3d_162[0][0\n",
            "                                                                 global_average_pooling3d_163[0][0\n",
            "                                                                 global_average_pooling3d_164[0][0\n",
            "                                                                 global_average_pooling3d_165[0][0\n",
            "                                                                 global_average_pooling3d_166[0][0\n",
            "                                                                 global_average_pooling3d_167[0][0\n",
            "                                                                 global_average_pooling3d_168[0][0\n",
            "                                                                 global_average_pooling3d_169[0][0\n",
            "                                                                 global_average_pooling3d_170[0][0\n",
            "                                                                 global_average_pooling3d_171[0][0\n",
            "                                                                 global_average_pooling3d_172[0][0\n",
            "                                                                 global_average_pooling3d_173[0][0\n",
            "                                                                 global_average_pooling3d_174[0][0\n",
            "                                                                 global_average_pooling3d_175[0][0\n",
            "                                                                 global_average_pooling3d_176[0][0\n",
            "                                                                 global_average_pooling3d_177[0][0\n",
            "                                                                 global_average_pooling3d_178[0][0\n",
            "                                                                 global_average_pooling3d_179[0][0\n",
            "                                                                 global_average_pooling3d_180[0][0\n",
            "                                                                 global_average_pooling3d_181[0][0\n",
            "                                                                 global_average_pooling3d_182[0][0\n",
            "                                                                 global_average_pooling3d_183[0][0\n",
            "                                                                 global_average_pooling3d_184[0][0\n",
            "                                                                 global_average_pooling3d_185[0][0\n",
            "                                                                 global_average_pooling3d_186[0][0\n",
            "                                                                 global_average_pooling3d_187[0][0\n",
            "                                                                 global_average_pooling3d_188[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 512)          111104      concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 512)          262656      dense_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 512)          262656      dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 1)            513         dense_26[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 371ms/step - loss: 0.9535 - accuracy: 0.5904 - val_loss: 0.8569 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.85695, saving model to ./mod6.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.8415 - accuracy: 0.6506 - val_loss: 0.8403 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.85695 to 0.84026, saving model to ./mod6.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.6877 - accuracy: 0.7711 - val_loss: 0.6693 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.84026 to 0.66934, saving model to ./mod6.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.6084 - accuracy: 0.8072 - val_loss: 0.5783 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.66934 to 0.57833, saving model to ./mod6.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.5015 - accuracy: 0.8675 - val_loss: 0.4890 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.57833 to 0.48896, saving model to ./mod6.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4853 - accuracy: 0.8916 - val_loss: 0.4064 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.48896 to 0.40638, saving model to ./mod6.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4050 - accuracy: 0.9036 - val_loss: 0.3553 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.40638 to 0.35530, saving model to ./mod6.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3223 - accuracy: 0.9880 - val_loss: 0.3331 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.35530 to 0.33306, saving model to ./mod6.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3020 - accuracy: 0.9639 - val_loss: 0.3065 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.33306 to 0.30649, saving model to ./mod6.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.2714 - accuracy: 1.0000 - val_loss: 0.2923 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.30649 to 0.29226, saving model to ./mod6.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.2366 - accuracy: 1.0000 - val_loss: 0.2853 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.29226 to 0.28531, saving model to ./mod6.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.2545 - accuracy: 0.9880 - val_loss: 0.3658 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.28531\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2186 - accuracy: 0.9880 - val_loss: 0.2305 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.28531 to 0.23050, saving model to ./mod6.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2274 - accuracy: 0.9759 - val_loss: 0.1927 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.23050 to 0.19271, saving model to ./mod6.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1979 - accuracy: 0.9880 - val_loss: 0.2055 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.19271\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1661 - accuracy: 1.0000 - val_loss: 0.1788 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.19271 to 0.17875, saving model to ./mod6.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1721 - accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.17875\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1594 - accuracy: 1.0000 - val_loss: 0.1668 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.17875 to 0.16679, saving model to ./mod6.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1426 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.16679\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1415 - accuracy: 1.0000 - val_loss: 0.1731 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.16679\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1371 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.16679 to 0.15851, saving model to ./mod6.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1307 - accuracy: 1.0000 - val_loss: 0.1520 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.15851 to 0.15196, saving model to ./mod6.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1323 - accuracy: 0.9880 - val_loss: 0.1774 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.15196\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1138 - accuracy: 1.0000 - val_loss: 0.2277 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.15196\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1071 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.15196\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1036 - accuracy: 1.0000 - val_loss: 0.1432 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.15196 to 0.14316, saving model to ./mod6.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0991 - accuracy: 1.0000 - val_loss: 0.1482 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.14316\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.14316 to 0.12932, saving model to ./mod6.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0956 - accuracy: 0.9880 - val_loss: 0.2132 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.12932\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0973 - accuracy: 1.0000 - val_loss: 0.1412 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.12932\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0777 - accuracy: 1.0000 - val_loss: 0.1056 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.12932 to 0.10558, saving model to ./mod6.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0737 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.10558\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0798 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.10558 to 0.09611, saving model to ./mod6.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0757 - accuracy: 1.0000 - val_loss: 0.1314 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.09611\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0671 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.09611\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 0.1162 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.09611\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0653 - accuracy: 1.0000 - val_loss: 0.1089 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.09611\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0588 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.09611\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0571 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.09611\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 0.1132 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.09611\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.0953 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.09611 to 0.09528, saving model to ./mod6.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0509 - accuracy: 1.0000 - val_loss: 0.0848 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.09528 to 0.08477, saving model to ./mod6.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.0821 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.08477 to 0.08212, saving model to ./mod6.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0449 - accuracy: 1.0000 - val_loss: 0.0873 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.08212\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 0.0751 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.08212 to 0.07505, saving model to ./mod6.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0416 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.07505 to 0.06564, saving model to ./mod6.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.06564 to 0.06469, saving model to ./mod6.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.06469\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0439 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.06469 to 0.05920, saving model to ./mod6.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.05920 to 0.05560, saving model to ./mod6.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.05560\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.05560\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.05560 to 0.05029, saving model to ./mod6.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0432 - accuracy: 0.9880 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.05029 to 0.04886, saving model to ./mod6.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.04886\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.04886\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.04886\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.04886\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.04886\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.04886\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.04886\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.04886\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.04886\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.04886 to 0.04881, saving model to ./mod6.h5\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.04881\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.04881 to 0.04874, saving model to ./mod6.h5\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.04874 to 0.04732, saving model to ./mod6.h5\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.04732 to 0.04560, saving model to ./mod6.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.04560\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.04560 to 0.04294, saving model to ./mod6.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.04294 to 0.04230, saving model to ./mod6.h5\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.04230\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.04230\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.04230 to 0.03944, saving model to ./mod6.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.03944\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.03944\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.03944\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.03944\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.03944 to 0.03921, saving model to ./mod6.h5\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.03921\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.03921\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.03921\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.03921 to 0.03857, saving model to ./mod6.h5\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.03857 to 0.03554, saving model to ./mod6.h5\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.03554 to 0.03486, saving model to ./mod6.h5\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.03486 to 0.03427, saving model to ./mod6.h5\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.03427 to 0.03405, saving model to ./mod6.h5\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.03405 to 0.03294, saving model to ./mod6.h5\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.03294\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.03294\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.03294\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.03294\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.03294\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.03294 to 0.03023, saving model to ./mod6.h5\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.03023\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.03023\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.03023\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.03023\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.03023\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.03023\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.03023\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.03023\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.03023\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.03023 to 0.02899, saving model to ./mod6.h5\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.02899 to 0.02893, saving model to ./mod6.h5\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.02893 to 0.02812, saving model to ./mod6.h5\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.02812\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.02812 to 0.02706, saving model to ./mod6.h5\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.02706 to 0.02677, saving model to ./mod6.h5\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.02677 to 0.02676, saving model to ./mod6.h5\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.02676 to 0.02557, saving model to ./mod6.h5\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.02557 to 0.02466, saving model to ./mod6.h5\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.02466\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.02466\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.02466 to 0.02354, saving model to ./mod6.h5\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.02354\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.02354\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.02354\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.02354\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0366 - accuracy: 0.9880 - val_loss: 0.9668 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.02354\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.1656 - accuracy: 0.9277 - val_loss: 0.3092 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.02354\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1227 - accuracy: 0.9398 - val_loss: 0.1186 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.02354\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.02354 to 0.02140, saving model to ./mod6.h5\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.02140\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.02140\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.0538 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.02140\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.02140\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.02140\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.02140\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.02140 to 0.01739, saving model to ./mod6.h5\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.01739 to 0.01562, saving model to ./mod6.h5\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.01562 to 0.01469, saving model to ./mod6.h5\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.01469 to 0.01409, saving model to ./mod6.h5\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.01409 to 0.01370, saving model to ./mod6.h5\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.01370 to 0.01332, saving model to ./mod6.h5\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.01332 to 0.01303, saving model to ./mod6.h5\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.01303 to 0.01282, saving model to ./mod6.h5\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.01282 to 0.01259, saving model to ./mod6.h5\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.01259 to 0.01231, saving model to ./mod6.h5\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.01231 to 0.01211, saving model to ./mod6.h5\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.01211 to 0.01186, saving model to ./mod6.h5\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.01186 to 0.01169, saving model to ./mod6.h5\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.01169 to 0.01151, saving model to ./mod6.h5\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.01151 to 0.01135, saving model to ./mod6.h5\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss improved from 0.01135 to 0.01121, saving model to ./mod6.h5\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.01121 to 0.01114, saving model to ./mod6.h5\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.01114 to 0.01108, saving model to ./mod6.h5\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.01108 to 0.01102, saving model to ./mod6.h5\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.01102 to 0.01101, saving model to ./mod6.h5\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.01101\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.01101\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.01101 to 0.01068, saving model to ./mod6.h5\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.01068 to 0.01049, saving model to ./mod6.h5\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.01049 to 0.01037, saving model to ./mod6.h5\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss improved from 0.01037 to 0.01030, saving model to ./mod6.h5\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss improved from 0.01030 to 0.01029, saving model to ./mod6.h5\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.01029 to 0.01025, saving model to ./mod6.h5\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.01025 to 0.01023, saving model to ./mod6.h5\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.01023\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.01023\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.01023\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.01023\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.01023\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.01023\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.01023\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.01023\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.01023\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.01023 to 0.01018, saving model to ./mod6.h5\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.01018 to 0.01008, saving model to ./mod6.h5\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.01008 to 0.01004, saving model to ./mod6.h5\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.01004\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss improved from 0.01004 to 0.01002, saving model to ./mod6.h5\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.01002\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.01002\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.01002 to 0.01000, saving model to ./mod6.h5\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.01000 to 0.00997, saving model to ./mod6.h5\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00997\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00997\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00997\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00997\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss improved from 0.00997 to 0.00994, saving model to ./mod6.h5\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00994\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00994\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00994\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00994\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00994\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00994\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00994\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00994\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00994\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00994\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00994\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.00994\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00994\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00994\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss improved from 0.00994 to 0.00971, saving model to ./mod6.h5\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss improved from 0.00971 to 0.00959, saving model to ./mod6.h5\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss improved from 0.00959 to 0.00955, saving model to ./mod6.h5\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss improved from 0.00955 to 0.00954, saving model to ./mod6.h5\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00954\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 18ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0734 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0095 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVEPB4XHLzU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KeWBKywEp6Ub",
        "outputId": "38b5b297-8d8f-44cd-b144-ca0fdd8d5cf2"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(7):\n",
        "  y_test_pred=load_model(f'./mod{i}.h5')\n",
        "  y_test_pred=y_test_pred.predict(x_test)\n",
        "\n",
        "  y_pred=y_test_pred.flatten()\n",
        "\n",
        "  y_test_pred=np.where(y_pred<0.5, 0,1)\n",
        "\n",
        "  c_matrix=confusion_matrix(y_test,y_test_pred)\n",
        "  ax=sns.heatmap(c_matrix,annot=True, xticklabels=['No ADHD','ADHD'],yticklabels=['No ADHD','ADHD'],cbar=False,cmap='Blues')\n",
        "  ax.set_xlabel('Prediction')\n",
        "  ax.set_ylabel('Actual')\n",
        "  plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd9c914a7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEGCAYAAABmXi5tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS1ElEQVR4nO3debRdZXnH8e8jUBEkEIZcsAQigwwCZbZWQIaWIkQJg0qAKg6EQUFArXVRFEUrFtEKWBlFoCKoDEsU0VWITGolEAQKNIJBVKYoIQwy5+kfZ99wc73DSXLee27u+/2sddY9e3yfJPv8su979n53ZCaSpLHvVd0uQJI0Mgx8SaqEgS9JlTDwJakSBr4kVWLZbhcwmJWnXuTlQxqVZp11QLdLkAbVM265GGyZZ/iSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEga+JFXCwJekShj4klQJA1+SKmHgS1IlDHxJqoSBL0mVMPAlqRIGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlTDwJakSBr4kVcLAl6RKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEga+JFXCwJekShj4klSJIoEfEbtExOUR8b/N63sRsXOJtiRJ7el44EfEXsA3gKuAA4GDgKuBb0TEnp1uT5LUnmUL7PPjwJTM/FWfebdHxAzgdFrhL0kaYSW6dNbsF/YAZOYdQE+B9iRJbSgR+M8s5jJJUkElunTWj4jvDzA/gPUKtCdJakOJwN97iGVfKtCeJKkNHQ/8zLy+0/uUJC25jgd+RNwJ5GDLM3OLTrcpSRpeiS6dyc3PAH4IeO29JI0CJbp0ftv7PiKe7zstSeoex9KRpEqU6MPfus/kayJiK1rdOwBk5m2dblOSNLwSffin9nn/CPDlPtMJ7FqgTUnSMEr04e/S6X1KkpZciTN8ImI1WiNlbtzMuge4ODMfL9GeJGl4JYZH3gS4C9gGmAX8GtgOuCsiNh5qW0lSOSXO8E8CPpKZ3+k7MyL2Az4P7FegTfVxxmFvZo+t1mbOk8/x5n++CoApb1qHf9n/b9jodSuz6wlXM/M3/rKl7jr5s//Kz266gfHjV+WCS6/sdjlVKHFZ5ub9wx4gMy8DNivQnvq5+Pr72e/kaxead/fvnuDgL1/Pzfc+2qWqpIXtMXkKp5x2ZrfLqEqJM3yHR+6yn937GOusvuJC82Y99GSXqpEGtuXW2/LwQ3/odhlVKRH4EyLiuAHmB7BGgfYkSW0o0aVzDrDSAK/XAucOtWFETIuIGREx44X7phcoTZLqVeI6/M8Mtiwithtm27OBswFWnnrRoCNuSpIWXZHr8PuKiE2Bqc3rCWDb0m1Kkv5SZHb+RDoiJvFKyL8IrAtsm5kPtLsPz/AX33lH7cAOm/Sw2krL89i8Z/nC9+5g7tPP8++HbMfq45Zn3p9f4M4H5rJvvyt51J5ZZx3Q7RLGhM8c/3Fm3noL8554glVXW433TTuSyXt71faS6hm3XAy2rOOBHxE/B8YBlwCXZOavI2J2Zr5+UfZj4Gu0MvA1mg0V+CW+tH2U1pe0PbxyVY7hLUld1vHAz8wpwObArcCJETEbGB8R23e6LUlS+4p8aZuZ84DzgfMjYgLwLuArEbFOZk4s0aYkaWjFn3iVmY9l5hmZ+RZgh9LtSZIGNqKPOPT5tpLUPT7TVpIqYeBLUiWKBX5ErB0RV0TEnIh4LCIui4i1S7UnSRpayTP884HvA2sBrwOuauZJkrqgZOCvkZnnZ+ZLzeubODyyJHVNycD/U0QcHBHLNK+DgT8VbE+SNISSgf9+WjdcPQI8DOwPvK9ge5KkIRQbHrm55v4dpfYvSVo0HQ/8iPjUEIszM0/qdJuSpOGN1EPMVwQ+AKwGGPiS1AUlHnF4au/7iFgJ+AitvvtLgFMH206SVFaRPvyIWBU4DjgIuADYOjPnlmhLktSeEn34pwD70noY+eaZ+XSn25AkLboSl2V+lNadtf8KPBQRTzavpyLiyQLtSZLaUKIP3wHZJGkUMpwlqRIGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlTDwJakSBr4kVcLAl6RKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEoM+8SoiTgdysOWZeXSRiiRJRQz1iMMZI1aFJKm4QQM/My8YyUIkSWUN+xDziFgD+ASwKbB87/zM3LVgXZKkDmvnS9tvAfcArwc+AzwA3FKwJklSAe0E/mqZeR7wYmZen5nvBzy7l6SlzLBdOsCLzc+HI2Iv4CFg1XIlSZJKaCfwPxcRKwMfBU4HxgHHFq1KktRxwwZ+Zv6geTsP2KVsOZKkUtq5Sud8BrgBq+nLlyQtJdrp0vlBn/fLA/vQ6seXJC1F2unSuazvdER8G7ipWEWSpCIic9DhcgbeIGIj4IeZuUGZklqee2nwcXykbhq/3Ye7XYI0qGdnnhGDLWunD/8pFu7Df4TWnbeSpKVIO106K41EIZKksoa90zYirm1nniRpdBtqPPzlgRWA1SNiPNDbLzQO+OsRqE2S1EFDdekcBhwDvA64lVcC/0ngjMJ1SZI6bKjx8L8KfDUijsrM00ewJklSAe2Mljk/IlbpnYiI8RFxZMGaJEkFtBP4h2bmE70TmTkXOLRcSZKkEtoJ/GUiYsGF/BGxDPBX5UqSJJXQzlg61wCXRsRZzfRhwI/KlSRJKqGdwP8EMA04vJm+A1izWEWSpCKG7dLJzPnA/9B6lu32tB5veE/ZsiRJnTbUjVdvAKY2rz8ClwJkpg9BkaSl0FBdOvcCNwKTM/M+gIjw0YaStJQaqktnX+BhYHpEnBMRu/HK3baSpKXMoIGfmVdm5gHAxsB0WsMsTIiIr0fE7iNVoCSpM9r50vaZzLw4M98OrA3MxPHwJWmp086NVwtk5tzMPDszdytVkCSpjEUKfEnS0svAl6RKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEga+JFXCwJekShj4klQJA1+SKmHgS1IlDHxJqoSBL0mVMPAlqRIGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlVi21I4jYhVgw2ZyVmbOK9WWJGl4HQ/8iHg1cBYwBZgNBLBuRFwBHJ6ZL3S6TUnS8Ep06RwPLAdMzMytMnNLYB1a/7mcUKA9SVIbSgT+vsChmflU74zm/ZHAPgXakyS1oUTgz8/MP/efmZlPA1mgPUlSG0p8aZsRMZ5W331/8wu0J0lqQ4nAXxm4lYED3zN8SeqSjgd+Zk7q9D4lSUuuxGWZWw+1PDNv63SbGtzNN97AF0/+PPNfns8++72TDxw6rdslqWJnfvog3rbTZsx5/Cm2fee/AfCpI/di8lu3YH4mcx5/immf/i8enuNtOyVEZmd7WSJiep/JbWh17/TKzNy1nf0895LdP0vq5Zdf5h17/SNnnXM+PT09HPju/Tn5lC+z/gYbdLu0pdr47T7c7RKWWm/Zen2e+fPznHvSexYE/korLs9TzzwHwJFT38rG663F0Z+/pJtlLtWenXnGQN3pQJkunV1630fEzL7TGll33XkHEyeuy9oTJwKwx5578dPp1xr46pqbb7ufddZadaF5vWEPsMJrXk2nT0L1imJDKzT8l+uixx59lDXXWnPB9ISeHu68444uViQN7MQPvZ2DJm/PvKefZY9pp3W7nDFrVA2eFhHTImJGRMw475yzu12OpBFy4teuYsO3ncAlP5rB4e/eqdvljFklvrQ9nVfO7NeOiIX+u87MowfbNjPPBs4G+/A7YUJPD488/MiC6ccefZSenp4uViQN7dKrb+GK04/gc2de3e1SxqQSXToz+ry/ddC1VNwbN9ucBx98gN///nf0TOjhmqt/yBdOObXbZUkLWX+dNbj/wTkATN55C2Y98GiXKxq7Snxpe0Gn96nFs+yyy/LJ4z/FEdM+yPz5LzNln/3YYIMNh99QKuSCLxzCjttsyOqrvJb7rjmJk868mj12eCMbrjuB+fOTBx9+3Ct0Cur4ZZkAEfFe4CPARs2se4DTMvPCdvdhl45GKy/L1Gg2opdlNmF/DHAccButIRa2Bk6JiMzMizrdpiRpeCWu0jkC2Cczp2fmvMx8IjOvA/YDPlSgPUlSG0oE/rjMfKD/zGbeuALtSZLaUCLwn13MZZKkgkpclrlJRAx0O2cA6xVoT5LUhiKBP8C8ACYCnyzQniSpDSWuw/9t7/uI2Ao4EHgnMBu4rNPtSZLaU+KyzDcAU5vXH4FLaV3v76iZktRFJbp07gVuBCZn5n0AEXFsgXYkSYugxFU6+wIPA9Mj4pyI2I2Bn28rSRpBHQ/8zLwyMw8ANgam07rrdkJEfD0idu90e5Kk9hQbDz8zn8nMizPz7cDawEzgE6XakyQNbUQegJKZczPz7MzcbSTakyT9pVH1xCtJUjkGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlTDwJakSBr4kVcLAl6RKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEga+JFXCwJekShj4klQJA1+SKmHgS1IlDHxJqoSBL0mVMPAlqRIGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlTDwJakSBr4kVcLAl6RKGPiSVAkDX5IqEZnZ7Ro0AiJiWmae3e06pP48NkeOZ/j1mNbtAqRBeGyOEANfkiph4EtSJQz8ethHqtHKY3OE+KWtJFXCM3xJqoSBL0mVMPBHmYjIiDi1z/THIuLExdjPlRHxi37zToyIP0TE7RHx64i4PCI27bP8pxGxbZ/pSRFxV/N+54iYFxEzI+L/IuKGiJi8WH9IjVkRMaU5hjdupidFxLPNcXNPRPwyIg7ps/4hEXFGv30sOA4j4oGIuLN53R0Rn4uI5Uf0DzWGGPijz/PAvhGx+uLuICJWAbYBVo6I9fot/kpmbpmZGwKXAtdFxBpt7vrGzNwqMzcCjgbOiIjdFrdOjUlTgZuan73ub46bTYADgGMi4n2LsM9dMnNzYHtgPeCsjlVbGQN/9HmJ1lULx/Zf0JwtXRcRd0TEtRGxziD72Be4CriE1gdsQJl5KfAT4MBFLTIzbwc+C3x4UbfV2BQRrwV2AD7AIMddZv4GOI7WCcMiycyngcOBKRGx6hKUWi0Df3T6GnBQRKzcb/7pwAWZuQXwLeC0QbafCny7eU0dZJ1etwEb95n+VtPlcztw9SJuq7rtDVyTmbOAP0XENoOs1/+4eXfvMdccd9sOsh2Z+SQwG9iwU0XXxMAfhZqD+kL+8izozcDFzfuLaJ1NLSQiemh9GG5qPngvRsRmQzQX/aYParp8tgT2HKbU/tuqblNp/VZJ83Owk43+x82lvcdcc9zNGKYdj7vFtGy3C9Cg/oPWmdD5i7jdu4DxwOyIABhH64N3/CDrb8XwH7DBbAXcs5jbagxpulh2BTaPiASWAZLWb6v9LfZxExErAZOAWYtXad08wx+lMvNx4Du0+kN7/YxX+kYPAm4cYNOpwB6ZOSkzJ9H68nbA/tSI2A/YnVbXzyKJiC2AExj4A6367A9clJnrNsfeRFpdLxP7rhQRk4Av0eqeXCTNdwT/CVyZmXOXuOIKeYY/up3Kwl+KHgWcHxEfB+YAC13p0HyY1gUWXI6ZmbObyynf1Mw6NiIOBlYE7gJ2zcw5bdazY0TMBFYAHgOOzsxrF/lPpbFoKvDFfvMuAz4JrN8cN8sDTwGnZeY3F2Hf06P16+qrgCuAk5a83Do5tIIkVcIuHUmqhIEvSZUw8CWpEga+JFXCwJekShj4GpMi4uXmVv27IuK7EbHCEuzrmxGxf/P+3L4jjA6w7s4R8Xd9pg+PiPcsbttSJxn4GquebW7V3wx4gdagWwtExGLdg5KZH8zMu4dYZWdgQeBn5pmZeeHitCV1moGvGtwIbNCcfd8YEd8H7o6IZSLilIi4pRmB9DCAaDmjGff/v4EJvTvqN1b7HhFxW0T8qhm9dBKt/1iObX672LF5BsHHmvW3jIhfNG1dERHj++zzi81Y8bMiYscR/dtRNbzTVmNacyb/NuCaZtbWwGbNHcjTgHmZuV1EvBq4OSJ+Qmusl42ATYEe4G7gG/32uwZwDrBTs69VM/PxiDgTeDozv9Ss1/d5ARcCR2Xm9RHxWeDTwDHNsmUzc/uI2LOZ//ed/ruQDHyNVa9phtqF1hn+ebS6Wn6ZmbOb+bsDW/T2zwMr0xppdCfg25n5MvBQRFw3wP7/Frihd1/N2EeDaoa6XiUzr29mXQB8t88qlzc/b6U1OJjUcQa+xqpnm6F2F2hGD32m7yxaZ9w/7rfecMNCl/B88/Nl/FyqEPvwVbMfA0dExHIAEfGGiFgRuIHWQzmWiYi1gF0G2PYXwE4R8fpm294nMD0FrNR/5cycB8zt0z//T8D1/deTSvJMQjU7l1b3yW3NaIxzgCm0RmTclVbf/YPAz/tvmJlzmu8ALo+IV9EaPfQfaD1a8nsRsTet0U37ei9wZnOJ6G/oN9qpVJqjZUpSJezSkaRKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEv8PqDioLD2D4b0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd9c2b455f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}