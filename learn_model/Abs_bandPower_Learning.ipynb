{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb의 사본의 사본의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v2Z_0CIqrhM"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "control=[]\n",
        "adhd=[]\n",
        "for i in os.listdir('./drive/MyDrive/bp/ADHD/'):\n",
        "    b=[]\n",
        "    for j in os.listdir(f'./drive/MyDrive/bp/ADHD/{i}'):\n",
        "        b.append(pd.read_csv(f'./drive/MyDrive/bp/ADHD/{i}/'+j,index_col='Chan').loc[:,:'TotalAbsPow'].to_numpy())\n",
        "    adhd.append(b)\n",
        "    \n",
        "for i in os.listdir('./drive/MyDrive/bp/Control/'):\n",
        "    b=[]    \n",
        "    for j in os.listdir(f'./drive/MyDrive/bp/Control/{i}'):\n",
        "        b.append(pd.read_csv(f'./drive/MyDrive/bp/Control/{i}/'+j,index_col='Chan').loc[:,:'TotalAbsPow'].to_numpy())\n",
        "    control.append(b)\n",
        "    \n",
        "\n",
        "control= np.array(control).transpose(1,0,2,3)\n",
        "adhd=np.array(adhd).transpose(1,0,2,3)\n",
        "\n",
        "x=np.append(adhd,control,axis=0)\n",
        "y=np.array([1]*61+[0]*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNGrhl7tyjkZ"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "cc=[]\n",
        "for i in range(7):\n",
        "    cc.append(x[:,:,:,i].flatten())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnAvzkDcyKJ-"
      },
      "source": [
        "std=RobustScaler()\n",
        "x=std.fit_transform(np.array(cc).transpose()).reshape(121,19,19,7)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbREeoOqI4YY",
        "outputId": "0ff2811d-e874-40f5-bb8b-d06948c4c228"
      },
      "source": [
        "import joblib\n",
        "# 객체를 pickled binary file 형태로 저장한다\n",
        "file_name = 'scale.pkl'\n",
        "joblib.dump(std, file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scale.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex8uYJ5FHvvZ"
      },
      "source": [
        "def crop(dimension, start, end):\n",
        "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
        "    # example : to crop tensor x[:, :, 5:10]\n",
        "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
        "    def func(x):\n",
        "        if dimension == 0:\n",
        "            return x[start: end]\n",
        "        if dimension == 1:\n",
        "            return x[:, start: end]\n",
        "        if dimension == 2:\n",
        "            return x[:, :, start: end]\n",
        "        if dimension == 3:\n",
        "            return x[:, :, :, start: end]\n",
        "        if dimension == 4:\n",
        "            return x[:, :, :, :, start: end]\n",
        "    return Lambda(func)\n",
        "import math\n",
        "def slice_model(model_input,unit,row_num,col_num,term):\n",
        "  remain=math.ceil(unit/2)\n",
        "  return [crop(3,col_num-(j+unit),col_num-j)(crop(2,row_num-(i+unit),row_num-i)(model_input)) for i in range(0,row_num-unit+1,term) for j in range(0,col_num-unit+1,term)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlP81GCoqlnc"
      },
      "source": [
        "import sys\n",
        "import sys\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, GlobalMaxPooling3D,Lambda,concatenate,Conv3D, MaxPooling3D,GlobalAveragePooling3D\n",
        "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "def mk_model(filepath=None):\n",
        "    \n",
        "    FILTER_SIZE=3\n",
        "    NUM_FILTERS=8\n",
        "    INPUT_SIZE=19\n",
        "    MAXPOOL_SIZE=2\n",
        "\n",
        "    densors=[]\n",
        "    model_input=Input(shape=(19,INPUT_SIZE,7,1))\n",
        "    for idx in slice_model(model_input,3,19,7,2):\n",
        "            \n",
        "            model_output=Conv3D(NUM_FILTERS, (FILTER_SIZE,FILTER_SIZE,FILTER_SIZE),activation='relu')(idx)\n",
        "            model_output=Dropout(0.5)(model_output)                        \n",
        "            model_output=GlobalAveragePooling3D()(model_output)\n",
        "            densors.append(model_output)\n",
        "\n",
        "    model_output=concatenate(densors)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "\n",
        "\n",
        "    model_output=Dense(units=1,activation='sigmoid',kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(model_output)\n",
        "    model = Model(inputs = model_input, outputs = model_output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_Wxk0KWV7Bq",
        "outputId": "7e9d7b6b-38ed-46c3-955c-53bcddb92cb4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "x_tra,x_test,y_tra,y_test=train_test_split(x,y,train_size=0.8,stratify=y,random_state=128)\n",
        "\n",
        "kf=KFold(7,True )\n",
        "train_score=[]\n",
        "test_score=[]\n",
        "val_score=[]\n",
        "test_list=[]\n",
        "\n",
        "idx=0\n",
        "\n",
        "val_list=[]\n",
        "\n",
        "for train_index, test_index in kf.split(x_tra):\n",
        "    callback_list = [\n",
        "    EarlyStopping( #성능 향상이 멈추면 훈련을 중지\n",
        "    monitor='val_loss',  #모델 검증 정확도를 모니터링\n",
        "    patience=50        #1 에포크 보다 더 길게(즉, 2에포크 동안 정확도가 향상되지 않으면 훈련 중지\n",
        "),\n",
        "    ModelCheckpoint( #에포크마다 현재 가중치를 저장\n",
        "    filepath=f'./mod{idx}.h5', #모델 파일 경로\n",
        "    monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\n",
        "    save_best_only=True,\n",
        "    mode='auto',\n",
        "    verbose=1\n",
        ")\n",
        "]\n",
        "    x_train,x_val=x_tra[train_index],x_tra[test_index]\n",
        "    y_train,y_val=y_tra[train_index],y_tra[test_index]\n",
        "    \n",
        "    #with strategy.scope():\n",
        "    model=mk_model()\n",
        "    print(model.summary())\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    hist=model.fit(x_train, y_train, epochs=200, validation_data=(x_val, y_val),batch_size=36,callbacks=callback_list)\n",
        "    \n",
        "    \n",
        "    plt.plot(hist.history['loss'],label='train'+str(idx))\n",
        "    plt.plot(hist.history['val_loss'],label='train'+str(idx))\n",
        "    plt.title('loss',fontsize=15)\n",
        "    plt.legend(['train','val'])\n",
        "    plt.show()\n",
        "    plt.plot(hist.history['accuracy'],label='train'+str(idx))\n",
        "    plt.plot(hist.history['val_accuracy'],label='train'+str(idx))\n",
        "    plt.legend(['train','val'])\n",
        "    plt.title('acc',fontsize=15)\n",
        "    plt.show()\n",
        "    model=load_model(f'./mod{idx}.h5')\n",
        "    \n",
        "    train_score.append(model.evaluate(x_train,y_train))\n",
        "    test_score.append(model.evaluate(x_test,y_test))\n",
        "    val_score.append(model.evaluate(x_val,y_val))\n",
        "    idx+=1\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_21 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_23 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_27 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_29 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_31 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_33 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_35 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_37 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_39 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_41 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_43 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_45 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_47 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_49 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_51 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_53 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 19, 3, 3, 1)  0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_20 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_22 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_24 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_26 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_28 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_30 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_32 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_34 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_36 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_38 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_40 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_42 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_44 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_46 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_48 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_50 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_52 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d (Conv3D)                 (None, 17, 1, 1, 8)  224         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_1 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_2 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_3 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_4 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_5 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_6 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_7 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_8 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_9 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_10 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_11 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_12 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_13 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_14 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_15 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_16 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_17 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_18 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_19 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_20 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_21 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_22 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_23 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_24 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_25 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_26 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 17, 1, 1, 8)  0           conv3d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d (Globa (None, 8)            0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_1 (Glo (None, 8)            0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_2 (Glo (None, 8)            0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_3 (Glo (None, 8)            0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_4 (Glo (None, 8)            0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_5 (Glo (None, 8)            0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_6 (Glo (None, 8)            0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_7 (Glo (None, 8)            0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_8 (Glo (None, 8)            0           dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_9 (Glo (None, 8)            0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_10 (Gl (None, 8)            0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_11 (Gl (None, 8)            0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_12 (Gl (None, 8)            0           dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_13 (Gl (None, 8)            0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_14 (Gl (None, 8)            0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_15 (Gl (None, 8)            0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_16 (Gl (None, 8)            0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_17 (Gl (None, 8)            0           dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_18 (Gl (None, 8)            0           dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_19 (Gl (None, 8)            0           dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_20 (Gl (None, 8)            0           dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_21 (Gl (None, 8)            0           dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_22 (Gl (None, 8)            0           dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_23 (Gl (None, 8)            0           dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_24 (Gl (None, 8)            0           dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_25 (Gl (None, 8)            0           dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_26 (Gl (None, 8)            0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 216)          0           global_average_pooling3d[0][0]   \n",
            "                                                                 global_average_pooling3d_1[0][0] \n",
            "                                                                 global_average_pooling3d_2[0][0] \n",
            "                                                                 global_average_pooling3d_3[0][0] \n",
            "                                                                 global_average_pooling3d_4[0][0] \n",
            "                                                                 global_average_pooling3d_5[0][0] \n",
            "                                                                 global_average_pooling3d_6[0][0] \n",
            "                                                                 global_average_pooling3d_7[0][0] \n",
            "                                                                 global_average_pooling3d_8[0][0] \n",
            "                                                                 global_average_pooling3d_9[0][0] \n",
            "                                                                 global_average_pooling3d_10[0][0]\n",
            "                                                                 global_average_pooling3d_11[0][0]\n",
            "                                                                 global_average_pooling3d_12[0][0]\n",
            "                                                                 global_average_pooling3d_13[0][0]\n",
            "                                                                 global_average_pooling3d_14[0][0]\n",
            "                                                                 global_average_pooling3d_15[0][0]\n",
            "                                                                 global_average_pooling3d_16[0][0]\n",
            "                                                                 global_average_pooling3d_17[0][0]\n",
            "                                                                 global_average_pooling3d_18[0][0]\n",
            "                                                                 global_average_pooling3d_19[0][0]\n",
            "                                                                 global_average_pooling3d_20[0][0]\n",
            "                                                                 global_average_pooling3d_21[0][0]\n",
            "                                                                 global_average_pooling3d_22[0][0]\n",
            "                                                                 global_average_pooling3d_23[0][0]\n",
            "                                                                 global_average_pooling3d_24[0][0]\n",
            "                                                                 global_average_pooling3d_25[0][0]\n",
            "                                                                 global_average_pooling3d_26[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          111104      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          262656      dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          262656      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            513         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 374ms/step - loss: 99.2550 - accuracy: 0.5732 - val_loss: 93.2498 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.24981, saving model to ./mod0.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 91.6429 - accuracy: 0.5244 - val_loss: 85.9259 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.24981 to 85.92591, saving model to ./mod0.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 84.3295 - accuracy: 0.8659 - val_loss: 78.9791 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00003: val_loss improved from 85.92591 to 78.97914, saving model to ./mod0.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 77.4317 - accuracy: 0.8049 - val_loss: 72.2439 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00004: val_loss improved from 78.97914 to 72.24390, saving model to ./mod0.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 70.8011 - accuracy: 0.8780 - val_loss: 65.8969 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.24390 to 65.89694, saving model to ./mod0.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 64.5020 - accuracy: 0.9390 - val_loss: 59.8540 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00006: val_loss improved from 65.89694 to 59.85399, saving model to ./mod0.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 58.5411 - accuracy: 0.9024 - val_loss: 54.1313 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00007: val_loss improved from 59.85399 to 54.13125, saving model to ./mod0.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 52.8833 - accuracy: 0.9024 - val_loss: 48.7217 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.13125 to 48.72166, saving model to ./mod0.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 47.4995 - accuracy: 0.9634 - val_loss: 43.5811 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.72166 to 43.58110, saving model to ./mod0.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 42.4633 - accuracy: 0.9390 - val_loss: 38.7831 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.58110 to 38.78310, saving model to ./mod0.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 37.6682 - accuracy: 0.9756 - val_loss: 34.2316 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.78310 to 34.23160, saving model to ./mod0.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 33.1903 - accuracy: 0.9756 - val_loss: 29.9822 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.23160 to 29.98221, saving model to ./mod0.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 29.0217 - accuracy: 0.9878 - val_loss: 26.0778 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00013: val_loss improved from 29.98221 to 26.07778, saving model to ./mod0.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 25.1364 - accuracy: 1.0000 - val_loss: 22.4019 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.07778 to 22.40194, saving model to ./mod0.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 21.5502 - accuracy: 1.0000 - val_loss: 19.0371 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.40194 to 19.03706, saving model to ./mod0.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 18.2831 - accuracy: 1.0000 - val_loss: 15.9914 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.03706 to 15.99143, saving model to ./mod0.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 15.2690 - accuracy: 1.0000 - val_loss: 13.2259 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 15.99143 to 13.22587, saving model to ./mod0.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 12.6140 - accuracy: 0.9878 - val_loss: 11.0247 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.22587 to 11.02474, saving model to ./mod0.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 10.2757 - accuracy: 1.0000 - val_loss: 8.6522 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.02474 to 8.65217, saving model to ./mod0.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 8.1674 - accuracy: 1.0000 - val_loss: 6.9030 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.65217 to 6.90302, saving model to ./mod0.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 6.4193 - accuracy: 1.0000 - val_loss: 5.2740 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 6.90302 to 5.27402, saving model to ./mod0.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 4.9440 - accuracy: 1.0000 - val_loss: 4.0327 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.27402 to 4.03272, saving model to ./mod0.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 3.7493 - accuracy: 1.0000 - val_loss: 3.0863 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.03272 to 3.08628, saving model to ./mod0.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 2.8957 - accuracy: 0.9878 - val_loss: 2.6310 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.08628 to 2.63105, saving model to ./mod0.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 2.8602 - accuracy: 0.7439 - val_loss: 2.3195 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.63105 to 2.31950, saving model to ./mod0.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 2.5085 - accuracy: 0.8537 - val_loss: 2.2433 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.31950 to 2.24328, saving model to ./mod0.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 2.1976 - accuracy: 0.9878 - val_loss: 2.4045 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.24328\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 2.0966 - accuracy: 0.9390 - val_loss: 1.7790 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.24328 to 1.77898, saving model to ./mod0.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 1.6425 - accuracy: 1.0000 - val_loss: 1.5358 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.77898 to 1.53575, saving model to ./mod0.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 1.4759 - accuracy: 0.9634 - val_loss: 1.2059 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.53575 to 1.20588, saving model to ./mod0.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 1.1568 - accuracy: 1.0000 - val_loss: 1.2000 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.20588 to 1.19998, saving model to ./mod0.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 1.0957 - accuracy: 1.0000 - val_loss: 1.0036 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.19998 to 1.00356, saving model to ./mod0.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.9344 - accuracy: 1.0000 - val_loss: 0.9043 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.00356 to 0.90433, saving model to ./mod0.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.8506 - accuracy: 0.9878 - val_loss: 0.8093 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.90433 to 0.80931, saving model to ./mod0.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.7521 - accuracy: 1.0000 - val_loss: 0.7930 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.80931 to 0.79303, saving model to ./mod0.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.6958 - accuracy: 1.0000 - val_loss: 0.6861 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.79303 to 0.68607, saving model to ./mod0.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.6353 - accuracy: 1.0000 - val_loss: 0.6428 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.68607 to 0.64282, saving model to ./mod0.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.5942 - accuracy: 1.0000 - val_loss: 0.6102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.64282 to 0.61021, saving model to ./mod0.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.5492 - accuracy: 1.0000 - val_loss: 0.5779 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.61021 to 0.57786, saving model to ./mod0.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.5224 - accuracy: 1.0000 - val_loss: 0.5605 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.57786 to 0.56049, saving model to ./mod0.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.4981 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.56049 to 0.53355, saving model to ./mod0.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.4814 - accuracy: 1.0000 - val_loss: 0.5227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.53355 to 0.52274, saving model to ./mod0.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4645 - accuracy: 1.0000 - val_loss: 0.5044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.52274 to 0.50435, saving model to ./mod0.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.4503 - accuracy: 1.0000 - val_loss: 0.4935 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.50435 to 0.49354, saving model to ./mod0.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4444 - accuracy: 1.0000 - val_loss: 0.4880 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.49354 to 0.48798, saving model to ./mod0.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4319 - accuracy: 1.0000 - val_loss: 0.4895 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.48798\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.4243 - accuracy: 1.0000 - val_loss: 0.4693 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.48798 to 0.46927, saving model to ./mod0.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4125 - accuracy: 1.0000 - val_loss: 0.4672 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.46927 to 0.46725, saving model to ./mod0.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.4118 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.46725 to 0.46655, saving model to ./mod0.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4104 - accuracy: 1.0000 - val_loss: 0.4510 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.46655 to 0.45097, saving model to ./mod0.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3981 - accuracy: 1.0000 - val_loss: 0.4534 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.45097\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3938 - accuracy: 1.0000 - val_loss: 0.4397 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.45097 to 0.43967, saving model to ./mod0.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3913 - accuracy: 1.0000 - val_loss: 0.4374 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.43967 to 0.43740, saving model to ./mod0.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3883 - accuracy: 1.0000 - val_loss: 0.4408 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.43740\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3847 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.43740 to 0.43357, saving model to ./mod0.h5\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3869 - accuracy: 1.0000 - val_loss: 0.4362 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.43357\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3799 - accuracy: 1.0000 - val_loss: 0.4305 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.43357 to 0.43047, saving model to ./mod0.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3760 - accuracy: 1.0000 - val_loss: 0.4315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.43047\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3751 - accuracy: 1.0000 - val_loss: 0.4260 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.43047 to 0.42600, saving model to ./mod0.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3695 - accuracy: 1.0000 - val_loss: 0.4221 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.42600 to 0.42205, saving model to ./mod0.h5\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3696 - accuracy: 1.0000 - val_loss: 0.4283 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.42205\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3709 - accuracy: 1.0000 - val_loss: 0.4384 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.42205\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3734 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.42205 to 0.42167, saving model to ./mod0.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3822 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.42167 to 0.41909, saving model to ./mod0.h5\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3741 - accuracy: 1.0000 - val_loss: 0.4131 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.41909 to 0.41307, saving model to ./mod0.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3679 - accuracy: 1.0000 - val_loss: 0.4176 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.41307\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3658 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.41307\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3607 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.41307 to 0.40743, saving model to ./mod0.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3594 - accuracy: 1.0000 - val_loss: 0.4344 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.40743\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3641 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.40743 to 0.40617, saving model to ./mod0.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3586 - accuracy: 1.0000 - val_loss: 0.4056 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.40617 to 0.40561, saving model to ./mod0.h5\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3541 - accuracy: 1.0000 - val_loss: 0.4013 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.40561 to 0.40128, saving model to ./mod0.h5\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3501 - accuracy: 1.0000 - val_loss: 0.4050 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.40128\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3511 - accuracy: 1.0000 - val_loss: 0.3979 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.40128 to 0.39791, saving model to ./mod0.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3512 - accuracy: 1.0000 - val_loss: 0.3978 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.39791 to 0.39777, saving model to ./mod0.h5\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3485 - accuracy: 1.0000 - val_loss: 0.3998 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.39777\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3460 - accuracy: 1.0000 - val_loss: 0.3937 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.39777 to 0.39374, saving model to ./mod0.h5\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3455 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.39374 to 0.39131, saving model to ./mod0.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3451 - accuracy: 1.0000 - val_loss: 0.3895 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.39131 to 0.38949, saving model to ./mod0.h5\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3416 - accuracy: 1.0000 - val_loss: 0.3908 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.38949\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3398 - accuracy: 1.0000 - val_loss: 0.3897 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.38949\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3421 - accuracy: 1.0000 - val_loss: 0.3853 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.38949 to 0.38529, saving model to ./mod0.h5\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3410 - accuracy: 1.0000 - val_loss: 0.3985 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.38529\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3403 - accuracy: 1.0000 - val_loss: 0.3844 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.38529 to 0.38438, saving model to ./mod0.h5\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3382 - accuracy: 1.0000 - val_loss: 0.3938 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.38438\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3400 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.38438\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3400 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.38438 to 0.38239, saving model to ./mod0.h5\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3368 - accuracy: 1.0000 - val_loss: 0.3840 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.38239\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3365 - accuracy: 1.0000 - val_loss: 0.3811 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.38239 to 0.38106, saving model to ./mod0.h5\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3363 - accuracy: 1.0000 - val_loss: 0.3803 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.38106 to 0.38031, saving model to ./mod0.h5\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3340 - accuracy: 1.0000 - val_loss: 0.3813 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.38031\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3328 - accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.38031\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3349 - accuracy: 1.0000 - val_loss: 0.4018 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.38031\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3390 - accuracy: 1.0000 - val_loss: 0.3788 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.38031 to 0.37879, saving model to ./mod0.h5\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3354 - accuracy: 1.0000 - val_loss: 0.3833 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.37879\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3352 - accuracy: 1.0000 - val_loss: 0.3813 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.37879\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3336 - accuracy: 1.0000 - val_loss: 0.3804 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.37879\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.3312 - accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.37879\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3319 - accuracy: 1.0000 - val_loss: 0.3781 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.37879 to 0.37809, saving model to ./mod0.h5\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3290 - accuracy: 1.0000 - val_loss: 0.3814 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.37809\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3300 - accuracy: 1.0000 - val_loss: 0.3909 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.37809\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3326 - accuracy: 1.0000 - val_loss: 0.3861 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.37809\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3326 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.37809\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3303 - accuracy: 1.0000 - val_loss: 0.3802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.37809\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3292 - accuracy: 1.0000 - val_loss: 0.3862 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.37809\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3290 - accuracy: 1.0000 - val_loss: 0.3773 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.37809 to 0.37734, saving model to ./mod0.h5\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3272 - accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.37734\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.3757 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.37734 to 0.37571, saving model to ./mod0.h5\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.3765 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.37571\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3256 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.37571\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3263 - accuracy: 1.0000 - val_loss: 0.3742 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.37571 to 0.37422, saving model to ./mod0.h5\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3259 - accuracy: 1.0000 - val_loss: 0.3752 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.37422\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3246 - accuracy: 1.0000 - val_loss: 0.3831 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.37422\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3266 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.37422\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3268 - accuracy: 1.0000 - val_loss: 0.3857 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.37422\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3308 - accuracy: 1.0000 - val_loss: 0.6645 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.37422\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3848 - accuracy: 1.0000 - val_loss: 0.6515 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.37422\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4393 - accuracy: 0.9878 - val_loss: 0.6269 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.37422\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4341 - accuracy: 1.0000 - val_loss: 0.8596 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.37422\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4156 - accuracy: 1.0000 - val_loss: 0.5616 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.37422\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.4023 - accuracy: 1.0000 - val_loss: 0.4513 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.37422\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3852 - accuracy: 1.0000 - val_loss: 0.4371 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.37422\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3718 - accuracy: 1.0000 - val_loss: 0.4284 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.37422\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3590 - accuracy: 1.0000 - val_loss: 0.4255 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.37422\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3495 - accuracy: 1.0000 - val_loss: 0.4200 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.37422\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3401 - accuracy: 1.0000 - val_loss: 0.4204 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.37422\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3347 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.37422\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3295 - accuracy: 1.0000 - val_loss: 0.4186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.37422\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3268 - accuracy: 1.0000 - val_loss: 0.4196 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.37422\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3274 - accuracy: 1.0000 - val_loss: 0.4173 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.37422\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3259 - accuracy: 1.0000 - val_loss: 0.4127 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.37422\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3250 - accuracy: 1.0000 - val_loss: 0.4100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.37422\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.4085 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.37422\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.4047 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.37422\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.4007 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.37422\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3225 - accuracy: 1.0000 - val_loss: 0.4046 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.37422\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3213 - accuracy: 1.0000 - val_loss: 0.4063 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.37422\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3217 - accuracy: 1.0000 - val_loss: 0.3998 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.37422\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3216 - accuracy: 1.0000 - val_loss: 0.3964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.37422\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.4009 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.37422\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3197 - accuracy: 1.0000 - val_loss: 0.3958 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.37422\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3198 - accuracy: 1.0000 - val_loss: 0.3942 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.37422\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3949 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.37422\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3201 - accuracy: 1.0000 - val_loss: 0.3895 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.37422\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 0.3901 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.37422\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3185 - accuracy: 1.0000 - val_loss: 0.3919 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.37422\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3182 - accuracy: 1.0000 - val_loss: 0.3881 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.37422\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3176 - accuracy: 1.0000 - val_loss: 0.3868 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.37422\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3174 - accuracy: 1.0000 - val_loss: 0.3810 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.37422\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3173 - accuracy: 1.0000 - val_loss: 0.3846 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.37422\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3181 - accuracy: 1.0000 - val_loss: 0.3928 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.37422\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3189 - accuracy: 1.0000 - val_loss: 0.3809 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.37422\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3179 - accuracy: 1.0000 - val_loss: 0.3815 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.37422\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3164 - accuracy: 1.0000 - val_loss: 0.3815 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.37422\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3165 - accuracy: 1.0000 - val_loss: 0.3794 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.37422\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3158 - accuracy: 1.0000 - val_loss: 0.3771 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.37422\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.37422\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3152 - accuracy: 1.0000 - val_loss: 0.3785 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.37422\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.3798 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.37422\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3158 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.37422\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3156 - accuracy: 1.0000 - val_loss: 0.3904 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.37422\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 11ms/step - loss: 0.3247 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4327 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3742 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_55 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_57 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_59 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_61 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_63 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_65 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_67 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_69 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_71 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_73 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_75 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_77 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_79 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_81 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_83 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_85 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_87 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_89 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_91 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_93 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_95 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_97 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_99 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_101 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_103 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_105 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_107 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_54 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_56 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_58 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_60 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_62 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_64 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_66 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_68 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_70 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_72 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_74 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_76 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_78 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_80 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_82 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_84 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_86 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_88 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_90 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_92 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_94 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_96 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_98 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_100 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_102 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_104 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_106 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_27 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_28 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_29 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_30 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_31 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_32 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_33 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_34 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_35 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_36 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_37 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_38 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_39 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_40 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_41 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_42 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_43 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_44 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_45 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_46 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_47 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_48 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_49 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_50 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_51 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_52 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_53 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_27 (Gl (None, 8)            0           dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_28 (Gl (None, 8)            0           dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_29 (Gl (None, 8)            0           dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_30 (Gl (None, 8)            0           dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_31 (Gl (None, 8)            0           dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_32 (Gl (None, 8)            0           dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_33 (Gl (None, 8)            0           dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_34 (Gl (None, 8)            0           dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_35 (Gl (None, 8)            0           dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_36 (Gl (None, 8)            0           dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_37 (Gl (None, 8)            0           dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_38 (Gl (None, 8)            0           dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_39 (Gl (None, 8)            0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_40 (Gl (None, 8)            0           dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_41 (Gl (None, 8)            0           dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_42 (Gl (None, 8)            0           dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_43 (Gl (None, 8)            0           dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_44 (Gl (None, 8)            0           dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_45 (Gl (None, 8)            0           dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_46 (Gl (None, 8)            0           dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_47 (Gl (None, 8)            0           dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_48 (Gl (None, 8)            0           dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_49 (Gl (None, 8)            0           dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_50 (Gl (None, 8)            0           dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_51 (Gl (None, 8)            0           dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_52 (Gl (None, 8)            0           dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_53 (Gl (None, 8)            0           dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 216)          0           global_average_pooling3d_27[0][0]\n",
            "                                                                 global_average_pooling3d_28[0][0]\n",
            "                                                                 global_average_pooling3d_29[0][0]\n",
            "                                                                 global_average_pooling3d_30[0][0]\n",
            "                                                                 global_average_pooling3d_31[0][0]\n",
            "                                                                 global_average_pooling3d_32[0][0]\n",
            "                                                                 global_average_pooling3d_33[0][0]\n",
            "                                                                 global_average_pooling3d_34[0][0]\n",
            "                                                                 global_average_pooling3d_35[0][0]\n",
            "                                                                 global_average_pooling3d_36[0][0]\n",
            "                                                                 global_average_pooling3d_37[0][0]\n",
            "                                                                 global_average_pooling3d_38[0][0]\n",
            "                                                                 global_average_pooling3d_39[0][0]\n",
            "                                                                 global_average_pooling3d_40[0][0]\n",
            "                                                                 global_average_pooling3d_41[0][0]\n",
            "                                                                 global_average_pooling3d_42[0][0]\n",
            "                                                                 global_average_pooling3d_43[0][0]\n",
            "                                                                 global_average_pooling3d_44[0][0]\n",
            "                                                                 global_average_pooling3d_45[0][0]\n",
            "                                                                 global_average_pooling3d_46[0][0]\n",
            "                                                                 global_average_pooling3d_47[0][0]\n",
            "                                                                 global_average_pooling3d_48[0][0]\n",
            "                                                                 global_average_pooling3d_49[0][0]\n",
            "                                                                 global_average_pooling3d_50[0][0]\n",
            "                                                                 global_average_pooling3d_51[0][0]\n",
            "                                                                 global_average_pooling3d_52[0][0]\n",
            "                                                                 global_average_pooling3d_53[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          111104      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 512)          262656      dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 512)          262656      dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            513         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 490ms/step - loss: 99.2194 - accuracy: 0.5610 - val_loss: 93.2410 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.24102, saving model to ./mod1.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 91.5667 - accuracy: 0.8171 - val_loss: 85.9611 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.24102 to 85.96106, saving model to ./mod1.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 84.2489 - accuracy: 0.8293 - val_loss: 78.8765 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00003: val_loss improved from 85.96106 to 78.87650, saving model to ./mod1.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 77.2937 - accuracy: 0.8293 - val_loss: 72.2065 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00004: val_loss improved from 78.87650 to 72.20648, saving model to ./mod1.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 70.6515 - accuracy: 0.8659 - val_loss: 65.8688 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.20648 to 65.86883, saving model to ./mod1.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 64.3493 - accuracy: 0.8780 - val_loss: 59.7908 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00006: val_loss improved from 65.86883 to 59.79077, saving model to ./mod1.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 58.3370 - accuracy: 0.8902 - val_loss: 54.1351 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss improved from 59.79077 to 54.13509, saving model to ./mod1.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 52.6534 - accuracy: 0.9146 - val_loss: 48.5979 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.13509 to 48.59795, saving model to ./mod1.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 47.2612 - accuracy: 0.9634 - val_loss: 43.5218 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.59795 to 43.52175, saving model to ./mod1.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 42.2261 - accuracy: 0.9390 - val_loss: 38.6611 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.52175 to 38.66110, saving model to ./mod1.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 37.4472 - accuracy: 0.9878 - val_loss: 34.1074 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.66110 to 34.10736, saving model to ./mod1.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 33.0014 - accuracy: 0.9512 - val_loss: 29.9166 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.10736 to 29.91658, saving model to ./mod1.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 28.8408 - accuracy: 0.9878 - val_loss: 25.9298 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 29.91658 to 25.92985, saving model to ./mod1.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 24.9870 - accuracy: 1.0000 - val_loss: 22.3182 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 25.92985 to 22.31821, saving model to ./mod1.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 21.4532 - accuracy: 0.9756 - val_loss: 19.0666 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.31821 to 19.06664, saving model to ./mod1.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 18.1648 - accuracy: 1.0000 - val_loss: 15.9752 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.06664 to 15.97522, saving model to ./mod1.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 15.1824 - accuracy: 1.0000 - val_loss: 13.2043 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 15.97522 to 13.20430, saving model to ./mod1.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 12.5447 - accuracy: 1.0000 - val_loss: 11.2507 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.20430 to 11.25069, saving model to ./mod1.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 10.2544 - accuracy: 0.9634 - val_loss: 9.0253 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.25069 to 9.02529, saving model to ./mod1.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 8.2770 - accuracy: 0.9268 - val_loss: 8.3443 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00020: val_loss improved from 9.02529 to 8.34430, saving model to ./mod1.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 6.6348 - accuracy: 0.9268 - val_loss: 5.4927 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00021: val_loss improved from 8.34430 to 5.49273, saving model to ./mod1.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 5.1055 - accuracy: 0.9878 - val_loss: 4.4084 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.49273 to 4.40841, saving model to ./mod1.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 3.9614 - accuracy: 0.9878 - val_loss: 3.3077 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.40841 to 3.30774, saving model to ./mod1.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 3.0543 - accuracy: 0.9878 - val_loss: 2.5740 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.30774 to 2.57397, saving model to ./mod1.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 2.4242 - accuracy: 1.0000 - val_loss: 2.2962 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.57397 to 2.29619, saving model to ./mod1.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 2.0790 - accuracy: 1.0000 - val_loss: 1.9987 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.29619 to 1.99872, saving model to ./mod1.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.9030 - accuracy: 1.0000 - val_loss: 1.7847 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.99872 to 1.78472, saving model to ./mod1.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 1.6529 - accuracy: 1.0000 - val_loss: 1.5356 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.78472 to 1.53561, saving model to ./mod1.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 1.3896 - accuracy: 1.0000 - val_loss: 1.2648 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.53561 to 1.26484, saving model to ./mod1.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 1.1489 - accuracy: 1.0000 - val_loss: 1.0786 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.26484 to 1.07859, saving model to ./mod1.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.9962 - accuracy: 1.0000 - val_loss: 1.0034 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.07859 to 1.00343, saving model to ./mod1.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.9143 - accuracy: 1.0000 - val_loss: 0.9124 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.00343 to 0.91243, saving model to ./mod1.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.8274 - accuracy: 1.0000 - val_loss: 0.9614 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.91243\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.7296 - accuracy: 1.0000 - val_loss: 0.7609 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.91243 to 0.76093, saving model to ./mod1.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.6733 - accuracy: 1.0000 - val_loss: 0.7233 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.76093 to 0.72328, saving model to ./mod1.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.6293 - accuracy: 1.0000 - val_loss: 0.7832 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.72328\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.5815 - accuracy: 1.0000 - val_loss: 0.6345 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.72328 to 0.63455, saving model to ./mod1.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.5416 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.63455 to 0.61218, saving model to ./mod1.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5491 - accuracy: 0.9878 - val_loss: 0.9416 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.61218\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.7256 - accuracy: 0.9024 - val_loss: 1.9074 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.61218\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.7861 - accuracy: 1.0000 - val_loss: 0.8562 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.61218\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.7936 - accuracy: 1.0000 - val_loss: 0.7897 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.61218\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.6786 - accuracy: 1.0000 - val_loss: 0.6694 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.61218\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.6098 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.61218 to 0.60317, saving model to ./mod1.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5539 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.60317 to 0.56953, saving model to ./mod1.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.5193 - accuracy: 1.0000 - val_loss: 0.7573 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.56953\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.5047 - accuracy: 0.9878 - val_loss: 0.5213 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.56953 to 0.52130, saving model to ./mod1.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4722 - accuracy: 1.0000 - val_loss: 0.5447 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.52130\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4524 - accuracy: 1.0000 - val_loss: 0.4862 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.52130 to 0.48621, saving model to ./mod1.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.4285 - accuracy: 1.0000 - val_loss: 0.4700 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.48621 to 0.47003, saving model to ./mod1.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4227 - accuracy: 1.0000 - val_loss: 0.4667 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.47003 to 0.46673, saving model to ./mod1.h5\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4079 - accuracy: 1.0000 - val_loss: 0.4834 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.46673\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.4075 - accuracy: 1.0000 - val_loss: 0.4500 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.46673 to 0.44997, saving model to ./mod1.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3944 - accuracy: 1.0000 - val_loss: 0.4367 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.44997 to 0.43675, saving model to ./mod1.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3938 - accuracy: 1.0000 - val_loss: 0.4654 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.43675\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3892 - accuracy: 1.0000 - val_loss: 0.4301 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.43675 to 0.43009, saving model to ./mod1.h5\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3837 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.43009 to 0.41914, saving model to ./mod1.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3798 - accuracy: 1.0000 - val_loss: 0.4157 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.41914 to 0.41574, saving model to ./mod1.h5\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3790 - accuracy: 1.0000 - val_loss: 0.4125 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.41574 to 0.41252, saving model to ./mod1.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3841 - accuracy: 1.0000 - val_loss: 0.4450 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.41252\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3847 - accuracy: 1.0000 - val_loss: 0.4104 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.41252 to 0.41043, saving model to ./mod1.h5\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3843 - accuracy: 1.0000 - val_loss: 0.4386 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.41043\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3775 - accuracy: 1.0000 - val_loss: 0.4260 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.41043\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3662 - accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.41043 to 0.40911, saving model to ./mod1.h5\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3663 - accuracy: 1.0000 - val_loss: 0.4090 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.40911 to 0.40899, saving model to ./mod1.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3634 - accuracy: 1.0000 - val_loss: 0.4209 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.40899\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3591 - accuracy: 1.0000 - val_loss: 0.4317 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.40899\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3569 - accuracy: 1.0000 - val_loss: 0.3945 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.40899 to 0.39448, saving model to ./mod1.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3597 - accuracy: 1.0000 - val_loss: 0.4218 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.39448\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3558 - accuracy: 1.0000 - val_loss: 0.4030 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.39448\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3512 - accuracy: 1.0000 - val_loss: 0.4248 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.39448\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3518 - accuracy: 1.0000 - val_loss: 0.3944 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.39448 to 0.39437, saving model to ./mod1.h5\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.3492 - accuracy: 1.0000 - val_loss: 0.4517 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.39437\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3518 - accuracy: 1.0000 - val_loss: 0.3869 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.39437 to 0.38690, saving model to ./mod1.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3472 - accuracy: 1.0000 - val_loss: 0.4266 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.38690\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3458 - accuracy: 1.0000 - val_loss: 0.3820 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.38690 to 0.38202, saving model to ./mod1.h5\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3451 - accuracy: 1.0000 - val_loss: 0.4546 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.38202\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3445 - accuracy: 1.0000 - val_loss: 0.3860 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.38202\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3436 - accuracy: 1.0000 - val_loss: 0.4162 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.38202\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3426 - accuracy: 1.0000 - val_loss: 0.5163 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.38202\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3501 - accuracy: 1.0000 - val_loss: 0.3964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.38202\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3475 - accuracy: 1.0000 - val_loss: 0.3886 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.38202\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3497 - accuracy: 1.0000 - val_loss: 0.4261 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.38202\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3981 - accuracy: 1.0000 - val_loss: 0.4986 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.38202\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4140 - accuracy: 1.0000 - val_loss: 1.0244 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.38202\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3914 - accuracy: 1.0000 - val_loss: 0.4707 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.38202\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3756 - accuracy: 1.0000 - val_loss: 0.4500 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.38202\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3627 - accuracy: 1.0000 - val_loss: 0.4535 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.38202\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3531 - accuracy: 1.0000 - val_loss: 0.3985 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.38202\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3480 - accuracy: 1.0000 - val_loss: 0.4040 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.38202\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3429 - accuracy: 1.0000 - val_loss: 0.4243 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.38202\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3392 - accuracy: 1.0000 - val_loss: 0.4095 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.38202\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3371 - accuracy: 1.0000 - val_loss: 0.4095 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.38202\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3365 - accuracy: 1.0000 - val_loss: 0.3923 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.38202\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3360 - accuracy: 1.0000 - val_loss: 0.4252 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.38202\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3360 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.38202\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3350 - accuracy: 1.0000 - val_loss: 0.4252 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.38202\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3337 - accuracy: 1.0000 - val_loss: 0.4273 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.38202\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3326 - accuracy: 1.0000 - val_loss: 0.4039 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.38202\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3328 - accuracy: 1.0000 - val_loss: 0.3835 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.38202\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3306 - accuracy: 1.0000 - val_loss: 0.4098 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.38202\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3308 - accuracy: 1.0000 - val_loss: 0.4169 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.38202\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3291 - accuracy: 1.0000 - val_loss: 0.4248 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.38202\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3275 - accuracy: 1.0000 - val_loss: 0.4096 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.38202\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3285 - accuracy: 1.0000 - val_loss: 0.3617 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.38202 to 0.36174, saving model to ./mod1.h5\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3329 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.36174\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3311 - accuracy: 1.0000 - val_loss: 0.3845 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.36174\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3290 - accuracy: 1.0000 - val_loss: 0.4270 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.36174\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3293 - accuracy: 1.0000 - val_loss: 0.3819 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.36174\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3298 - accuracy: 1.0000 - val_loss: 0.4031 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.36174\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3289 - accuracy: 1.0000 - val_loss: 0.4544 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.36174\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3283 - accuracy: 1.0000 - val_loss: 0.3707 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.36174\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3270 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.36174\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3679 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.36174\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3276 - accuracy: 1.0000 - val_loss: 0.4498 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.36174\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3253 - accuracy: 1.0000 - val_loss: 0.3751 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.36174\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3258 - accuracy: 1.0000 - val_loss: 0.3785 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.36174\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.3922 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.36174\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3239 - accuracy: 1.0000 - val_loss: 0.3881 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.36174\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3233 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.36174\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3732 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.36174\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.4441 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.36174\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3258 - accuracy: 1.0000 - val_loss: 0.3559 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.36174 to 0.35589, saving model to ./mod1.h5\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.3899 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.35589\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.3970 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.35589\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.4082 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.35589\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3230 - accuracy: 1.0000 - val_loss: 0.3915 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.35589\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.35589\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3214 - accuracy: 1.0000 - val_loss: 0.4249 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.35589\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3212 - accuracy: 1.0000 - val_loss: 0.3521 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.35589 to 0.35213, saving model to ./mod1.h5\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3227 - accuracy: 1.0000 - val_loss: 0.4097 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.35213\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3575 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.35213\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3224 - accuracy: 1.0000 - val_loss: 0.3639 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.35213\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3222 - accuracy: 1.0000 - val_loss: 0.4013 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.35213\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3227 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.35213\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.3978 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.35213\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3211 - accuracy: 1.0000 - val_loss: 0.4269 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.35213\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3206 - accuracy: 1.0000 - val_loss: 0.4068 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.35213\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3210 - accuracy: 1.0000 - val_loss: 0.4991 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.35213\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3217 - accuracy: 1.0000 - val_loss: 0.3739 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.35213\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.4323 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.35213\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3214 - accuracy: 1.0000 - val_loss: 0.3683 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.35213\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3238 - accuracy: 1.0000 - val_loss: 0.5573 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.35213\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3284 - accuracy: 1.0000 - val_loss: 0.4075 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.35213\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3266 - accuracy: 1.0000 - val_loss: 0.3889 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.35213\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3234 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.35213\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3242 - accuracy: 1.0000 - val_loss: 0.3738 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.35213\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3203 - accuracy: 1.0000 - val_loss: 0.4528 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.35213\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3189 - accuracy: 1.0000 - val_loss: 0.3943 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.35213\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3187 - accuracy: 1.0000 - val_loss: 0.4120 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.35213\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.35213\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3176 - accuracy: 1.0000 - val_loss: 0.3924 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.35213\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3172 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.35213\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.3823 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.35213\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3175 - accuracy: 1.0000 - val_loss: 0.3668 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.35213\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3167 - accuracy: 1.0000 - val_loss: 0.4227 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.35213\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3183 - accuracy: 1.0000 - val_loss: 0.3533 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.35213\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3179 - accuracy: 1.0000 - val_loss: 0.4079 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.35213\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3735 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.35213\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3535 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.35213\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3174 - accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.35213\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.3541 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.35213\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3167 - accuracy: 1.0000 - val_loss: 0.4034 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.35213\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.35213\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.3930 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.35213\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.3852 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.35213\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3881 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.35213\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.4081 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.35213\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.3636 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.35213\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.3893 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.35213\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3956 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.35213\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3138 - accuracy: 1.0000 - val_loss: 0.3628 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.35213\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.3816 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.35213\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3143 - accuracy: 1.0000 - val_loss: 0.3507 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss improved from 0.35213 to 0.35073, saving model to ./mod1.h5\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3157 - accuracy: 1.0000 - val_loss: 0.4042 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.35073\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3151 - accuracy: 1.0000 - val_loss: 0.3612 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.35073\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3158 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.35073\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3152 - accuracy: 1.0000 - val_loss: 0.3988 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.35073\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3146 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.35073\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3140 - accuracy: 1.0000 - val_loss: 0.4107 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.35073\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3149 - accuracy: 1.0000 - val_loss: 0.4046 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.35073\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3136 - accuracy: 1.0000 - val_loss: 0.4141 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.35073\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3145 - accuracy: 1.0000 - val_loss: 0.3752 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.35073\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3142 - accuracy: 1.0000 - val_loss: 0.4300 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.35073\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3152 - accuracy: 1.0000 - val_loss: 0.3837 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.35073\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.4167 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.35073\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.3639 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.35073\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3142 - accuracy: 1.0000 - val_loss: 0.4171 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.35073\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.3749 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.35073\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3135 - accuracy: 1.0000 - val_loss: 0.3866 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.35073\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3127 - accuracy: 1.0000 - val_loss: 0.4033 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.35073\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3127 - accuracy: 1.0000 - val_loss: 0.3695 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.35073\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3126 - accuracy: 1.0000 - val_loss: 0.4078 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.35073\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3120 - accuracy: 1.0000 - val_loss: 0.3815 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.35073\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3124 - accuracy: 1.0000 - val_loss: 0.4003 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.35073\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3115 - accuracy: 1.0000 - val_loss: 0.3621 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.35073\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3133 - accuracy: 1.0000 - val_loss: 0.3694 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.35073\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3135 - accuracy: 1.0000 - val_loss: 0.3989 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.35073\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3129 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.35073\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3125 - accuracy: 1.0000 - val_loss: 0.3904 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.35073\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 13ms/step - loss: 0.3149 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.1262 - accuracy: 0.8400\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3507 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_109 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_111 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_113 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_115 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_117 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_119 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_121 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_123 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_125 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_127 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_129 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_131 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_133 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_135 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_137 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_139 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_141 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_143 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_145 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_147 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_149 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_151 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_153 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_155 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_157 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_159 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_161 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_108 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_110 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_112 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_114 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_116 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_118 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_120 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_122 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_124 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_126 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_128 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_130 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_132 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_134 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_136 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_138 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_140 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_142 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_144 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_146 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_148 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_150 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_152 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_154 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_156 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_158 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_160 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_54 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_55 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_56 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_57 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_58 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_59 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_60 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_61 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_62 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_63 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_64 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_65 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_66 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_67 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_68 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_69 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_70 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_71 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_72 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_73 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_74 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_75 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_76 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_77 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_78 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_79 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_80 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_65 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_67 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_71 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_72 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_73 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_76 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_77 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_79 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_80 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_54 (Gl (None, 8)            0           dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_55 (Gl (None, 8)            0           dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_56 (Gl (None, 8)            0           dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_57 (Gl (None, 8)            0           dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_58 (Gl (None, 8)            0           dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_59 (Gl (None, 8)            0           dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_60 (Gl (None, 8)            0           dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_61 (Gl (None, 8)            0           dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_62 (Gl (None, 8)            0           dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_63 (Gl (None, 8)            0           dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_64 (Gl (None, 8)            0           dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_65 (Gl (None, 8)            0           dropout_65[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_66 (Gl (None, 8)            0           dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_67 (Gl (None, 8)            0           dropout_67[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_68 (Gl (None, 8)            0           dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_69 (Gl (None, 8)            0           dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_70 (Gl (None, 8)            0           dropout_70[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_71 (Gl (None, 8)            0           dropout_71[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_72 (Gl (None, 8)            0           dropout_72[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_73 (Gl (None, 8)            0           dropout_73[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_74 (Gl (None, 8)            0           dropout_74[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_75 (Gl (None, 8)            0           dropout_75[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_76 (Gl (None, 8)            0           dropout_76[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_77 (Gl (None, 8)            0           dropout_77[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_78 (Gl (None, 8)            0           dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_79 (Gl (None, 8)            0           dropout_79[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_80 (Gl (None, 8)            0           dropout_80[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 216)          0           global_average_pooling3d_54[0][0]\n",
            "                                                                 global_average_pooling3d_55[0][0]\n",
            "                                                                 global_average_pooling3d_56[0][0]\n",
            "                                                                 global_average_pooling3d_57[0][0]\n",
            "                                                                 global_average_pooling3d_58[0][0]\n",
            "                                                                 global_average_pooling3d_59[0][0]\n",
            "                                                                 global_average_pooling3d_60[0][0]\n",
            "                                                                 global_average_pooling3d_61[0][0]\n",
            "                                                                 global_average_pooling3d_62[0][0]\n",
            "                                                                 global_average_pooling3d_63[0][0]\n",
            "                                                                 global_average_pooling3d_64[0][0]\n",
            "                                                                 global_average_pooling3d_65[0][0]\n",
            "                                                                 global_average_pooling3d_66[0][0]\n",
            "                                                                 global_average_pooling3d_67[0][0]\n",
            "                                                                 global_average_pooling3d_68[0][0]\n",
            "                                                                 global_average_pooling3d_69[0][0]\n",
            "                                                                 global_average_pooling3d_70[0][0]\n",
            "                                                                 global_average_pooling3d_71[0][0]\n",
            "                                                                 global_average_pooling3d_72[0][0]\n",
            "                                                                 global_average_pooling3d_73[0][0]\n",
            "                                                                 global_average_pooling3d_74[0][0]\n",
            "                                                                 global_average_pooling3d_75[0][0]\n",
            "                                                                 global_average_pooling3d_76[0][0]\n",
            "                                                                 global_average_pooling3d_77[0][0]\n",
            "                                                                 global_average_pooling3d_78[0][0]\n",
            "                                                                 global_average_pooling3d_79[0][0]\n",
            "                                                                 global_average_pooling3d_80[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          111104      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 512)          262656      dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          262656      dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            513         dense_10[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 372ms/step - loss: 99.4793 - accuracy: 0.4512 - val_loss: 93.5406 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.54060, saving model to ./mod2.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 91.8115 - accuracy: 0.8171 - val_loss: 86.1878 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.54060 to 86.18778, saving model to ./mod2.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 84.5409 - accuracy: 0.7683 - val_loss: 79.1948 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.18778 to 79.19482, saving model to ./mod2.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 77.6053 - accuracy: 0.8659 - val_loss: 72.6224 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.19482 to 72.62244, saving model to ./mod2.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 70.9789 - accuracy: 0.8537 - val_loss: 66.2072 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.62244 to 66.20719, saving model to ./mod2.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 64.6922 - accuracy: 0.9146 - val_loss: 60.2134 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.20719 to 60.21340, saving model to ./mod2.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 58.6960 - accuracy: 0.8659 - val_loss: 54.5092 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.21340 to 54.50919, saving model to ./mod2.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 53.0247 - accuracy: 0.8537 - val_loss: 48.9574 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.50919 to 48.95743, saving model to ./mod2.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 47.6538 - accuracy: 0.9390 - val_loss: 43.8310 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.95743 to 43.83098, saving model to ./mod2.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 42.5528 - accuracy: 0.9756 - val_loss: 39.1184 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.83098 to 39.11836, saving model to ./mod2.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 37.8187 - accuracy: 0.9390 - val_loss: 34.4526 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00011: val_loss improved from 39.11836 to 34.45263, saving model to ./mod2.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 33.3330 - accuracy: 0.9756 - val_loss: 30.2144 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.45263 to 30.21439, saving model to ./mod2.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 29.1224 - accuracy: 0.9878 - val_loss: 26.2761 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.21439 to 26.27611, saving model to ./mod2.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 25.2559 - accuracy: 0.9878 - val_loss: 22.6593 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.27611 to 22.65933, saving model to ./mod2.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 21.6838 - accuracy: 0.9878 - val_loss: 19.2818 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.65933 to 19.28185, saving model to ./mod2.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 18.3972 - accuracy: 0.9878 - val_loss: 16.2349 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.28185 to 16.23490, saving model to ./mod2.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 15.3883 - accuracy: 1.0000 - val_loss: 13.4493 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.23490 to 13.44928, saving model to ./mod2.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 12.7162 - accuracy: 1.0000 - val_loss: 11.1255 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.44928 to 11.12552, saving model to ./mod2.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 10.3243 - accuracy: 0.9878 - val_loss: 8.8199 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.12552 to 8.81991, saving model to ./mod2.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 8.2292 - accuracy: 1.0000 - val_loss: 7.1266 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.81991 to 7.12660, saving model to ./mod2.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 6.4347 - accuracy: 1.0000 - val_loss: 5.3876 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.12660 to 5.38763, saving model to ./mod2.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 4.9336 - accuracy: 1.0000 - val_loss: 4.1125 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.38763 to 4.11250, saving model to ./mod2.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 3.7479 - accuracy: 1.0000 - val_loss: 3.5742 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.11250 to 3.57422, saving model to ./mod2.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 3.2871 - accuracy: 0.7561 - val_loss: 3.0280 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.57422 to 3.02796, saving model to ./mod2.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 2.5549 - accuracy: 0.9024 - val_loss: 2.5116 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00025: val_loss improved from 3.02796 to 2.51157, saving model to ./mod2.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 2.3763 - accuracy: 0.9512 - val_loss: 2.2993 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.51157 to 2.29927, saving model to ./mod2.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 2.0834 - accuracy: 1.0000 - val_loss: 2.5374 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.29927\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 1.9250 - accuracy: 0.9390 - val_loss: 1.7374 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.29927 to 1.73736, saving model to ./mod2.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 1.5201 - accuracy: 1.0000 - val_loss: 1.4613 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.73736 to 1.46131, saving model to ./mod2.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 1.2814 - accuracy: 0.9878 - val_loss: 1.2587 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.46131 to 1.25873, saving model to ./mod2.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 1.0876 - accuracy: 0.9878 - val_loss: 1.1597 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.25873 to 1.15969, saving model to ./mod2.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.9803 - accuracy: 1.0000 - val_loss: 1.0357 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.15969 to 1.03573, saving model to ./mod2.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.8848 - accuracy: 1.0000 - val_loss: 1.0165 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.03573 to 1.01653, saving model to ./mod2.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.7845 - accuracy: 1.0000 - val_loss: 0.8617 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.01653 to 0.86166, saving model to ./mod2.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.7077 - accuracy: 1.0000 - val_loss: 0.8130 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.86166 to 0.81303, saving model to ./mod2.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.6639 - accuracy: 1.0000 - val_loss: 0.7874 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.81303 to 0.78743, saving model to ./mod2.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.6096 - accuracy: 1.0000 - val_loss: 0.6888 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.78743 to 0.68883, saving model to ./mod2.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.5757 - accuracy: 1.0000 - val_loss: 0.7286 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68883\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.5381 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.68883 to 0.65167, saving model to ./mod2.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5064 - accuracy: 1.0000 - val_loss: 0.6163 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.65167 to 0.61630, saving model to ./mod2.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4837 - accuracy: 1.0000 - val_loss: 0.6309 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.61630\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4700 - accuracy: 1.0000 - val_loss: 0.5745 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.61630 to 0.57454, saving model to ./mod2.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4493 - accuracy: 1.0000 - val_loss: 0.5743 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.57454 to 0.57432, saving model to ./mod2.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.4441 - accuracy: 1.0000 - val_loss: 0.5322 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.57432 to 0.53217, saving model to ./mod2.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.4319 - accuracy: 1.0000 - val_loss: 0.5588 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.53217\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4210 - accuracy: 1.0000 - val_loss: 0.5058 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.53217 to 0.50581, saving model to ./mod2.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4146 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.50581\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4125 - accuracy: 1.0000 - val_loss: 0.5020 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.50581 to 0.50202, saving model to ./mod2.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4052 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.50202\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3947 - accuracy: 1.0000 - val_loss: 0.6216 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.50202\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4341 - accuracy: 1.0000 - val_loss: 0.5304 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.50202\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.4703 - accuracy: 1.0000 - val_loss: 0.5350 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.50202\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4485 - accuracy: 1.0000 - val_loss: 0.6478 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.50202\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4254 - accuracy: 1.0000 - val_loss: 0.4879 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.50202 to 0.48786, saving model to ./mod2.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4131 - accuracy: 1.0000 - val_loss: 0.4897 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.48786\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3974 - accuracy: 1.0000 - val_loss: 0.5111 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.48786\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3942 - accuracy: 1.0000 - val_loss: 0.4801 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.48786 to 0.48012, saving model to ./mod2.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3809 - accuracy: 1.0000 - val_loss: 0.4884 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.48012\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3750 - accuracy: 1.0000 - val_loss: 0.4512 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.48012 to 0.45121, saving model to ./mod2.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3717 - accuracy: 1.0000 - val_loss: 0.4364 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.45121 to 0.43638, saving model to ./mod2.h5\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3716 - accuracy: 1.0000 - val_loss: 0.4655 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.43638\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3699 - accuracy: 1.0000 - val_loss: 0.5149 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.43638\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3681 - accuracy: 1.0000 - val_loss: 0.4505 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.43638\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3669 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.43638\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3807 - accuracy: 1.0000 - val_loss: 0.4225 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.43638 to 0.42251, saving model to ./mod2.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3747 - accuracy: 1.0000 - val_loss: 0.4281 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.42251\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3633 - accuracy: 1.0000 - val_loss: 0.4874 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.42251\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3594 - accuracy: 1.0000 - val_loss: 0.4326 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.42251\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3561 - accuracy: 1.0000 - val_loss: 0.4515 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.42251\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3571 - accuracy: 1.0000 - val_loss: 0.4493 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.42251\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3510 - accuracy: 1.0000 - val_loss: 0.4497 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.42251\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3489 - accuracy: 1.0000 - val_loss: 0.4396 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.42251\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3488 - accuracy: 1.0000 - val_loss: 0.4211 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.42251 to 0.42108, saving model to ./mod2.h5\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3449 - accuracy: 1.0000 - val_loss: 0.4443 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.42108\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3467 - accuracy: 1.0000 - val_loss: 0.4152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.42108 to 0.41523, saving model to ./mod2.h5\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3435 - accuracy: 1.0000 - val_loss: 0.4304 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.41523\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3456 - accuracy: 1.0000 - val_loss: 0.4378 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.41523\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3432 - accuracy: 1.0000 - val_loss: 0.4028 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.41523 to 0.40279, saving model to ./mod2.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3441 - accuracy: 1.0000 - val_loss: 0.4253 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.40279\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3403 - accuracy: 1.0000 - val_loss: 0.4181 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.40279\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3421 - accuracy: 1.0000 - val_loss: 0.4298 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.40279\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3390 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.40279\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3400 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.40279\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3424 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.40279 to 0.39925, saving model to ./mod2.h5\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3386 - accuracy: 1.0000 - val_loss: 0.4351 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.39925\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3377 - accuracy: 1.0000 - val_loss: 0.4015 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.39925\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3360 - accuracy: 1.0000 - val_loss: 0.4079 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.39925\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3349 - accuracy: 1.0000 - val_loss: 0.4142 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.39925\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3346 - accuracy: 1.0000 - val_loss: 0.3894 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.39925 to 0.38936, saving model to ./mod2.h5\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3323 - accuracy: 1.0000 - val_loss: 0.3994 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.38936\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3324 - accuracy: 1.0000 - val_loss: 0.3989 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.38936\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3314 - accuracy: 1.0000 - val_loss: 0.4099 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.38936\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3303 - accuracy: 1.0000 - val_loss: 0.3859 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.38936 to 0.38587, saving model to ./mod2.h5\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3328 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.38587\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3317 - accuracy: 1.0000 - val_loss: 0.3853 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.38587 to 0.38527, saving model to ./mod2.h5\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3328 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.38527\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3341 - accuracy: 1.0000 - val_loss: 0.3964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.38527\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3305 - accuracy: 1.0000 - val_loss: 0.3905 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.38527\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3297 - accuracy: 1.0000 - val_loss: 0.3876 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.38527\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3982 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.38527\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3289 - accuracy: 1.0000 - val_loss: 0.3845 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.38527 to 0.38447, saving model to ./mod2.h5\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3277 - accuracy: 1.0000 - val_loss: 0.4254 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.38447\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.4007 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.38447\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.4035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.38447\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3279 - accuracy: 1.0000 - val_loss: 0.4003 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.38447\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3262 - accuracy: 1.0000 - val_loss: 0.3960 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.38447\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3254 - accuracy: 1.0000 - val_loss: 0.3927 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.38447\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3239 - accuracy: 1.0000 - val_loss: 0.3921 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.38447\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3239 - accuracy: 1.0000 - val_loss: 0.3949 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.38447\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3254 - accuracy: 1.0000 - val_loss: 0.3737 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.38447 to 0.37373, saving model to ./mod2.h5\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3252 - accuracy: 1.0000 - val_loss: 0.3928 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.37373\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3242 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.37373\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3236 - accuracy: 1.0000 - val_loss: 0.3956 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.37373\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3235 - accuracy: 1.0000 - val_loss: 0.3955 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.37373\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3230 - accuracy: 1.0000 - val_loss: 0.3900 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.37373\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3227 - accuracy: 1.0000 - val_loss: 0.3628 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.37373 to 0.36275, saving model to ./mod2.h5\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3280 - accuracy: 1.0000 - val_loss: 0.3745 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.36275\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3255 - accuracy: 1.0000 - val_loss: 0.3948 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.36275\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3241 - accuracy: 1.0000 - val_loss: 0.3787 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.36275\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3246 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.36275\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3236 - accuracy: 1.0000 - val_loss: 0.3892 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.36275\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3223 - accuracy: 1.0000 - val_loss: 0.3731 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.36275\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3256 - accuracy: 1.0000 - val_loss: 0.4223 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.36275\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 0.3996 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.36275\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.3787 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.36275\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3223 - accuracy: 1.0000 - val_loss: 0.3949 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.36275\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 0.3586 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.36275 to 0.35862, saving model to ./mod2.h5\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3241 - accuracy: 1.0000 - val_loss: 0.3932 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.35862\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.3948 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.35862\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3221 - accuracy: 1.0000 - val_loss: 0.3796 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.35862\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3935 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.35862\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3202 - accuracy: 1.0000 - val_loss: 0.3940 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.35862\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.3668 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.35862\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3205 - accuracy: 1.0000 - val_loss: 0.4202 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.35862\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3204 - accuracy: 1.0000 - val_loss: 0.3643 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.35862\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3193 - accuracy: 1.0000 - val_loss: 0.3858 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.35862\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.3541 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.35862 to 0.35408, saving model to ./mod2.h5\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3910 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.35408\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3557 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.35408\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.3631 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.35408\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3692 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.35408\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.3926 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.35408\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3191 - accuracy: 1.0000 - val_loss: 0.3760 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.35408\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.35408\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3175 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.35408\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3170 - accuracy: 1.0000 - val_loss: 0.3799 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.35408\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3173 - accuracy: 1.0000 - val_loss: 0.3768 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.35408\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3168 - accuracy: 1.0000 - val_loss: 0.3902 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.35408\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3178 - accuracy: 1.0000 - val_loss: 0.3941 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.35408\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3876 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.35408\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3762 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.35408\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.35408\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3179 - accuracy: 1.0000 - val_loss: 0.3807 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.35408\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3165 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.35408\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3179 - accuracy: 1.0000 - val_loss: 0.3898 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.35408\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.3680 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.35408\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3180 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.35408\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3520 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.35408 to 0.35197, saving model to ./mod2.h5\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.4444 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.35197\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.3645 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.35197\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.4158 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.35197\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3289 - accuracy: 1.0000 - val_loss: 0.4345 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.35197\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3306 - accuracy: 1.0000 - val_loss: 0.3546 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.35197\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3308 - accuracy: 1.0000 - val_loss: 0.3739 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.35197\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3290 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.35197\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3253 - accuracy: 1.0000 - val_loss: 0.3487 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.35197 to 0.34871, saving model to ./mod2.h5\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3222 - accuracy: 1.0000 - val_loss: 0.3830 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.34871\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3191 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.34871\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3180 - accuracy: 1.0000 - val_loss: 0.3535 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.34871\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.3785 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.34871\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3606 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.34871\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3138 - accuracy: 1.0000 - val_loss: 0.3802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.34871\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3133 - accuracy: 1.0000 - val_loss: 0.3528 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.34871\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3138 - accuracy: 1.0000 - val_loss: 0.3591 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.34871\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.34871\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3159 - accuracy: 1.0000 - val_loss: 0.3679 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.34871\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3196 - accuracy: 1.0000 - val_loss: 0.4067 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.34871\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3486 - accuracy: 0.9878 - val_loss: 2.4085 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.34871\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.6634 - accuracy: 0.9512 - val_loss: 0.8731 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.34871\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.8893 - accuracy: 1.0000 - val_loss: 0.9252 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.34871\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.9181 - accuracy: 1.0000 - val_loss: 0.8819 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.34871\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.8619 - accuracy: 1.0000 - val_loss: 0.8653 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.34871\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.7822 - accuracy: 1.0000 - val_loss: 0.8329 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.34871\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.7191 - accuracy: 0.9878 - val_loss: 0.6490 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.34871\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.6347 - accuracy: 1.0000 - val_loss: 0.6652 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.34871\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.5805 - accuracy: 1.0000 - val_loss: 0.8960 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.34871\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5313 - accuracy: 1.0000 - val_loss: 1.0919 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.34871\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4912 - accuracy: 1.0000 - val_loss: 1.1800 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.34871\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4603 - accuracy: 1.0000 - val_loss: 1.2259 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.34871\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.4431 - accuracy: 0.9878 - val_loss: 0.5162 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.34871\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4402 - accuracy: 1.0000 - val_loss: 0.4315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.34871\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4252 - accuracy: 1.0000 - val_loss: 0.4666 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.34871\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4098 - accuracy: 1.0000 - val_loss: 0.6799 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.34871\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4503 - accuracy: 0.9756 - val_loss: 0.6651 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.34871\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.5257 - accuracy: 0.9878 - val_loss: 1.9871 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.34871\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.6227 - accuracy: 0.9878 - val_loss: 1.4829 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.34871\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.6448 - accuracy: 1.0000 - val_loss: 0.7146 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.34871\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.6318 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.34871\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.5975 - accuracy: 1.0000 - val_loss: 0.5669 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.34871\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5569 - accuracy: 1.0000 - val_loss: 0.5286 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.34871\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 13ms/step - loss: 0.3229 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.5600 - accuracy: 0.8800\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3487 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_163 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_165 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_167 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_169 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_171 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_173 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_175 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_177 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_179 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_181 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_183 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_185 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_187 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_189 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_191 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_193 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_195 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_197 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_199 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_201 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_203 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_205 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_207 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_209 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_211 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_213 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_215 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_162 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_164 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_166 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_168 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_170 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_172 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_174 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_176 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_178 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_180 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_182 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_184 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_186 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_188 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_190 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_192 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_194 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_196 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_198 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_200 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_202 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_204 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_206 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_208 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_210 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_212 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_214 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_81 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_82 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_83 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_84 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_85 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_86 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_87 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_88 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_89 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_90 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_91 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_92 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_93 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_94 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_95 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_96 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_97 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_98 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_99 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_100 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_101 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_102 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_103 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_104 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_105 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_106 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_107 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_81 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_82 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_83 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_84 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_85 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_86 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_87 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_88 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_89 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_90 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_91 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_92 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_93 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_94 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_95 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_96 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_97 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_98 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_99 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_100 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_101 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_102 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_103 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_104 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_105 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_106 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_107 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_81 (Gl (None, 8)            0           dropout_81[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_82 (Gl (None, 8)            0           dropout_82[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_83 (Gl (None, 8)            0           dropout_83[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_84 (Gl (None, 8)            0           dropout_84[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_85 (Gl (None, 8)            0           dropout_85[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_86 (Gl (None, 8)            0           dropout_86[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_87 (Gl (None, 8)            0           dropout_87[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_88 (Gl (None, 8)            0           dropout_88[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_89 (Gl (None, 8)            0           dropout_89[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_90 (Gl (None, 8)            0           dropout_90[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_91 (Gl (None, 8)            0           dropout_91[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_92 (Gl (None, 8)            0           dropout_92[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_93 (Gl (None, 8)            0           dropout_93[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_94 (Gl (None, 8)            0           dropout_94[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_95 (Gl (None, 8)            0           dropout_95[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_96 (Gl (None, 8)            0           dropout_96[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_97 (Gl (None, 8)            0           dropout_97[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_98 (Gl (None, 8)            0           dropout_98[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_99 (Gl (None, 8)            0           dropout_99[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_100 (G (None, 8)            0           dropout_100[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_101 (G (None, 8)            0           dropout_101[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_102 (G (None, 8)            0           dropout_102[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_103 (G (None, 8)            0           dropout_103[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_104 (G (None, 8)            0           dropout_104[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_105 (G (None, 8)            0           dropout_105[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_106 (G (None, 8)            0           dropout_106[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_107 (G (None, 8)            0           dropout_107[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 216)          0           global_average_pooling3d_81[0][0]\n",
            "                                                                 global_average_pooling3d_82[0][0]\n",
            "                                                                 global_average_pooling3d_83[0][0]\n",
            "                                                                 global_average_pooling3d_84[0][0]\n",
            "                                                                 global_average_pooling3d_85[0][0]\n",
            "                                                                 global_average_pooling3d_86[0][0]\n",
            "                                                                 global_average_pooling3d_87[0][0]\n",
            "                                                                 global_average_pooling3d_88[0][0]\n",
            "                                                                 global_average_pooling3d_89[0][0]\n",
            "                                                                 global_average_pooling3d_90[0][0]\n",
            "                                                                 global_average_pooling3d_91[0][0]\n",
            "                                                                 global_average_pooling3d_92[0][0]\n",
            "                                                                 global_average_pooling3d_93[0][0]\n",
            "                                                                 global_average_pooling3d_94[0][0]\n",
            "                                                                 global_average_pooling3d_95[0][0]\n",
            "                                                                 global_average_pooling3d_96[0][0]\n",
            "                                                                 global_average_pooling3d_97[0][0]\n",
            "                                                                 global_average_pooling3d_98[0][0]\n",
            "                                                                 global_average_pooling3d_99[0][0]\n",
            "                                                                 global_average_pooling3d_100[0][0\n",
            "                                                                 global_average_pooling3d_101[0][0\n",
            "                                                                 global_average_pooling3d_102[0][0\n",
            "                                                                 global_average_pooling3d_103[0][0\n",
            "                                                                 global_average_pooling3d_104[0][0\n",
            "                                                                 global_average_pooling3d_105[0][0\n",
            "                                                                 global_average_pooling3d_106[0][0\n",
            "                                                                 global_average_pooling3d_107[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 512)          111104      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 512)          262656      dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 512)          262656      dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1)            513         dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 390ms/step - loss: 99.4353 - accuracy: 0.5244 - val_loss: 93.6186 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.61864, saving model to ./mod3.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 91.7885 - accuracy: 0.6098 - val_loss: 86.1528 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.61864 to 86.15282, saving model to ./mod3.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 84.5053 - accuracy: 0.6951 - val_loss: 79.1496 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.15282 to 79.14956, saving model to ./mod3.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 77.5991 - accuracy: 0.7683 - val_loss: 72.6795 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.14956 to 72.67950, saving model to ./mod3.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 71.0057 - accuracy: 0.7439 - val_loss: 66.1728 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.67950 to 66.17284, saving model to ./mod3.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 64.7149 - accuracy: 0.8659 - val_loss: 60.0636 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.17284 to 60.06356, saving model to ./mod3.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 58.7317 - accuracy: 0.8780 - val_loss: 54.3619 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.06356 to 54.36186, saving model to ./mod3.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 53.0293 - accuracy: 0.9268 - val_loss: 48.8383 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.36186 to 48.83829, saving model to ./mod3.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 47.6640 - accuracy: 0.9146 - val_loss: 43.6912 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.83829 to 43.69125, saving model to ./mod3.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 42.5816 - accuracy: 0.9268 - val_loss: 38.8669 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.69125 to 38.86693, saving model to ./mod3.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 37.7900 - accuracy: 0.9634 - val_loss: 34.2915 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.86693 to 34.29155, saving model to ./mod3.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 33.3188 - accuracy: 0.9390 - val_loss: 30.0885 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.29155 to 30.08851, saving model to ./mod3.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 29.1226 - accuracy: 0.9878 - val_loss: 26.0641 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.08851 to 26.06406, saving model to ./mod3.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 25.2152 - accuracy: 0.9878 - val_loss: 22.4542 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.06406 to 22.45418, saving model to ./mod3.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 21.6359 - accuracy: 1.0000 - val_loss: 19.0836 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.45418 to 19.08357, saving model to ./mod3.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 18.3603 - accuracy: 1.0000 - val_loss: 16.0575 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.08357 to 16.05753, saving model to ./mod3.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 15.3654 - accuracy: 1.0000 - val_loss: 13.2683 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.05753 to 13.26833, saving model to ./mod3.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 12.7242 - accuracy: 0.9634 - val_loss: 10.8811 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.26833 to 10.88106, saving model to ./mod3.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 10.4162 - accuracy: 0.9512 - val_loss: 8.7460 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 10.88106 to 8.74596, saving model to ./mod3.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 8.5151 - accuracy: 0.8415 - val_loss: 7.0129 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.74596 to 7.01292, saving model to ./mod3.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 6.6706 - accuracy: 0.9634 - val_loss: 5.5070 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.01292 to 5.50701, saving model to ./mod3.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 5.1489 - accuracy: 0.9878 - val_loss: 4.2456 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.50701 to 4.24560, saving model to ./mod3.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 4.0008 - accuracy: 0.9878 - val_loss: 3.2496 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.24560 to 3.24957, saving model to ./mod3.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 3.0717 - accuracy: 1.0000 - val_loss: 2.5887 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.24957 to 2.58867, saving model to ./mod3.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 2.4435 - accuracy: 1.0000 - val_loss: 2.1478 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.58867 to 2.14781, saving model to ./mod3.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 2.0941 - accuracy: 1.0000 - val_loss: 2.0625 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.14781 to 2.06248, saving model to ./mod3.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.9335 - accuracy: 1.0000 - val_loss: 1.7526 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.06248 to 1.75260, saving model to ./mod3.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.6947 - accuracy: 1.0000 - val_loss: 1.5938 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.75260 to 1.59384, saving model to ./mod3.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.4609 - accuracy: 0.9756 - val_loss: 1.2610 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.59384 to 1.26095, saving model to ./mod3.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 1.2060 - accuracy: 1.0000 - val_loss: 1.1160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.26095 to 1.11605, saving model to ./mod3.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 1.0616 - accuracy: 1.0000 - val_loss: 0.9803 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.11605 to 0.98026, saving model to ./mod3.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.9736 - accuracy: 1.0000 - val_loss: 0.8831 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.98026 to 0.88312, saving model to ./mod3.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.8430 - accuracy: 1.0000 - val_loss: 0.7910 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.88312 to 0.79101, saving model to ./mod3.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.7506 - accuracy: 1.0000 - val_loss: 0.8186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.79101\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.7292 - accuracy: 1.0000 - val_loss: 0.6959 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.79101 to 0.69585, saving model to ./mod3.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.7016 - accuracy: 1.0000 - val_loss: 0.7847 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69585\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.7227 - accuracy: 0.9878 - val_loss: 0.6464 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.69585 to 0.64641, saving model to ./mod3.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.6366 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.64641 to 0.58500, saving model to ./mod3.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5832 - accuracy: 1.0000 - val_loss: 0.6802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.58500\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.5625 - accuracy: 1.0000 - val_loss: 0.5291 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.58500 to 0.52914, saving model to ./mod3.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5271 - accuracy: 1.0000 - val_loss: 0.5704 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.52914\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5021 - accuracy: 1.0000 - val_loss: 0.4842 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.52914 to 0.48422, saving model to ./mod3.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4766 - accuracy: 1.0000 - val_loss: 0.4763 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.48422 to 0.47627, saving model to ./mod3.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4606 - accuracy: 1.0000 - val_loss: 0.4760 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.47627 to 0.47601, saving model to ./mod3.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4482 - accuracy: 1.0000 - val_loss: 0.4498 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.47601 to 0.44976, saving model to ./mod3.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.4331 - accuracy: 1.0000 - val_loss: 0.4823 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.44976\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4288 - accuracy: 1.0000 - val_loss: 0.4326 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.44976 to 0.43264, saving model to ./mod3.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4266 - accuracy: 1.0000 - val_loss: 0.4473 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.43264\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4140 - accuracy: 1.0000 - val_loss: 0.4222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.43264 to 0.42225, saving model to ./mod3.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4059 - accuracy: 1.0000 - val_loss: 0.4412 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.42225\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4023 - accuracy: 1.0000 - val_loss: 0.4103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.42225 to 0.41035, saving model to ./mod3.h5\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3964 - accuracy: 1.0000 - val_loss: 0.4059 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.41035 to 0.40590, saving model to ./mod3.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3885 - accuracy: 1.0000 - val_loss: 0.4002 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.40590 to 0.40023, saving model to ./mod3.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3859 - accuracy: 1.0000 - val_loss: 0.3986 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.40023 to 0.39860, saving model to ./mod3.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3809 - accuracy: 1.0000 - val_loss: 0.3934 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.39860 to 0.39337, saving model to ./mod3.h5\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3779 - accuracy: 1.0000 - val_loss: 0.3919 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.39337 to 0.39194, saving model to ./mod3.h5\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3766 - accuracy: 1.0000 - val_loss: 0.3885 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.39194 to 0.38849, saving model to ./mod3.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3746 - accuracy: 1.0000 - val_loss: 0.3920 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.38849\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3748 - accuracy: 1.0000 - val_loss: 0.3840 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.38849 to 0.38400, saving model to ./mod3.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3715 - accuracy: 1.0000 - val_loss: 0.3866 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.38400\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3699 - accuracy: 1.0000 - val_loss: 0.3773 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.38400 to 0.37734, saving model to ./mod3.h5\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3701 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.37734 to 0.37590, saving model to ./mod3.h5\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3645 - accuracy: 1.0000 - val_loss: 0.3712 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.37590 to 0.37115, saving model to ./mod3.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3605 - accuracy: 1.0000 - val_loss: 0.3712 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.37115\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3718 - accuracy: 1.0000 - val_loss: 0.4186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.37115\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3761 - accuracy: 1.0000 - val_loss: 0.3761 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.37115\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3667 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.37115\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3614 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.37115 to 0.37012, saving model to ./mod3.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3620 - accuracy: 1.0000 - val_loss: 0.3709 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.37012\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3581 - accuracy: 1.0000 - val_loss: 0.3645 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.37012 to 0.36450, saving model to ./mod3.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3686 - accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.36450\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3620 - accuracy: 1.0000 - val_loss: 0.3657 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.36450\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3586 - accuracy: 1.0000 - val_loss: 0.3837 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.36450\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3574 - accuracy: 1.0000 - val_loss: 0.3616 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.36450 to 0.36160, saving model to ./mod3.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3567 - accuracy: 1.0000 - val_loss: 0.3756 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.36160\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3513 - accuracy: 1.0000 - val_loss: 0.3568 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.36160 to 0.35681, saving model to ./mod3.h5\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3507 - accuracy: 1.0000 - val_loss: 0.3572 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.35681\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3489 - accuracy: 1.0000 - val_loss: 0.3543 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.35681 to 0.35431, saving model to ./mod3.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3460 - accuracy: 1.0000 - val_loss: 0.3556 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.35431\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3427 - accuracy: 1.0000 - val_loss: 0.3504 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.35431 to 0.35036, saving model to ./mod3.h5\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3447 - accuracy: 1.0000 - val_loss: 0.3604 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.35036\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3493 - accuracy: 1.0000 - val_loss: 0.3569 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.35036\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3489 - accuracy: 1.0000 - val_loss: 0.3661 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.35036\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3469 - accuracy: 1.0000 - val_loss: 0.3548 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.35036\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3519 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.35036\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3507 - accuracy: 1.0000 - val_loss: 0.3515 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.35036\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3440 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.35036\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3418 - accuracy: 1.0000 - val_loss: 0.3511 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.35036\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3388 - accuracy: 1.0000 - val_loss: 0.3448 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.35036 to 0.34481, saving model to ./mod3.h5\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3388 - accuracy: 1.0000 - val_loss: 0.3441 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.34481 to 0.34409, saving model to ./mod3.h5\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3386 - accuracy: 1.0000 - val_loss: 0.3465 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.34409\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3385 - accuracy: 1.0000 - val_loss: 0.3420 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.34409 to 0.34197, saving model to ./mod3.h5\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3368 - accuracy: 1.0000 - val_loss: 0.3557 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.34197\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3410 - accuracy: 1.0000 - val_loss: 0.3451 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.34197\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3423 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.34197\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3410 - accuracy: 1.0000 - val_loss: 0.3430 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.34197\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3403 - accuracy: 1.0000 - val_loss: 0.3466 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.34197\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3376 - accuracy: 1.0000 - val_loss: 0.3406 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.34197 to 0.34064, saving model to ./mod3.h5\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3404 - accuracy: 1.0000 - val_loss: 0.3384 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.34064 to 0.33839, saving model to ./mod3.h5\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3348 - accuracy: 1.0000 - val_loss: 0.3420 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.33839\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3337 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.33839\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3323 - accuracy: 1.0000 - val_loss: 0.3381 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.33839 to 0.33811, saving model to ./mod3.h5\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3318 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.33811 to 0.33702, saving model to ./mod3.h5\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3326 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.33702 to 0.33615, saving model to ./mod3.h5\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3320 - accuracy: 1.0000 - val_loss: 0.3394 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.33615\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3303 - accuracy: 1.0000 - val_loss: 0.3354 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.33615 to 0.33544, saving model to ./mod3.h5\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3328 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.33544 to 0.33284, saving model to ./mod3.h5\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3269 - accuracy: 1.0000 - val_loss: 0.3344 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.33284\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.33284 to 0.33253, saving model to ./mod3.h5\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3276 - accuracy: 1.0000 - val_loss: 0.3354 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.33253\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3276 - accuracy: 1.0000 - val_loss: 0.3329 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.33253\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3271 - accuracy: 1.0000 - val_loss: 0.3315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.33253 to 0.33153, saving model to ./mod3.h5\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3280 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.33153\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3280 - accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.33153\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3276 - accuracy: 1.0000 - val_loss: 0.3343 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.33153\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3270 - accuracy: 1.0000 - val_loss: 0.3320 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.33153\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3258 - accuracy: 1.0000 - val_loss: 0.3304 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.33153 to 0.33042, saving model to ./mod3.h5\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3265 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.33042\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3262 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.33042 to 0.33001, saving model to ./mod3.h5\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.3326 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.33001\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3242 - accuracy: 1.0000 - val_loss: 0.3311 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.33001\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3247 - accuracy: 1.0000 - val_loss: 0.3314 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.33001\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.33001 to 0.32997, saving model to ./mod3.h5\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3227 - accuracy: 1.0000 - val_loss: 0.3279 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.32997 to 0.32788, saving model to ./mod3.h5\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3242 - accuracy: 1.0000 - val_loss: 0.3489 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.32788\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.32788\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3281 - accuracy: 1.0000 - val_loss: 0.3295 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.32788\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3256 - accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.32788\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.3280 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.32788\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3246 - accuracy: 1.0000 - val_loss: 0.3307 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.32788\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.3335 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.32788\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3258 - accuracy: 1.0000 - val_loss: 0.3278 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.32788 to 0.32778, saving model to ./mod3.h5\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3248 - accuracy: 1.0000 - val_loss: 0.3281 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.32778\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3244 - accuracy: 1.0000 - val_loss: 0.3288 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.32778\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3239 - accuracy: 1.0000 - val_loss: 0.3271 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.32778 to 0.32709, saving model to ./mod3.h5\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 0.3266 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.32709 to 0.32660, saving model to ./mod3.h5\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3221 - accuracy: 1.0000 - val_loss: 0.3247 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.32660 to 0.32470, saving model to ./mod3.h5\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3222 - accuracy: 1.0000 - val_loss: 0.3390 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.32470\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.3250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.32470\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3209 - accuracy: 1.0000 - val_loss: 0.3311 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.32470\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.3266 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.32470\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3259 - accuracy: 1.0000 - val_loss: 0.3259 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.32470\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3236 - accuracy: 1.0000 - val_loss: 0.3292 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.32470\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3231 - accuracy: 1.0000 - val_loss: 0.3249 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.32470\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3257 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.32470\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3213 - accuracy: 1.0000 - val_loss: 0.3231 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.32470 to 0.32312, saving model to ./mod3.h5\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3206 - accuracy: 1.0000 - val_loss: 0.3232 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.32312\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3186 - accuracy: 1.0000 - val_loss: 0.3227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.32312 to 0.32267, saving model to ./mod3.h5\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.32267\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3189 - accuracy: 1.0000 - val_loss: 0.3246 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.32267\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3212 - accuracy: 1.0000 - val_loss: 0.3321 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.32267\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3245 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.32267\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3214 - accuracy: 1.0000 - val_loss: 0.3248 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.32267\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3207 - accuracy: 1.0000 - val_loss: 0.3263 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.32267\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3236 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.32267\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3204 - accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.32267\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3200 - accuracy: 1.0000 - val_loss: 0.3233 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.32267\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3230 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.32267\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3184 - accuracy: 1.0000 - val_loss: 0.3218 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.32267 to 0.32175, saving model to ./mod3.h5\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3208 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.32175 to 0.32075, saving model to ./mod3.h5\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 0.3213 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.32075\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.32075\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3197 - accuracy: 1.0000 - val_loss: 0.3217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.32075\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3200 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.32075\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3196 - accuracy: 1.0000 - val_loss: 0.3238 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.32075\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3195 - accuracy: 1.0000 - val_loss: 0.3234 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.32075\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3195 - accuracy: 1.0000 - val_loss: 0.3244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.32075\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3218 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.32075\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3185 - accuracy: 1.0000 - val_loss: 0.3210 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.32075\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3175 - accuracy: 1.0000 - val_loss: 0.3202 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.32075 to 0.32021, saving model to ./mod3.h5\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3167 - accuracy: 1.0000 - val_loss: 0.3183 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.32021 to 0.31832, saving model to ./mod3.h5\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3189 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.31832\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3185 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.31832\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3188 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.31832\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3153 - accuracy: 1.0000 - val_loss: 0.3184 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.31832\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3157 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.31832\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3193 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.31832\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3157 - accuracy: 1.0000 - val_loss: 0.3187 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.31832\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3159 - accuracy: 1.0000 - val_loss: 0.3185 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.31832\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3169 - accuracy: 1.0000 - val_loss: 0.3190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.31832\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3164 - accuracy: 1.0000 - val_loss: 0.3190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.31832\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.3206 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.31832\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3165 - accuracy: 1.0000 - val_loss: 0.3177 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss improved from 0.31832 to 0.31775, saving model to ./mod3.h5\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3167 - accuracy: 1.0000 - val_loss: 0.3175 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.31775 to 0.31749, saving model to ./mod3.h5\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3187 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.31749\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3145 - accuracy: 1.0000 - val_loss: 0.3179 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.31749\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3173 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.31749 to 0.31732, saving model to ./mod3.h5\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.31732\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.3160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.31732 to 0.31598, saving model to ./mod3.h5\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3144 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.31598\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3139 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.31598\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3139 - accuracy: 1.0000 - val_loss: 0.3167 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.31598\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3140 - accuracy: 1.0000 - val_loss: 0.3170 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.31598\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3134 - accuracy: 1.0000 - val_loss: 0.3179 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.31598\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3144 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.31598\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3137 - accuracy: 1.0000 - val_loss: 0.3166 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.31598\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3146 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.31598\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3143 - accuracy: 1.0000 - val_loss: 0.3169 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.31598\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss improved from 0.31598 to 0.31582, saving model to ./mod3.h5\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3136 - accuracy: 1.0000 - val_loss: 0.3149 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.31582 to 0.31485, saving model to ./mod3.h5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 12ms/step - loss: 0.3123 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.5819 - accuracy: 0.9200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3149 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_217 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_219 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_221 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_223 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_225 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_227 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_229 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_231 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_233 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_235 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_237 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_239 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_241 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_243 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_245 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_247 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_249 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_251 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_253 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_255 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_257 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_259 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_261 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_263 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_265 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_267 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_269 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_216 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_218 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_220 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_222 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_224 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_226 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_228 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_230 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_232 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_234 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_236 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_238 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_240 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_242 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_244 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_246 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_248 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_250 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_252 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_254 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_256 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_258 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_260 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_262 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_264 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_266 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_268 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_108 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_109 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_110 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_111 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_112 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_113 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_114 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_115 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_116 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_117 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_118 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_119 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_120 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_121 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_122 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_123 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_124 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_125 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_126 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_127 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_128 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_129 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_130 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_131 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_132 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_133 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_134 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_108 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_109 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_110 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_111 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_114 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_115 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_116 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_117 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_118 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_119 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_120 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_121 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_122 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_123 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_124 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_125 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_126 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_127 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_128 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_129 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_130 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_131 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_132 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_133 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_134 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_108 (G (None, 8)            0           dropout_108[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_109 (G (None, 8)            0           dropout_109[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_110 (G (None, 8)            0           dropout_110[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_111 (G (None, 8)            0           dropout_111[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_112 (G (None, 8)            0           dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_113 (G (None, 8)            0           dropout_113[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_114 (G (None, 8)            0           dropout_114[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_115 (G (None, 8)            0           dropout_115[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_116 (G (None, 8)            0           dropout_116[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_117 (G (None, 8)            0           dropout_117[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_118 (G (None, 8)            0           dropout_118[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_119 (G (None, 8)            0           dropout_119[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_120 (G (None, 8)            0           dropout_120[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_121 (G (None, 8)            0           dropout_121[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_122 (G (None, 8)            0           dropout_122[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_123 (G (None, 8)            0           dropout_123[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_124 (G (None, 8)            0           dropout_124[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_125 (G (None, 8)            0           dropout_125[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_126 (G (None, 8)            0           dropout_126[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_127 (G (None, 8)            0           dropout_127[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_128 (G (None, 8)            0           dropout_128[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_129 (G (None, 8)            0           dropout_129[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_130 (G (None, 8)            0           dropout_130[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_131 (G (None, 8)            0           dropout_131[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_132 (G (None, 8)            0           dropout_132[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_133 (G (None, 8)            0           dropout_133[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_134 (G (None, 8)            0           dropout_134[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 216)          0           global_average_pooling3d_108[0][0\n",
            "                                                                 global_average_pooling3d_109[0][0\n",
            "                                                                 global_average_pooling3d_110[0][0\n",
            "                                                                 global_average_pooling3d_111[0][0\n",
            "                                                                 global_average_pooling3d_112[0][0\n",
            "                                                                 global_average_pooling3d_113[0][0\n",
            "                                                                 global_average_pooling3d_114[0][0\n",
            "                                                                 global_average_pooling3d_115[0][0\n",
            "                                                                 global_average_pooling3d_116[0][0\n",
            "                                                                 global_average_pooling3d_117[0][0\n",
            "                                                                 global_average_pooling3d_118[0][0\n",
            "                                                                 global_average_pooling3d_119[0][0\n",
            "                                                                 global_average_pooling3d_120[0][0\n",
            "                                                                 global_average_pooling3d_121[0][0\n",
            "                                                                 global_average_pooling3d_122[0][0\n",
            "                                                                 global_average_pooling3d_123[0][0\n",
            "                                                                 global_average_pooling3d_124[0][0\n",
            "                                                                 global_average_pooling3d_125[0][0\n",
            "                                                                 global_average_pooling3d_126[0][0\n",
            "                                                                 global_average_pooling3d_127[0][0\n",
            "                                                                 global_average_pooling3d_128[0][0\n",
            "                                                                 global_average_pooling3d_129[0][0\n",
            "                                                                 global_average_pooling3d_130[0][0\n",
            "                                                                 global_average_pooling3d_131[0][0\n",
            "                                                                 global_average_pooling3d_132[0][0\n",
            "                                                                 global_average_pooling3d_133[0][0\n",
            "                                                                 global_average_pooling3d_134[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 512)          111104      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 512)          262656      dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 512)          262656      dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 1)            513         dense_18[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 372ms/step - loss: 99.2574 - accuracy: 0.5610 - val_loss: 93.3634 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.36339, saving model to ./mod4.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 91.6729 - accuracy: 0.5366 - val_loss: 86.0929 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.36339 to 86.09286, saving model to ./mod4.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 84.4719 - accuracy: 0.6098 - val_loss: 79.1192 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.09286 to 79.11916, saving model to ./mod4.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 77.5543 - accuracy: 0.8293 - val_loss: 72.4670 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.11916 to 72.46701, saving model to ./mod4.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 70.9387 - accuracy: 0.8171 - val_loss: 66.1034 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.46701 to 66.10344, saving model to ./mod4.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 64.6737 - accuracy: 0.8902 - val_loss: 60.0802 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.10344 to 60.08016, saving model to ./mod4.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 58.6780 - accuracy: 0.8659 - val_loss: 54.4303 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.08016 to 54.43034, saving model to ./mod4.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 53.0251 - accuracy: 0.8780 - val_loss: 48.9315 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.43034 to 48.93154, saving model to ./mod4.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 47.7110 - accuracy: 0.8293 - val_loss: 43.8189 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.93154 to 43.81894, saving model to ./mod4.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 42.6463 - accuracy: 0.9024 - val_loss: 39.1364 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.81894 to 39.13642, saving model to ./mod4.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 37.9029 - accuracy: 0.9146 - val_loss: 34.4854 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00011: val_loss improved from 39.13642 to 34.48539, saving model to ./mod4.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 33.4705 - accuracy: 0.8780 - val_loss: 30.2635 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.48539 to 30.26346, saving model to ./mod4.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 29.2818 - accuracy: 0.9634 - val_loss: 26.5742 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.26346 to 26.57424, saving model to ./mod4.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 25.4903 - accuracy: 0.8780 - val_loss: 22.7508 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.57424 
