{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb의 사본의 사본의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v2Z_0CIqrhM"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "control=[]\n",
        "adhd=[]\n",
        "for i in os.listdir('./drive/MyDrive/bp/ADHD/'):\n",
        "    b=[]\n",
        "    for j in os.listdir(f'./drive/MyDrive/bp/ADHD/{i}'):\n",
        "        b.append(pd.read_csv(f'./drive/MyDrive/bp/ADHD/{i}/'+j,index_col='Chan').loc[:,:'TotalAbsPow'].to_numpy())\n",
        "    adhd.append(b)\n",
        "    \n",
        "for i in os.listdir('./drive/MyDrive/bp/Control/'):\n",
        "    b=[]    \n",
        "    for j in os.listdir(f'./drive/MyDrive/bp/Control/{i}'):\n",
        "        b.append(pd.read_csv(f'./drive/MyDrive/bp/Control/{i}/'+j,index_col='Chan').loc[:,:'TotalAbsPow'].to_numpy())\n",
        "    control.append(b)\n",
        "    \n",
        "\n",
        "control= np.array(control).transpose(1,0,2,3)\n",
        "adhd=np.array(adhd).transpose(1,0,2,3)\n",
        "\n",
        "x=np.append(adhd,control,axis=0)\n",
        "y=np.array([1]*61+[0]*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNGrhl7tyjkZ"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "cc=[]\n",
        "for i in range(7):\n",
        "    cc.append(x[:,:,:,i].flatten())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnAvzkDcyKJ-"
      },
      "source": [
        "std=RobustScaler()\n",
        "x=std.fit_transform(np.array(cc).transpose()).reshape(121,19,19,7)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbREeoOqI4YY",
        "outputId": "0ff2811d-e874-40f5-bb8b-d06948c4c228"
      },
      "source": [
        "import joblib\n",
        "# 객체를 pickled binary file 형태로 저장한다\n",
        "file_name = 'scale.pkl'\n",
        "joblib.dump(std, file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scale.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex8uYJ5FHvvZ"
      },
      "source": [
        "def crop(dimension, start, end):\n",
        "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
        "    # example : to crop tensor x[:, :, 5:10]\n",
        "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
        "    def func(x):\n",
        "        if dimension == 0:\n",
        "            return x[start: end]\n",
        "        if dimension == 1:\n",
        "            return x[:, start: end]\n",
        "        if dimension == 2:\n",
        "            return x[:, :, start: end]\n",
        "        if dimension == 3:\n",
        "            return x[:, :, :, start: end]\n",
        "        if dimension == 4:\n",
        "            return x[:, :, :, :, start: end]\n",
        "    return Lambda(func)\n",
        "import math\n",
        "def slice_model(model_input,unit,row_num,col_num,term):\n",
        "  remain=math.ceil(unit/2)\n",
        "  return [crop(3,col_num-(j+unit),col_num-j)(crop(2,row_num-(i+unit),row_num-i)(model_input)) for i in range(0,row_num-unit+1,term) for j in range(0,col_num-unit+1,term)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlP81GCoqlnc"
      },
      "source": [
        "import sys\n",
        "import sys\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, GlobalMaxPooling3D,Lambda,concatenate,Conv3D, MaxPooling3D,GlobalAveragePooling3D\n",
        "from tensorflow.keras.regularizers import l1,l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l1,l2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "def mk_model(filepath=None):\n",
        "    \n",
        "    FILTER_SIZE=3\n",
        "    NUM_FILTERS=8\n",
        "    INPUT_SIZE=19\n",
        "    MAXPOOL_SIZE=2\n",
        "\n",
        "    densors=[]\n",
        "    model_input=Input(shape=(19,INPUT_SIZE,7,1))\n",
        "    for idx in slice_model(model_input,3,19,7,2):\n",
        "            \n",
        "            model_output=Conv3D(NUM_FILTERS, (FILTER_SIZE,FILTER_SIZE,FILTER_SIZE),activation='relu')(idx)\n",
        "            model_output=Dropout(0.5)(model_output)                        \n",
        "            model_output=GlobalAveragePooling3D()(model_output)\n",
        "            densors.append(model_output)\n",
        "\n",
        "    model_output=concatenate(densors)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "\n",
        "\n",
        "    model_output=Dense(units=1,activation='sigmoid',kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(model_output)\n",
        "    model = Model(inputs = model_input, outputs = model_output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_Wxk0KWV7Bq",
        "outputId": "7e9d7b6b-38ed-46c3-955c-53bcddb92cb4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "x_tra,x_test,y_tra,y_test=train_test_split(x,y,train_size=0.8,stratify=y,random_state=128)\n",
        "\n",
        "kf=KFold(7,True )\n",
        "train_score=[]\n",
        "test_score=[]\n",
        "val_score=[]\n",
        "test_list=[]\n",
        "\n",
        "idx=0\n",
        "\n",
        "val_list=[]\n",
        "\n",
        "for train_index, test_index in kf.split(x_tra):\n",
        "    callback_list = [\n",
        "    EarlyStopping( #성능 향상이 멈추면 훈련을 중지\n",
        "    monitor='val_loss',  #모델 검증 정확도를 모니터링\n",
        "    patience=50        #1 에포크 보다 더 길게(즉, 2에포크 동안 정확도가 향상되지 않으면 훈련 중지\n",
        "),\n",
        "    ModelCheckpoint( #에포크마다 현재 가중치를 저장\n",
        "    filepath=f'./mod{idx}.h5', #모델 파일 경로\n",
        "    monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\n",
        "    save_best_only=True,\n",
        "    mode='auto',\n",
        "    verbose=1\n",
        ")\n",
        "]\n",
        "    x_train,x_val=x_tra[train_index],x_tra[test_index]\n",
        "    y_train,y_val=y_tra[train_index],y_tra[test_index]\n",
        "    \n",
        "    #with strategy.scope():\n",
        "    model=mk_model()\n",
        "    print(model.summary())\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    hist=model.fit(x_train, y_train, epochs=200, validation_data=(x_val, y_val),batch_size=36,callbacks=callback_list)\n",
        "    \n",
        "    \n",
        "    plt.plot(hist.history['loss'],label='train'+str(idx))\n",
        "    plt.plot(hist.history['val_loss'],label='train'+str(idx))\n",
        "    plt.title('loss',fontsize=15)\n",
        "    plt.legend(['train','val'])\n",
        "    plt.show()\n",
        "    plt.plot(hist.history['accuracy'],label='train'+str(idx))\n",
        "    plt.plot(hist.history['val_accuracy'],label='train'+str(idx))\n",
        "    plt.legend(['train','val'])\n",
        "    plt.title('acc',fontsize=15)\n",
        "    plt.show()\n",
        "    model=load_model(f'./mod{idx}.h5')\n",
        "    \n",
        "    train_score.append(model.evaluate(x_train,y_train))\n",
        "    test_score.append(model.evaluate(x_test,y_test))\n",
        "    val_score.append(model.evaluate(x_val,y_val))\n",
        "    idx+=1\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_21 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_23 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_27 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_29 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_31 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_33 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_35 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_37 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_39 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_41 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_43 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_45 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_47 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_49 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_51 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_53 (Lambda)              (None, 19, 3, 7, 1)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 19, 3, 3, 1)  0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 19, 3, 3, 1)  0           lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_20 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_22 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_24 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_26 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_28 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_30 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_32 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_34 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_36 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_38 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_40 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_42 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_44 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_46 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_48 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_50 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_52 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d (Conv3D)                 (None, 17, 1, 1, 8)  224         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_1 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_2 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_3 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_4 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_5 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_6 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_7 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_8 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_9 (Conv3D)               (None, 17, 1, 1, 8)  224         lambda_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_10 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_11 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_12 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_13 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_14 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_15 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_16 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_17 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_18 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_19 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_20 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_21 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_22 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_23 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_24 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_25 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_26 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 17, 1, 1, 8)  0           conv3d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 17, 1, 1, 8)  0           conv3d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d (Globa (None, 8)            0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_1 (Glo (None, 8)            0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_2 (Glo (None, 8)            0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_3 (Glo (None, 8)            0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_4 (Glo (None, 8)            0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_5 (Glo (None, 8)            0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_6 (Glo (None, 8)            0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_7 (Glo (None, 8)            0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_8 (Glo (None, 8)            0           dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_9 (Glo (None, 8)            0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_10 (Gl (None, 8)            0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_11 (Gl (None, 8)            0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_12 (Gl (None, 8)            0           dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_13 (Gl (None, 8)            0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_14 (Gl (None, 8)            0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_15 (Gl (None, 8)            0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_16 (Gl (None, 8)            0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_17 (Gl (None, 8)            0           dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_18 (Gl (None, 8)            0           dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_19 (Gl (None, 8)            0           dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_20 (Gl (None, 8)            0           dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_21 (Gl (None, 8)            0           dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_22 (Gl (None, 8)            0           dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_23 (Gl (None, 8)            0           dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_24 (Gl (None, 8)            0           dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_25 (Gl (None, 8)            0           dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_26 (Gl (None, 8)            0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 216)          0           global_average_pooling3d[0][0]   \n",
            "                                                                 global_average_pooling3d_1[0][0] \n",
            "                                                                 global_average_pooling3d_2[0][0] \n",
            "                                                                 global_average_pooling3d_3[0][0] \n",
            "                                                                 global_average_pooling3d_4[0][0] \n",
            "                                                                 global_average_pooling3d_5[0][0] \n",
            "                                                                 global_average_pooling3d_6[0][0] \n",
            "                                                                 global_average_pooling3d_7[0][0] \n",
            "                                                                 global_average_pooling3d_8[0][0] \n",
            "                                                                 global_average_pooling3d_9[0][0] \n",
            "                                                                 global_average_pooling3d_10[0][0]\n",
            "                                                                 global_average_pooling3d_11[0][0]\n",
            "                                                                 global_average_pooling3d_12[0][0]\n",
            "                                                                 global_average_pooling3d_13[0][0]\n",
            "                                                                 global_average_pooling3d_14[0][0]\n",
            "                                                                 global_average_pooling3d_15[0][0]\n",
            "                                                                 global_average_pooling3d_16[0][0]\n",
            "                                                                 global_average_pooling3d_17[0][0]\n",
            "                                                                 global_average_pooling3d_18[0][0]\n",
            "                                                                 global_average_pooling3d_19[0][0]\n",
            "                                                                 global_average_pooling3d_20[0][0]\n",
            "                                                                 global_average_pooling3d_21[0][0]\n",
            "                                                                 global_average_pooling3d_22[0][0]\n",
            "                                                                 global_average_pooling3d_23[0][0]\n",
            "                                                                 global_average_pooling3d_24[0][0]\n",
            "                                                                 global_average_pooling3d_25[0][0]\n",
            "                                                                 global_average_pooling3d_26[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          111104      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          262656      dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          262656      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            513         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 374ms/step - loss: 99.2550 - accuracy: 0.5732 - val_loss: 93.2498 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.24981, saving model to ./mod0.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 91.6429 - accuracy: 0.5244 - val_loss: 85.9259 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.24981 to 85.92591, saving model to ./mod0.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 84.3295 - accuracy: 0.8659 - val_loss: 78.9791 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00003: val_loss improved from 85.92591 to 78.97914, saving model to ./mod0.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 77.4317 - accuracy: 0.8049 - val_loss: 72.2439 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00004: val_loss improved from 78.97914 to 72.24390, saving model to ./mod0.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 70.8011 - accuracy: 0.8780 - val_loss: 65.8969 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.24390 to 65.89694, saving model to ./mod0.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 64.5020 - accuracy: 0.9390 - val_loss: 59.8540 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00006: val_loss improved from 65.89694 to 59.85399, saving model to ./mod0.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 58.5411 - accuracy: 0.9024 - val_loss: 54.1313 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00007: val_loss improved from 59.85399 to 54.13125, saving model to ./mod0.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 52.8833 - accuracy: 0.9024 - val_loss: 48.7217 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.13125 to 48.72166, saving model to ./mod0.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 47.4995 - accuracy: 0.9634 - val_loss: 43.5811 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.72166 to 43.58110, saving model to ./mod0.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 42.4633 - accuracy: 0.9390 - val_loss: 38.7831 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.58110 to 38.78310, saving model to ./mod0.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 37.6682 - accuracy: 0.9756 - val_loss: 34.2316 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.78310 to 34.23160, saving model to ./mod0.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 33.1903 - accuracy: 0.9756 - val_loss: 29.9822 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.23160 to 29.98221, saving model to ./mod0.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 29.0217 - accuracy: 0.9878 - val_loss: 26.0778 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00013: val_loss improved from 29.98221 to 26.07778, saving model to ./mod0.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 25.1364 - accuracy: 1.0000 - val_loss: 22.4019 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.07778 to 22.40194, saving model to ./mod0.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 21.5502 - accuracy: 1.0000 - val_loss: 19.0371 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.40194 to 19.03706, saving model to ./mod0.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 18.2831 - accuracy: 1.0000 - val_loss: 15.9914 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.03706 to 15.99143, saving model to ./mod0.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 15.2690 - accuracy: 1.0000 - val_loss: 13.2259 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 15.99143 to 13.22587, saving model to ./mod0.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 12.6140 - accuracy: 0.9878 - val_loss: 11.0247 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.22587 to 11.02474, saving model to ./mod0.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 10.2757 - accuracy: 1.0000 - val_loss: 8.6522 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.02474 to 8.65217, saving model to ./mod0.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 8.1674 - accuracy: 1.0000 - val_loss: 6.9030 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.65217 to 6.90302, saving model to ./mod0.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 6.4193 - accuracy: 1.0000 - val_loss: 5.2740 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 6.90302 to 5.27402, saving model to ./mod0.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 4.9440 - accuracy: 1.0000 - val_loss: 4.0327 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.27402 to 4.03272, saving model to ./mod0.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 3.7493 - accuracy: 1.0000 - val_loss: 3.0863 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.03272 to 3.08628, saving model to ./mod0.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 2.8957 - accuracy: 0.9878 - val_loss: 2.6310 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.08628 to 2.63105, saving model to ./mod0.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 2.8602 - accuracy: 0.7439 - val_loss: 2.3195 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.63105 to 2.31950, saving model to ./mod0.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 2.5085 - accuracy: 0.8537 - val_loss: 2.2433 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.31950 to 2.24328, saving model to ./mod0.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 2.1976 - accuracy: 0.9878 - val_loss: 2.4045 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.24328\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 2.0966 - accuracy: 0.9390 - val_loss: 1.7790 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.24328 to 1.77898, saving model to ./mod0.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 1.6425 - accuracy: 1.0000 - val_loss: 1.5358 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.77898 to 1.53575, saving model to ./mod0.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 1.4759 - accuracy: 0.9634 - val_loss: 1.2059 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.53575 to 1.20588, saving model to ./mod0.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 1.1568 - accuracy: 1.0000 - val_loss: 1.2000 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.20588 to 1.19998, saving model to ./mod0.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 1.0957 - accuracy: 1.0000 - val_loss: 1.0036 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.19998 to 1.00356, saving model to ./mod0.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.9344 - accuracy: 1.0000 - val_loss: 0.9043 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.00356 to 0.90433, saving model to ./mod0.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.8506 - accuracy: 0.9878 - val_loss: 0.8093 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.90433 to 0.80931, saving model to ./mod0.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.7521 - accuracy: 1.0000 - val_loss: 0.7930 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.80931 to 0.79303, saving model to ./mod0.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.6958 - accuracy: 1.0000 - val_loss: 0.6861 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.79303 to 0.68607, saving model to ./mod0.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.6353 - accuracy: 1.0000 - val_loss: 0.6428 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.68607 to 0.64282, saving model to ./mod0.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.5942 - accuracy: 1.0000 - val_loss: 0.6102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.64282 to 0.61021, saving model to ./mod0.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.5492 - accuracy: 1.0000 - val_loss: 0.5779 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.61021 to 0.57786, saving model to ./mod0.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.5224 - accuracy: 1.0000 - val_loss: 0.5605 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.57786 to 0.56049, saving model to ./mod0.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.4981 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.56049 to 0.53355, saving model to ./mod0.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.4814 - accuracy: 1.0000 - val_loss: 0.5227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.53355 to 0.52274, saving model to ./mod0.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4645 - accuracy: 1.0000 - val_loss: 0.5044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.52274 to 0.50435, saving model to ./mod0.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.4503 - accuracy: 1.0000 - val_loss: 0.4935 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.50435 to 0.49354, saving model to ./mod0.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4444 - accuracy: 1.0000 - val_loss: 0.4880 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.49354 to 0.48798, saving model to ./mod0.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4319 - accuracy: 1.0000 - val_loss: 0.4895 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.48798\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.4243 - accuracy: 1.0000 - val_loss: 0.4693 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.48798 to 0.46927, saving model to ./mod0.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4125 - accuracy: 1.0000 - val_loss: 0.4672 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.46927 to 0.46725, saving model to ./mod0.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.4118 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.46725 to 0.46655, saving model to ./mod0.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4104 - accuracy: 1.0000 - val_loss: 0.4510 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.46655 to 0.45097, saving model to ./mod0.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3981 - accuracy: 1.0000 - val_loss: 0.4534 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.45097\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3938 - accuracy: 1.0000 - val_loss: 0.4397 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.45097 to 0.43967, saving model to ./mod0.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3913 - accuracy: 1.0000 - val_loss: 0.4374 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.43967 to 0.43740, saving model to ./mod0.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3883 - accuracy: 1.0000 - val_loss: 0.4408 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.43740\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3847 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.43740 to 0.43357, saving model to ./mod0.h5\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3869 - accuracy: 1.0000 - val_loss: 0.4362 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.43357\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3799 - accuracy: 1.0000 - val_loss: 0.4305 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.43357 to 0.43047, saving model to ./mod0.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3760 - accuracy: 1.0000 - val_loss: 0.4315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.43047\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3751 - accuracy: 1.0000 - val_loss: 0.4260 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.43047 to 0.42600, saving model to ./mod0.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3695 - accuracy: 1.0000 - val_loss: 0.4221 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.42600 to 0.42205, saving model to ./mod0.h5\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3696 - accuracy: 1.0000 - val_loss: 0.4283 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.42205\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3709 - accuracy: 1.0000 - val_loss: 0.4384 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.42205\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3734 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.42205 to 0.42167, saving model to ./mod0.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3822 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.42167 to 0.41909, saving model to ./mod0.h5\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3741 - accuracy: 1.0000 - val_loss: 0.4131 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.41909 to 0.41307, saving model to ./mod0.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3679 - accuracy: 1.0000 - val_loss: 0.4176 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.41307\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3658 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.41307\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3607 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.41307 to 0.40743, saving model to ./mod0.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3594 - accuracy: 1.0000 - val_loss: 0.4344 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.40743\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3641 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.40743 to 0.40617, saving model to ./mod0.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3586 - accuracy: 1.0000 - val_loss: 0.4056 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.40617 to 0.40561, saving model to ./mod0.h5\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3541 - accuracy: 1.0000 - val_loss: 0.4013 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.40561 to 0.40128, saving model to ./mod0.h5\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3501 - accuracy: 1.0000 - val_loss: 0.4050 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.40128\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3511 - accuracy: 1.0000 - val_loss: 0.3979 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.40128 to 0.39791, saving model to ./mod0.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3512 - accuracy: 1.0000 - val_loss: 0.3978 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.39791 to 0.39777, saving model to ./mod0.h5\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3485 - accuracy: 1.0000 - val_loss: 0.3998 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.39777\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3460 - accuracy: 1.0000 - val_loss: 0.3937 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.39777 to 0.39374, saving model to ./mod0.h5\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3455 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.39374 to 0.39131, saving model to ./mod0.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3451 - accuracy: 1.0000 - val_loss: 0.3895 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.39131 to 0.38949, saving model to ./mod0.h5\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3416 - accuracy: 1.0000 - val_loss: 0.3908 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.38949\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3398 - accuracy: 1.0000 - val_loss: 0.3897 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.38949\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3421 - accuracy: 1.0000 - val_loss: 0.3853 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.38949 to 0.38529, saving model to ./mod0.h5\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3410 - accuracy: 1.0000 - val_loss: 0.3985 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.38529\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3403 - accuracy: 1.0000 - val_loss: 0.3844 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.38529 to 0.38438, saving model to ./mod0.h5\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3382 - accuracy: 1.0000 - val_loss: 0.3938 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.38438\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3400 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.38438\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3400 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.38438 to 0.38239, saving model to ./mod0.h5\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3368 - accuracy: 1.0000 - val_loss: 0.3840 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.38239\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3365 - accuracy: 1.0000 - val_loss: 0.3811 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.38239 to 0.38106, saving model to ./mod0.h5\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3363 - accuracy: 1.0000 - val_loss: 0.3803 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.38106 to 0.38031, saving model to ./mod0.h5\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3340 - accuracy: 1.0000 - val_loss: 0.3813 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.38031\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3328 - accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.38031\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3349 - accuracy: 1.0000 - val_loss: 0.4018 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.38031\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3390 - accuracy: 1.0000 - val_loss: 0.3788 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.38031 to 0.37879, saving model to ./mod0.h5\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3354 - accuracy: 1.0000 - val_loss: 0.3833 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.37879\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3352 - accuracy: 1.0000 - val_loss: 0.3813 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.37879\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3336 - accuracy: 1.0000 - val_loss: 0.3804 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.37879\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.3312 - accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.37879\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3319 - accuracy: 1.0000 - val_loss: 0.3781 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.37879 to 0.37809, saving model to ./mod0.h5\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3290 - accuracy: 1.0000 - val_loss: 0.3814 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.37809\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3300 - accuracy: 1.0000 - val_loss: 0.3909 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.37809\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3326 - accuracy: 1.0000 - val_loss: 0.3861 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.37809\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3326 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.37809\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3303 - accuracy: 1.0000 - val_loss: 0.3802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.37809\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3292 - accuracy: 1.0000 - val_loss: 0.3862 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.37809\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3290 - accuracy: 1.0000 - val_loss: 0.3773 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.37809 to 0.37734, saving model to ./mod0.h5\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3272 - accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.37734\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.3757 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.37734 to 0.37571, saving model to ./mod0.h5\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.3765 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.37571\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3256 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.37571\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3263 - accuracy: 1.0000 - val_loss: 0.3742 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.37571 to 0.37422, saving model to ./mod0.h5\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3259 - accuracy: 1.0000 - val_loss: 0.3752 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.37422\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3246 - accuracy: 1.0000 - val_loss: 0.3831 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.37422\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3266 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.37422\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3268 - accuracy: 1.0000 - val_loss: 0.3857 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.37422\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3308 - accuracy: 1.0000 - val_loss: 0.6645 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.37422\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3848 - accuracy: 1.0000 - val_loss: 0.6515 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.37422\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4393 - accuracy: 0.9878 - val_loss: 0.6269 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.37422\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4341 - accuracy: 1.0000 - val_loss: 0.8596 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.37422\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4156 - accuracy: 1.0000 - val_loss: 0.5616 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.37422\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.4023 - accuracy: 1.0000 - val_loss: 0.4513 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.37422\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3852 - accuracy: 1.0000 - val_loss: 0.4371 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.37422\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3718 - accuracy: 1.0000 - val_loss: 0.4284 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.37422\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3590 - accuracy: 1.0000 - val_loss: 0.4255 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.37422\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3495 - accuracy: 1.0000 - val_loss: 0.4200 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.37422\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3401 - accuracy: 1.0000 - val_loss: 0.4204 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.37422\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3347 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.37422\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3295 - accuracy: 1.0000 - val_loss: 0.4186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.37422\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3268 - accuracy: 1.0000 - val_loss: 0.4196 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.37422\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3274 - accuracy: 1.0000 - val_loss: 0.4173 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.37422\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3259 - accuracy: 1.0000 - val_loss: 0.4127 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.37422\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3250 - accuracy: 1.0000 - val_loss: 0.4100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.37422\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.4085 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.37422\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.4047 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.37422\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.4007 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.37422\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3225 - accuracy: 1.0000 - val_loss: 0.4046 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.37422\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3213 - accuracy: 1.0000 - val_loss: 0.4063 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.37422\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3217 - accuracy: 1.0000 - val_loss: 0.3998 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.37422\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3216 - accuracy: 1.0000 - val_loss: 0.3964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.37422\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.4009 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.37422\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3197 - accuracy: 1.0000 - val_loss: 0.3958 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.37422\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3198 - accuracy: 1.0000 - val_loss: 0.3942 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.37422\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3949 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.37422\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3201 - accuracy: 1.0000 - val_loss: 0.3895 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.37422\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 0.3901 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.37422\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3185 - accuracy: 1.0000 - val_loss: 0.3919 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.37422\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3182 - accuracy: 1.0000 - val_loss: 0.3881 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.37422\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3176 - accuracy: 1.0000 - val_loss: 0.3868 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.37422\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3174 - accuracy: 1.0000 - val_loss: 0.3810 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.37422\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3173 - accuracy: 1.0000 - val_loss: 0.3846 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.37422\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3181 - accuracy: 1.0000 - val_loss: 0.3928 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.37422\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3189 - accuracy: 1.0000 - val_loss: 0.3809 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.37422\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3179 - accuracy: 1.0000 - val_loss: 0.3815 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.37422\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3164 - accuracy: 1.0000 - val_loss: 0.3815 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.37422\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3165 - accuracy: 1.0000 - val_loss: 0.3794 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.37422\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3158 - accuracy: 1.0000 - val_loss: 0.3771 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.37422\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.37422\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3152 - accuracy: 1.0000 - val_loss: 0.3785 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.37422\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.3798 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.37422\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3158 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.37422\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3156 - accuracy: 1.0000 - val_loss: 0.3904 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.37422\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 11ms/step - loss: 0.3247 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4327 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3742 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_55 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_57 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_59 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_61 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_63 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_65 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_67 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_69 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_71 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_73 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_75 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_77 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_79 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_81 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_83 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_85 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_87 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_89 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_91 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_93 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_95 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_97 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_99 (Lambda)              (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_101 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_103 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_105 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_107 (Lambda)             (None, 19, 3, 7, 1)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_54 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_56 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_58 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_60 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_62 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_64 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_66 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_68 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_70 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_72 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_74 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_76 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_78 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_80 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_82 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_84 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_86 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_88 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_90 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_92 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_94 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_96 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_98 (Lambda)              (None, 19, 3, 3, 1)  0           lambda_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_100 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_102 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_104 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_106 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_27 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_28 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_29 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_30 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_31 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_32 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_33 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_34 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_35 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_36 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_37 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_38 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_39 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_40 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_41 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_42 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_43 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_44 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_45 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_46 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_47 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_48 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_49 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_50 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_51 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_52 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_53 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_27 (Gl (None, 8)            0           dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_28 (Gl (None, 8)            0           dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_29 (Gl (None, 8)            0           dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_30 (Gl (None, 8)            0           dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_31 (Gl (None, 8)            0           dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_32 (Gl (None, 8)            0           dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_33 (Gl (None, 8)            0           dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_34 (Gl (None, 8)            0           dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_35 (Gl (None, 8)            0           dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_36 (Gl (None, 8)            0           dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_37 (Gl (None, 8)            0           dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_38 (Gl (None, 8)            0           dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_39 (Gl (None, 8)            0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_40 (Gl (None, 8)            0           dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_41 (Gl (None, 8)            0           dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_42 (Gl (None, 8)            0           dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_43 (Gl (None, 8)            0           dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_44 (Gl (None, 8)            0           dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_45 (Gl (None, 8)            0           dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_46 (Gl (None, 8)            0           dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_47 (Gl (None, 8)            0           dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_48 (Gl (None, 8)            0           dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_49 (Gl (None, 8)            0           dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_50 (Gl (None, 8)            0           dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_51 (Gl (None, 8)            0           dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_52 (Gl (None, 8)            0           dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_53 (Gl (None, 8)            0           dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 216)          0           global_average_pooling3d_27[0][0]\n",
            "                                                                 global_average_pooling3d_28[0][0]\n",
            "                                                                 global_average_pooling3d_29[0][0]\n",
            "                                                                 global_average_pooling3d_30[0][0]\n",
            "                                                                 global_average_pooling3d_31[0][0]\n",
            "                                                                 global_average_pooling3d_32[0][0]\n",
            "                                                                 global_average_pooling3d_33[0][0]\n",
            "                                                                 global_average_pooling3d_34[0][0]\n",
            "                                                                 global_average_pooling3d_35[0][0]\n",
            "                                                                 global_average_pooling3d_36[0][0]\n",
            "                                                                 global_average_pooling3d_37[0][0]\n",
            "                                                                 global_average_pooling3d_38[0][0]\n",
            "                                                                 global_average_pooling3d_39[0][0]\n",
            "                                                                 global_average_pooling3d_40[0][0]\n",
            "                                                                 global_average_pooling3d_41[0][0]\n",
            "                                                                 global_average_pooling3d_42[0][0]\n",
            "                                                                 global_average_pooling3d_43[0][0]\n",
            "                                                                 global_average_pooling3d_44[0][0]\n",
            "                                                                 global_average_pooling3d_45[0][0]\n",
            "                                                                 global_average_pooling3d_46[0][0]\n",
            "                                                                 global_average_pooling3d_47[0][0]\n",
            "                                                                 global_average_pooling3d_48[0][0]\n",
            "                                                                 global_average_pooling3d_49[0][0]\n",
            "                                                                 global_average_pooling3d_50[0][0]\n",
            "                                                                 global_average_pooling3d_51[0][0]\n",
            "                                                                 global_average_pooling3d_52[0][0]\n",
            "                                                                 global_average_pooling3d_53[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          111104      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 512)          262656      dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 512)          262656      dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            513         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 490ms/step - loss: 99.2194 - accuracy: 0.5610 - val_loss: 93.2410 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.24102, saving model to ./mod1.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 91.5667 - accuracy: 0.8171 - val_loss: 85.9611 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.24102 to 85.96106, saving model to ./mod1.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 84.2489 - accuracy: 0.8293 - val_loss: 78.8765 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00003: val_loss improved from 85.96106 to 78.87650, saving model to ./mod1.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 77.2937 - accuracy: 0.8293 - val_loss: 72.2065 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00004: val_loss improved from 78.87650 to 72.20648, saving model to ./mod1.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 70.6515 - accuracy: 0.8659 - val_loss: 65.8688 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.20648 to 65.86883, saving model to ./mod1.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 64.3493 - accuracy: 0.8780 - val_loss: 59.7908 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00006: val_loss improved from 65.86883 to 59.79077, saving model to ./mod1.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 58.3370 - accuracy: 0.8902 - val_loss: 54.1351 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss improved from 59.79077 to 54.13509, saving model to ./mod1.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 52.6534 - accuracy: 0.9146 - val_loss: 48.5979 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.13509 to 48.59795, saving model to ./mod1.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 47.2612 - accuracy: 0.9634 - val_loss: 43.5218 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.59795 to 43.52175, saving model to ./mod1.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 42.2261 - accuracy: 0.9390 - val_loss: 38.6611 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.52175 to 38.66110, saving model to ./mod1.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 37.4472 - accuracy: 0.9878 - val_loss: 34.1074 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.66110 to 34.10736, saving model to ./mod1.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 33.0014 - accuracy: 0.9512 - val_loss: 29.9166 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.10736 to 29.91658, saving model to ./mod1.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 28.8408 - accuracy: 0.9878 - val_loss: 25.9298 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 29.91658 to 25.92985, saving model to ./mod1.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 24.9870 - accuracy: 1.0000 - val_loss: 22.3182 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 25.92985 to 22.31821, saving model to ./mod1.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 21.4532 - accuracy: 0.9756 - val_loss: 19.0666 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.31821 to 19.06664, saving model to ./mod1.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 18.1648 - accuracy: 1.0000 - val_loss: 15.9752 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.06664 to 15.97522, saving model to ./mod1.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 15.1824 - accuracy: 1.0000 - val_loss: 13.2043 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 15.97522 to 13.20430, saving model to ./mod1.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 12.5447 - accuracy: 1.0000 - val_loss: 11.2507 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.20430 to 11.25069, saving model to ./mod1.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 10.2544 - accuracy: 0.9634 - val_loss: 9.0253 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.25069 to 9.02529, saving model to ./mod1.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 8.2770 - accuracy: 0.9268 - val_loss: 8.3443 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00020: val_loss improved from 9.02529 to 8.34430, saving model to ./mod1.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 6.6348 - accuracy: 0.9268 - val_loss: 5.4927 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00021: val_loss improved from 8.34430 to 5.49273, saving model to ./mod1.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 5.1055 - accuracy: 0.9878 - val_loss: 4.4084 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.49273 to 4.40841, saving model to ./mod1.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 3.9614 - accuracy: 0.9878 - val_loss: 3.3077 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.40841 to 3.30774, saving model to ./mod1.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 3.0543 - accuracy: 0.9878 - val_loss: 2.5740 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.30774 to 2.57397, saving model to ./mod1.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 2.4242 - accuracy: 1.0000 - val_loss: 2.2962 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.57397 to 2.29619, saving model to ./mod1.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 2.0790 - accuracy: 1.0000 - val_loss: 1.9987 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.29619 to 1.99872, saving model to ./mod1.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.9030 - accuracy: 1.0000 - val_loss: 1.7847 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.99872 to 1.78472, saving model to ./mod1.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 1.6529 - accuracy: 1.0000 - val_loss: 1.5356 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.78472 to 1.53561, saving model to ./mod1.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 1.3896 - accuracy: 1.0000 - val_loss: 1.2648 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.53561 to 1.26484, saving model to ./mod1.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 1.1489 - accuracy: 1.0000 - val_loss: 1.0786 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.26484 to 1.07859, saving model to ./mod1.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.9962 - accuracy: 1.0000 - val_loss: 1.0034 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.07859 to 1.00343, saving model to ./mod1.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.9143 - accuracy: 1.0000 - val_loss: 0.9124 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.00343 to 0.91243, saving model to ./mod1.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.8274 - accuracy: 1.0000 - val_loss: 0.9614 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.91243\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.7296 - accuracy: 1.0000 - val_loss: 0.7609 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.91243 to 0.76093, saving model to ./mod1.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.6733 - accuracy: 1.0000 - val_loss: 0.7233 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.76093 to 0.72328, saving model to ./mod1.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.6293 - accuracy: 1.0000 - val_loss: 0.7832 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.72328\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.5815 - accuracy: 1.0000 - val_loss: 0.6345 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.72328 to 0.63455, saving model to ./mod1.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.5416 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.63455 to 0.61218, saving model to ./mod1.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5491 - accuracy: 0.9878 - val_loss: 0.9416 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.61218\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.7256 - accuracy: 0.9024 - val_loss: 1.9074 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.61218\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.7861 - accuracy: 1.0000 - val_loss: 0.8562 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.61218\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.7936 - accuracy: 1.0000 - val_loss: 0.7897 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.61218\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.6786 - accuracy: 1.0000 - val_loss: 0.6694 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.61218\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.6098 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.61218 to 0.60317, saving model to ./mod1.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5539 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.60317 to 0.56953, saving model to ./mod1.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.5193 - accuracy: 1.0000 - val_loss: 0.7573 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.56953\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.5047 - accuracy: 0.9878 - val_loss: 0.5213 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.56953 to 0.52130, saving model to ./mod1.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4722 - accuracy: 1.0000 - val_loss: 0.5447 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.52130\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4524 - accuracy: 1.0000 - val_loss: 0.4862 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.52130 to 0.48621, saving model to ./mod1.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.4285 - accuracy: 1.0000 - val_loss: 0.4700 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.48621 to 0.47003, saving model to ./mod1.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4227 - accuracy: 1.0000 - val_loss: 0.4667 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.47003 to 0.46673, saving model to ./mod1.h5\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4079 - accuracy: 1.0000 - val_loss: 0.4834 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.46673\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.4075 - accuracy: 1.0000 - val_loss: 0.4500 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.46673 to 0.44997, saving model to ./mod1.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3944 - accuracy: 1.0000 - val_loss: 0.4367 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.44997 to 0.43675, saving model to ./mod1.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3938 - accuracy: 1.0000 - val_loss: 0.4654 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.43675\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3892 - accuracy: 1.0000 - val_loss: 0.4301 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.43675 to 0.43009, saving model to ./mod1.h5\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3837 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.43009 to 0.41914, saving model to ./mod1.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3798 - accuracy: 1.0000 - val_loss: 0.4157 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.41914 to 0.41574, saving model to ./mod1.h5\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3790 - accuracy: 1.0000 - val_loss: 0.4125 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.41574 to 0.41252, saving model to ./mod1.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3841 - accuracy: 1.0000 - val_loss: 0.4450 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.41252\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3847 - accuracy: 1.0000 - val_loss: 0.4104 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.41252 to 0.41043, saving model to ./mod1.h5\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3843 - accuracy: 1.0000 - val_loss: 0.4386 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.41043\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3775 - accuracy: 1.0000 - val_loss: 0.4260 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.41043\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3662 - accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.41043 to 0.40911, saving model to ./mod1.h5\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3663 - accuracy: 1.0000 - val_loss: 0.4090 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.40911 to 0.40899, saving model to ./mod1.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3634 - accuracy: 1.0000 - val_loss: 0.4209 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.40899\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3591 - accuracy: 1.0000 - val_loss: 0.4317 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.40899\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3569 - accuracy: 1.0000 - val_loss: 0.3945 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.40899 to 0.39448, saving model to ./mod1.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3597 - accuracy: 1.0000 - val_loss: 0.4218 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.39448\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3558 - accuracy: 1.0000 - val_loss: 0.4030 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.39448\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3512 - accuracy: 1.0000 - val_loss: 0.4248 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.39448\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3518 - accuracy: 1.0000 - val_loss: 0.3944 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.39448 to 0.39437, saving model to ./mod1.h5\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.3492 - accuracy: 1.0000 - val_loss: 0.4517 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.39437\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3518 - accuracy: 1.0000 - val_loss: 0.3869 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.39437 to 0.38690, saving model to ./mod1.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3472 - accuracy: 1.0000 - val_loss: 0.4266 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.38690\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3458 - accuracy: 1.0000 - val_loss: 0.3820 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.38690 to 0.38202, saving model to ./mod1.h5\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3451 - accuracy: 1.0000 - val_loss: 0.4546 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.38202\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3445 - accuracy: 1.0000 - val_loss: 0.3860 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.38202\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3436 - accuracy: 1.0000 - val_loss: 0.4162 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.38202\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3426 - accuracy: 1.0000 - val_loss: 0.5163 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.38202\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3501 - accuracy: 1.0000 - val_loss: 0.3964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.38202\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3475 - accuracy: 1.0000 - val_loss: 0.3886 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.38202\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3497 - accuracy: 1.0000 - val_loss: 0.4261 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.38202\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3981 - accuracy: 1.0000 - val_loss: 0.4986 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.38202\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4140 - accuracy: 1.0000 - val_loss: 1.0244 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.38202\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3914 - accuracy: 1.0000 - val_loss: 0.4707 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.38202\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3756 - accuracy: 1.0000 - val_loss: 0.4500 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.38202\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3627 - accuracy: 1.0000 - val_loss: 0.4535 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.38202\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3531 - accuracy: 1.0000 - val_loss: 0.3985 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.38202\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3480 - accuracy: 1.0000 - val_loss: 0.4040 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.38202\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3429 - accuracy: 1.0000 - val_loss: 0.4243 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.38202\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3392 - accuracy: 1.0000 - val_loss: 0.4095 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.38202\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3371 - accuracy: 1.0000 - val_loss: 0.4095 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.38202\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3365 - accuracy: 1.0000 - val_loss: 0.3923 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.38202\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3360 - accuracy: 1.0000 - val_loss: 0.4252 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.38202\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3360 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.38202\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3350 - accuracy: 1.0000 - val_loss: 0.4252 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.38202\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3337 - accuracy: 1.0000 - val_loss: 0.4273 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.38202\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3326 - accuracy: 1.0000 - val_loss: 0.4039 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.38202\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3328 - accuracy: 1.0000 - val_loss: 0.3835 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.38202\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3306 - accuracy: 1.0000 - val_loss: 0.4098 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.38202\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3308 - accuracy: 1.0000 - val_loss: 0.4169 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.38202\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3291 - accuracy: 1.0000 - val_loss: 0.4248 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.38202\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3275 - accuracy: 1.0000 - val_loss: 0.4096 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.38202\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3285 - accuracy: 1.0000 - val_loss: 0.3617 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.38202 to 0.36174, saving model to ./mod1.h5\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3329 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.36174\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3311 - accuracy: 1.0000 - val_loss: 0.3845 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.36174\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3290 - accuracy: 1.0000 - val_loss: 0.4270 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.36174\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3293 - accuracy: 1.0000 - val_loss: 0.3819 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.36174\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3298 - accuracy: 1.0000 - val_loss: 0.4031 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.36174\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3289 - accuracy: 1.0000 - val_loss: 0.4544 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.36174\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3283 - accuracy: 1.0000 - val_loss: 0.3707 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.36174\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3270 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.36174\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3679 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.36174\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3276 - accuracy: 1.0000 - val_loss: 0.4498 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.36174\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3253 - accuracy: 1.0000 - val_loss: 0.3751 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.36174\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3258 - accuracy: 1.0000 - val_loss: 0.3785 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.36174\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.3922 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.36174\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3239 - accuracy: 1.0000 - val_loss: 0.3881 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.36174\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3233 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.36174\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3732 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.36174\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.4441 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.36174\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3258 - accuracy: 1.0000 - val_loss: 0.3559 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.36174 to 0.35589, saving model to ./mod1.h5\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.3899 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.35589\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.3970 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.35589\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.4082 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.35589\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3230 - accuracy: 1.0000 - val_loss: 0.3915 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.35589\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.35589\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3214 - accuracy: 1.0000 - val_loss: 0.4249 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.35589\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3212 - accuracy: 1.0000 - val_loss: 0.3521 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.35589 to 0.35213, saving model to ./mod1.h5\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3227 - accuracy: 1.0000 - val_loss: 0.4097 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.35213\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3575 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.35213\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3224 - accuracy: 1.0000 - val_loss: 0.3639 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.35213\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3222 - accuracy: 1.0000 - val_loss: 0.4013 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.35213\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3227 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.35213\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.3978 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.35213\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3211 - accuracy: 1.0000 - val_loss: 0.4269 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.35213\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3206 - accuracy: 1.0000 - val_loss: 0.4068 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.35213\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3210 - accuracy: 1.0000 - val_loss: 0.4991 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.35213\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3217 - accuracy: 1.0000 - val_loss: 0.3739 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.35213\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.4323 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.35213\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3214 - accuracy: 1.0000 - val_loss: 0.3683 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.35213\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3238 - accuracy: 1.0000 - val_loss: 0.5573 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.35213\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3284 - accuracy: 1.0000 - val_loss: 0.4075 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.35213\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3266 - accuracy: 1.0000 - val_loss: 0.3889 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.35213\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3234 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.35213\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3242 - accuracy: 1.0000 - val_loss: 0.3738 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.35213\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3203 - accuracy: 1.0000 - val_loss: 0.4528 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.35213\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3189 - accuracy: 1.0000 - val_loss: 0.3943 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.35213\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3187 - accuracy: 1.0000 - val_loss: 0.4120 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.35213\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.35213\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3176 - accuracy: 1.0000 - val_loss: 0.3924 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.35213\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3172 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.35213\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.3823 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.35213\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3175 - accuracy: 1.0000 - val_loss: 0.3668 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.35213\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3167 - accuracy: 1.0000 - val_loss: 0.4227 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.35213\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3183 - accuracy: 1.0000 - val_loss: 0.3533 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.35213\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3179 - accuracy: 1.0000 - val_loss: 0.4079 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.35213\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3735 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.35213\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3535 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.35213\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3174 - accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.35213\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.3541 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.35213\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3167 - accuracy: 1.0000 - val_loss: 0.4034 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.35213\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.35213\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.3930 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.35213\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.3852 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.35213\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3881 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.35213\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.4081 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.35213\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.3636 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.35213\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.3893 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.35213\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3956 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.35213\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3138 - accuracy: 1.0000 - val_loss: 0.3628 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.35213\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.3816 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.35213\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3143 - accuracy: 1.0000 - val_loss: 0.3507 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss improved from 0.35213 to 0.35073, saving model to ./mod1.h5\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3157 - accuracy: 1.0000 - val_loss: 0.4042 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.35073\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3151 - accuracy: 1.0000 - val_loss: 0.3612 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.35073\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3158 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.35073\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3152 - accuracy: 1.0000 - val_loss: 0.3988 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.35073\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3146 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.35073\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3140 - accuracy: 1.0000 - val_loss: 0.4107 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.35073\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3149 - accuracy: 1.0000 - val_loss: 0.4046 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.35073\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3136 - accuracy: 1.0000 - val_loss: 0.4141 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.35073\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3145 - accuracy: 1.0000 - val_loss: 0.3752 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.35073\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3142 - accuracy: 1.0000 - val_loss: 0.4300 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.35073\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3152 - accuracy: 1.0000 - val_loss: 0.3837 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.35073\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.4167 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.35073\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.3639 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.35073\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3142 - accuracy: 1.0000 - val_loss: 0.4171 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.35073\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.3749 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.35073\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3135 - accuracy: 1.0000 - val_loss: 0.3866 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.35073\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3127 - accuracy: 1.0000 - val_loss: 0.4033 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.35073\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3127 - accuracy: 1.0000 - val_loss: 0.3695 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.35073\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3126 - accuracy: 1.0000 - val_loss: 0.4078 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.35073\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3120 - accuracy: 1.0000 - val_loss: 0.3815 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.35073\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3124 - accuracy: 1.0000 - val_loss: 0.4003 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.35073\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3115 - accuracy: 1.0000 - val_loss: 0.3621 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.35073\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3133 - accuracy: 1.0000 - val_loss: 0.3694 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.35073\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3135 - accuracy: 1.0000 - val_loss: 0.3989 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.35073\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3129 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.35073\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3125 - accuracy: 1.0000 - val_loss: 0.3904 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.35073\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 13ms/step - loss: 0.3149 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.1262 - accuracy: 0.8400\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3507 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_109 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_111 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_113 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_115 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_117 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_119 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_121 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_123 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_125 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_127 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_129 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_131 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_133 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_135 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_137 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_139 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_141 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_143 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_145 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_147 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_149 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_151 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_153 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_155 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_157 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_159 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_161 (Lambda)             (None, 19, 3, 7, 1)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_108 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_110 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_112 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_114 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_116 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_118 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_120 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_122 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_124 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_126 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_128 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_130 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_132 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_134 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_136 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_138 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_140 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_142 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_144 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_146 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_148 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_150 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_152 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_154 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_156 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_158 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_160 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_54 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_55 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_56 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_57 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_58 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_59 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_60 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_61 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_62 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_63 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_64 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_65 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_66 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_67 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_68 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_69 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_70 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_71 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_72 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_73 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_74 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_75 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_76 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_77 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_78 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_79 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_80 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_65 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_67 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_71 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_72 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_73 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_76 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_77 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_79 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_80 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_54 (Gl (None, 8)            0           dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_55 (Gl (None, 8)            0           dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_56 (Gl (None, 8)            0           dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_57 (Gl (None, 8)            0           dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_58 (Gl (None, 8)            0           dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_59 (Gl (None, 8)            0           dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_60 (Gl (None, 8)            0           dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_61 (Gl (None, 8)            0           dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_62 (Gl (None, 8)            0           dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_63 (Gl (None, 8)            0           dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_64 (Gl (None, 8)            0           dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_65 (Gl (None, 8)            0           dropout_65[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_66 (Gl (None, 8)            0           dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_67 (Gl (None, 8)            0           dropout_67[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_68 (Gl (None, 8)            0           dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_69 (Gl (None, 8)            0           dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_70 (Gl (None, 8)            0           dropout_70[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_71 (Gl (None, 8)            0           dropout_71[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_72 (Gl (None, 8)            0           dropout_72[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_73 (Gl (None, 8)            0           dropout_73[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_74 (Gl (None, 8)            0           dropout_74[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_75 (Gl (None, 8)            0           dropout_75[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_76 (Gl (None, 8)            0           dropout_76[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_77 (Gl (None, 8)            0           dropout_77[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_78 (Gl (None, 8)            0           dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_79 (Gl (None, 8)            0           dropout_79[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_80 (Gl (None, 8)            0           dropout_80[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 216)          0           global_average_pooling3d_54[0][0]\n",
            "                                                                 global_average_pooling3d_55[0][0]\n",
            "                                                                 global_average_pooling3d_56[0][0]\n",
            "                                                                 global_average_pooling3d_57[0][0]\n",
            "                                                                 global_average_pooling3d_58[0][0]\n",
            "                                                                 global_average_pooling3d_59[0][0]\n",
            "                                                                 global_average_pooling3d_60[0][0]\n",
            "                                                                 global_average_pooling3d_61[0][0]\n",
            "                                                                 global_average_pooling3d_62[0][0]\n",
            "                                                                 global_average_pooling3d_63[0][0]\n",
            "                                                                 global_average_pooling3d_64[0][0]\n",
            "                                                                 global_average_pooling3d_65[0][0]\n",
            "                                                                 global_average_pooling3d_66[0][0]\n",
            "                                                                 global_average_pooling3d_67[0][0]\n",
            "                                                                 global_average_pooling3d_68[0][0]\n",
            "                                                                 global_average_pooling3d_69[0][0]\n",
            "                                                                 global_average_pooling3d_70[0][0]\n",
            "                                                                 global_average_pooling3d_71[0][0]\n",
            "                                                                 global_average_pooling3d_72[0][0]\n",
            "                                                                 global_average_pooling3d_73[0][0]\n",
            "                                                                 global_average_pooling3d_74[0][0]\n",
            "                                                                 global_average_pooling3d_75[0][0]\n",
            "                                                                 global_average_pooling3d_76[0][0]\n",
            "                                                                 global_average_pooling3d_77[0][0]\n",
            "                                                                 global_average_pooling3d_78[0][0]\n",
            "                                                                 global_average_pooling3d_79[0][0]\n",
            "                                                                 global_average_pooling3d_80[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          111104      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 512)          262656      dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          262656      dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            513         dense_10[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 372ms/step - loss: 99.4793 - accuracy: 0.4512 - val_loss: 93.5406 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.54060, saving model to ./mod2.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 91.8115 - accuracy: 0.8171 - val_loss: 86.1878 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.54060 to 86.18778, saving model to ./mod2.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 84.5409 - accuracy: 0.7683 - val_loss: 79.1948 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.18778 to 79.19482, saving model to ./mod2.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 77.6053 - accuracy: 0.8659 - val_loss: 72.6224 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.19482 to 72.62244, saving model to ./mod2.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 70.9789 - accuracy: 0.8537 - val_loss: 66.2072 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.62244 to 66.20719, saving model to ./mod2.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 64.6922 - accuracy: 0.9146 - val_loss: 60.2134 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.20719 to 60.21340, saving model to ./mod2.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 58.6960 - accuracy: 0.8659 - val_loss: 54.5092 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.21340 to 54.50919, saving model to ./mod2.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 53.0247 - accuracy: 0.8537 - val_loss: 48.9574 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.50919 to 48.95743, saving model to ./mod2.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 47.6538 - accuracy: 0.9390 - val_loss: 43.8310 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.95743 to 43.83098, saving model to ./mod2.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 42.5528 - accuracy: 0.9756 - val_loss: 39.1184 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.83098 to 39.11836, saving model to ./mod2.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 37.8187 - accuracy: 0.9390 - val_loss: 34.4526 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00011: val_loss improved from 39.11836 to 34.45263, saving model to ./mod2.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 33.3330 - accuracy: 0.9756 - val_loss: 30.2144 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.45263 to 30.21439, saving model to ./mod2.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 29.1224 - accuracy: 0.9878 - val_loss: 26.2761 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.21439 to 26.27611, saving model to ./mod2.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 25.2559 - accuracy: 0.9878 - val_loss: 22.6593 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.27611 to 22.65933, saving model to ./mod2.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 21.6838 - accuracy: 0.9878 - val_loss: 19.2818 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.65933 to 19.28185, saving model to ./mod2.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 18.3972 - accuracy: 0.9878 - val_loss: 16.2349 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.28185 to 16.23490, saving model to ./mod2.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 15.3883 - accuracy: 1.0000 - val_loss: 13.4493 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.23490 to 13.44928, saving model to ./mod2.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 12.7162 - accuracy: 1.0000 - val_loss: 11.1255 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.44928 to 11.12552, saving model to ./mod2.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 10.3243 - accuracy: 0.9878 - val_loss: 8.8199 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.12552 to 8.81991, saving model to ./mod2.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 8.2292 - accuracy: 1.0000 - val_loss: 7.1266 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.81991 to 7.12660, saving model to ./mod2.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 6.4347 - accuracy: 1.0000 - val_loss: 5.3876 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.12660 to 5.38763, saving model to ./mod2.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 4.9336 - accuracy: 1.0000 - val_loss: 4.1125 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.38763 to 4.11250, saving model to ./mod2.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 3.7479 - accuracy: 1.0000 - val_loss: 3.5742 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.11250 to 3.57422, saving model to ./mod2.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 3.2871 - accuracy: 0.7561 - val_loss: 3.0280 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.57422 to 3.02796, saving model to ./mod2.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 2.5549 - accuracy: 0.9024 - val_loss: 2.5116 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00025: val_loss improved from 3.02796 to 2.51157, saving model to ./mod2.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 2.3763 - accuracy: 0.9512 - val_loss: 2.2993 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.51157 to 2.29927, saving model to ./mod2.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 2.0834 - accuracy: 1.0000 - val_loss: 2.5374 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.29927\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 1.9250 - accuracy: 0.9390 - val_loss: 1.7374 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.29927 to 1.73736, saving model to ./mod2.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 1.5201 - accuracy: 1.0000 - val_loss: 1.4613 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.73736 to 1.46131, saving model to ./mod2.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 1.2814 - accuracy: 0.9878 - val_loss: 1.2587 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.46131 to 1.25873, saving model to ./mod2.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 1.0876 - accuracy: 0.9878 - val_loss: 1.1597 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.25873 to 1.15969, saving model to ./mod2.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.9803 - accuracy: 1.0000 - val_loss: 1.0357 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.15969 to 1.03573, saving model to ./mod2.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.8848 - accuracy: 1.0000 - val_loss: 1.0165 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.03573 to 1.01653, saving model to ./mod2.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.7845 - accuracy: 1.0000 - val_loss: 0.8617 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.01653 to 0.86166, saving model to ./mod2.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.7077 - accuracy: 1.0000 - val_loss: 0.8130 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.86166 to 0.81303, saving model to ./mod2.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.6639 - accuracy: 1.0000 - val_loss: 0.7874 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.81303 to 0.78743, saving model to ./mod2.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.6096 - accuracy: 1.0000 - val_loss: 0.6888 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.78743 to 0.68883, saving model to ./mod2.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.5757 - accuracy: 1.0000 - val_loss: 0.7286 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68883\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.5381 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.68883 to 0.65167, saving model to ./mod2.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5064 - accuracy: 1.0000 - val_loss: 0.6163 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.65167 to 0.61630, saving model to ./mod2.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4837 - accuracy: 1.0000 - val_loss: 0.6309 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.61630\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4700 - accuracy: 1.0000 - val_loss: 0.5745 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.61630 to 0.57454, saving model to ./mod2.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4493 - accuracy: 1.0000 - val_loss: 0.5743 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.57454 to 0.57432, saving model to ./mod2.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.4441 - accuracy: 1.0000 - val_loss: 0.5322 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.57432 to 0.53217, saving model to ./mod2.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.4319 - accuracy: 1.0000 - val_loss: 0.5588 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.53217\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4210 - accuracy: 1.0000 - val_loss: 0.5058 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.53217 to 0.50581, saving model to ./mod2.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4146 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.50581\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4125 - accuracy: 1.0000 - val_loss: 0.5020 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.50581 to 0.50202, saving model to ./mod2.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4052 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.50202\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3947 - accuracy: 1.0000 - val_loss: 0.6216 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.50202\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4341 - accuracy: 1.0000 - val_loss: 0.5304 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.50202\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.4703 - accuracy: 1.0000 - val_loss: 0.5350 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.50202\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4485 - accuracy: 1.0000 - val_loss: 0.6478 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.50202\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4254 - accuracy: 1.0000 - val_loss: 0.4879 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.50202 to 0.48786, saving model to ./mod2.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4131 - accuracy: 1.0000 - val_loss: 0.4897 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.48786\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3974 - accuracy: 1.0000 - val_loss: 0.5111 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.48786\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3942 - accuracy: 1.0000 - val_loss: 0.4801 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.48786 to 0.48012, saving model to ./mod2.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3809 - accuracy: 1.0000 - val_loss: 0.4884 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.48012\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3750 - accuracy: 1.0000 - val_loss: 0.4512 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.48012 to 0.45121, saving model to ./mod2.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3717 - accuracy: 1.0000 - val_loss: 0.4364 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.45121 to 0.43638, saving model to ./mod2.h5\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3716 - accuracy: 1.0000 - val_loss: 0.4655 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.43638\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3699 - accuracy: 1.0000 - val_loss: 0.5149 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.43638\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3681 - accuracy: 1.0000 - val_loss: 0.4505 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.43638\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3669 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.43638\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3807 - accuracy: 1.0000 - val_loss: 0.4225 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.43638 to 0.42251, saving model to ./mod2.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3747 - accuracy: 1.0000 - val_loss: 0.4281 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.42251\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3633 - accuracy: 1.0000 - val_loss: 0.4874 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.42251\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3594 - accuracy: 1.0000 - val_loss: 0.4326 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.42251\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3561 - accuracy: 1.0000 - val_loss: 0.4515 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.42251\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3571 - accuracy: 1.0000 - val_loss: 0.4493 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.42251\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3510 - accuracy: 1.0000 - val_loss: 0.4497 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.42251\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3489 - accuracy: 1.0000 - val_loss: 0.4396 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.42251\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3488 - accuracy: 1.0000 - val_loss: 0.4211 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.42251 to 0.42108, saving model to ./mod2.h5\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3449 - accuracy: 1.0000 - val_loss: 0.4443 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.42108\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3467 - accuracy: 1.0000 - val_loss: 0.4152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.42108 to 0.41523, saving model to ./mod2.h5\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3435 - accuracy: 1.0000 - val_loss: 0.4304 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.41523\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3456 - accuracy: 1.0000 - val_loss: 0.4378 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.41523\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3432 - accuracy: 1.0000 - val_loss: 0.4028 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.41523 to 0.40279, saving model to ./mod2.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3441 - accuracy: 1.0000 - val_loss: 0.4253 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.40279\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3403 - accuracy: 1.0000 - val_loss: 0.4181 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.40279\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3421 - accuracy: 1.0000 - val_loss: 0.4298 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.40279\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3390 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.40279\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3400 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.40279\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3424 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.40279 to 0.39925, saving model to ./mod2.h5\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3386 - accuracy: 1.0000 - val_loss: 0.4351 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.39925\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3377 - accuracy: 1.0000 - val_loss: 0.4015 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.39925\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3360 - accuracy: 1.0000 - val_loss: 0.4079 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.39925\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3349 - accuracy: 1.0000 - val_loss: 0.4142 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.39925\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3346 - accuracy: 1.0000 - val_loss: 0.3894 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.39925 to 0.38936, saving model to ./mod2.h5\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3323 - accuracy: 1.0000 - val_loss: 0.3994 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.38936\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3324 - accuracy: 1.0000 - val_loss: 0.3989 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.38936\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3314 - accuracy: 1.0000 - val_loss: 0.4099 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.38936\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3303 - accuracy: 1.0000 - val_loss: 0.3859 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.38936 to 0.38587, saving model to ./mod2.h5\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3328 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.38587\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3317 - accuracy: 1.0000 - val_loss: 0.3853 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.38587 to 0.38527, saving model to ./mod2.h5\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3328 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.38527\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3341 - accuracy: 1.0000 - val_loss: 0.3964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.38527\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3305 - accuracy: 1.0000 - val_loss: 0.3905 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.38527\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3297 - accuracy: 1.0000 - val_loss: 0.3876 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.38527\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3982 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.38527\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3289 - accuracy: 1.0000 - val_loss: 0.3845 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.38527 to 0.38447, saving model to ./mod2.h5\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3277 - accuracy: 1.0000 - val_loss: 0.4254 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.38447\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.4007 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.38447\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.4035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.38447\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3279 - accuracy: 1.0000 - val_loss: 0.4003 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.38447\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3262 - accuracy: 1.0000 - val_loss: 0.3960 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.38447\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3254 - accuracy: 1.0000 - val_loss: 0.3927 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.38447\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3239 - accuracy: 1.0000 - val_loss: 0.3921 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.38447\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3239 - accuracy: 1.0000 - val_loss: 0.3949 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.38447\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3254 - accuracy: 1.0000 - val_loss: 0.3737 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.38447 to 0.37373, saving model to ./mod2.h5\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3252 - accuracy: 1.0000 - val_loss: 0.3928 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.37373\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3242 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.37373\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3236 - accuracy: 1.0000 - val_loss: 0.3956 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.37373\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3235 - accuracy: 1.0000 - val_loss: 0.3955 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.37373\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3230 - accuracy: 1.0000 - val_loss: 0.3900 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.37373\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3227 - accuracy: 1.0000 - val_loss: 0.3628 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.37373 to 0.36275, saving model to ./mod2.h5\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3280 - accuracy: 1.0000 - val_loss: 0.3745 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.36275\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3255 - accuracy: 1.0000 - val_loss: 0.3948 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.36275\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3241 - accuracy: 1.0000 - val_loss: 0.3787 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.36275\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3246 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.36275\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3236 - accuracy: 1.0000 - val_loss: 0.3892 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.36275\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3223 - accuracy: 1.0000 - val_loss: 0.3731 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.36275\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3256 - accuracy: 1.0000 - val_loss: 0.4223 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.36275\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 0.3996 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.36275\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.3787 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.36275\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3223 - accuracy: 1.0000 - val_loss: 0.3949 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.36275\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 0.3586 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.36275 to 0.35862, saving model to ./mod2.h5\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3241 - accuracy: 1.0000 - val_loss: 0.3932 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.35862\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.3948 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.35862\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3221 - accuracy: 1.0000 - val_loss: 0.3796 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.35862\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3935 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.35862\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3202 - accuracy: 1.0000 - val_loss: 0.3940 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.35862\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.3668 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.35862\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3205 - accuracy: 1.0000 - val_loss: 0.4202 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.35862\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3204 - accuracy: 1.0000 - val_loss: 0.3643 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.35862\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3193 - accuracy: 1.0000 - val_loss: 0.3858 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.35862\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.3541 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.35862 to 0.35408, saving model to ./mod2.h5\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3910 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.35408\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3557 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.35408\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.3631 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.35408\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3692 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.35408\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.3926 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.35408\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3191 - accuracy: 1.0000 - val_loss: 0.3760 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.35408\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.35408\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3175 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.35408\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3170 - accuracy: 1.0000 - val_loss: 0.3799 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.35408\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3173 - accuracy: 1.0000 - val_loss: 0.3768 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.35408\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3168 - accuracy: 1.0000 - val_loss: 0.3902 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.35408\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3178 - accuracy: 1.0000 - val_loss: 0.3941 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.35408\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3876 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.35408\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3762 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.35408\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.35408\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3179 - accuracy: 1.0000 - val_loss: 0.3807 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.35408\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3165 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.35408\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3179 - accuracy: 1.0000 - val_loss: 0.3898 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.35408\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.3680 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.35408\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3180 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.35408\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3520 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.35408 to 0.35197, saving model to ./mod2.h5\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.4444 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.35197\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.3645 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.35197\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.4158 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.35197\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3289 - accuracy: 1.0000 - val_loss: 0.4345 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.35197\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3306 - accuracy: 1.0000 - val_loss: 0.3546 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.35197\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3308 - accuracy: 1.0000 - val_loss: 0.3739 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.35197\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3290 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.35197\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3253 - accuracy: 1.0000 - val_loss: 0.3487 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.35197 to 0.34871, saving model to ./mod2.h5\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3222 - accuracy: 1.0000 - val_loss: 0.3830 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.34871\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3191 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.34871\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3180 - accuracy: 1.0000 - val_loss: 0.3535 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.34871\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.3785 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.34871\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3606 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.34871\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3138 - accuracy: 1.0000 - val_loss: 0.3802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.34871\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3133 - accuracy: 1.0000 - val_loss: 0.3528 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.34871\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3138 - accuracy: 1.0000 - val_loss: 0.3591 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.34871\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.34871\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3159 - accuracy: 1.0000 - val_loss: 0.3679 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.34871\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3196 - accuracy: 1.0000 - val_loss: 0.4067 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.34871\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3486 - accuracy: 0.9878 - val_loss: 2.4085 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.34871\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.6634 - accuracy: 0.9512 - val_loss: 0.8731 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.34871\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.8893 - accuracy: 1.0000 - val_loss: 0.9252 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.34871\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.9181 - accuracy: 1.0000 - val_loss: 0.8819 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.34871\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.8619 - accuracy: 1.0000 - val_loss: 0.8653 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.34871\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.7822 - accuracy: 1.0000 - val_loss: 0.8329 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.34871\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.7191 - accuracy: 0.9878 - val_loss: 0.6490 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.34871\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.6347 - accuracy: 1.0000 - val_loss: 0.6652 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.34871\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.5805 - accuracy: 1.0000 - val_loss: 0.8960 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.34871\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5313 - accuracy: 1.0000 - val_loss: 1.0919 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.34871\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4912 - accuracy: 1.0000 - val_loss: 1.1800 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.34871\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4603 - accuracy: 1.0000 - val_loss: 1.2259 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.34871\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.4431 - accuracy: 0.9878 - val_loss: 0.5162 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.34871\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4402 - accuracy: 1.0000 - val_loss: 0.4315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.34871\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4252 - accuracy: 1.0000 - val_loss: 0.4666 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.34871\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4098 - accuracy: 1.0000 - val_loss: 0.6799 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.34871\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4503 - accuracy: 0.9756 - val_loss: 0.6651 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.34871\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.5257 - accuracy: 0.9878 - val_loss: 1.9871 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.34871\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.6227 - accuracy: 0.9878 - val_loss: 1.4829 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.34871\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.6448 - accuracy: 1.0000 - val_loss: 0.7146 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.34871\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.6318 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.34871\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.5975 - accuracy: 1.0000 - val_loss: 0.5669 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.34871\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5569 - accuracy: 1.0000 - val_loss: 0.5286 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.34871\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 13ms/step - loss: 0.3229 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.5600 - accuracy: 0.8800\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3487 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_163 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_165 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_167 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_169 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_171 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_173 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_175 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_177 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_179 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_181 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_183 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_185 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_187 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_189 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_191 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_193 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_195 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_197 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_199 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_201 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_203 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_205 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_207 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_209 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_211 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_213 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_215 (Lambda)             (None, 19, 3, 7, 1)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_162 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_164 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_166 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_168 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_170 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_172 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_174 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_176 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_178 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_180 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_182 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_184 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_186 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_188 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_190 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_192 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_194 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_196 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_198 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_200 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_202 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_204 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_206 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_208 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_210 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_212 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_214 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_81 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_82 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_83 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_84 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_85 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_86 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_87 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_88 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_89 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_90 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_91 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_92 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_93 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_94 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_95 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_96 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_97 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_98 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_99 (Conv3D)              (None, 17, 1, 1, 8)  224         lambda_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_100 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_101 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_102 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_103 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_104 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_105 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_106 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_107 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_81 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_82 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_83 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_84 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_85 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_86 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_87 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_88 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_89 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_90 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_91 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_92 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_93 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_94 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_95 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_96 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_97 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_98 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_99 (Dropout)            (None, 17, 1, 1, 8)  0           conv3d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_100 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_101 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_102 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_103 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_104 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_105 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_106 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_107 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_81 (Gl (None, 8)            0           dropout_81[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_82 (Gl (None, 8)            0           dropout_82[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_83 (Gl (None, 8)            0           dropout_83[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_84 (Gl (None, 8)            0           dropout_84[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_85 (Gl (None, 8)            0           dropout_85[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_86 (Gl (None, 8)            0           dropout_86[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_87 (Gl (None, 8)            0           dropout_87[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_88 (Gl (None, 8)            0           dropout_88[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_89 (Gl (None, 8)            0           dropout_89[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_90 (Gl (None, 8)            0           dropout_90[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_91 (Gl (None, 8)            0           dropout_91[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_92 (Gl (None, 8)            0           dropout_92[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_93 (Gl (None, 8)            0           dropout_93[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_94 (Gl (None, 8)            0           dropout_94[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_95 (Gl (None, 8)            0           dropout_95[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_96 (Gl (None, 8)            0           dropout_96[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_97 (Gl (None, 8)            0           dropout_97[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_98 (Gl (None, 8)            0           dropout_98[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_99 (Gl (None, 8)            0           dropout_99[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_100 (G (None, 8)            0           dropout_100[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_101 (G (None, 8)            0           dropout_101[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_102 (G (None, 8)            0           dropout_102[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_103 (G (None, 8)            0           dropout_103[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_104 (G (None, 8)            0           dropout_104[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_105 (G (None, 8)            0           dropout_105[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_106 (G (None, 8)            0           dropout_106[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_107 (G (None, 8)            0           dropout_107[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 216)          0           global_average_pooling3d_81[0][0]\n",
            "                                                                 global_average_pooling3d_82[0][0]\n",
            "                                                                 global_average_pooling3d_83[0][0]\n",
            "                                                                 global_average_pooling3d_84[0][0]\n",
            "                                                                 global_average_pooling3d_85[0][0]\n",
            "                                                                 global_average_pooling3d_86[0][0]\n",
            "                                                                 global_average_pooling3d_87[0][0]\n",
            "                                                                 global_average_pooling3d_88[0][0]\n",
            "                                                                 global_average_pooling3d_89[0][0]\n",
            "                                                                 global_average_pooling3d_90[0][0]\n",
            "                                                                 global_average_pooling3d_91[0][0]\n",
            "                                                                 global_average_pooling3d_92[0][0]\n",
            "                                                                 global_average_pooling3d_93[0][0]\n",
            "                                                                 global_average_pooling3d_94[0][0]\n",
            "                                                                 global_average_pooling3d_95[0][0]\n",
            "                                                                 global_average_pooling3d_96[0][0]\n",
            "                                                                 global_average_pooling3d_97[0][0]\n",
            "                                                                 global_average_pooling3d_98[0][0]\n",
            "                                                                 global_average_pooling3d_99[0][0]\n",
            "                                                                 global_average_pooling3d_100[0][0\n",
            "                                                                 global_average_pooling3d_101[0][0\n",
            "                                                                 global_average_pooling3d_102[0][0\n",
            "                                                                 global_average_pooling3d_103[0][0\n",
            "                                                                 global_average_pooling3d_104[0][0\n",
            "                                                                 global_average_pooling3d_105[0][0\n",
            "                                                                 global_average_pooling3d_106[0][0\n",
            "                                                                 global_average_pooling3d_107[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 512)          111104      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 512)          262656      dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 512)          262656      dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1)            513         dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 390ms/step - loss: 99.4353 - accuracy: 0.5244 - val_loss: 93.6186 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.61864, saving model to ./mod3.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 91.7885 - accuracy: 0.6098 - val_loss: 86.1528 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.61864 to 86.15282, saving model to ./mod3.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 84.5053 - accuracy: 0.6951 - val_loss: 79.1496 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.15282 to 79.14956, saving model to ./mod3.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 77.5991 - accuracy: 0.7683 - val_loss: 72.6795 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.14956 to 72.67950, saving model to ./mod3.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 71.0057 - accuracy: 0.7439 - val_loss: 66.1728 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.67950 to 66.17284, saving model to ./mod3.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 64.7149 - accuracy: 0.8659 - val_loss: 60.0636 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.17284 to 60.06356, saving model to ./mod3.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 58.7317 - accuracy: 0.8780 - val_loss: 54.3619 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.06356 to 54.36186, saving model to ./mod3.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 53.0293 - accuracy: 0.9268 - val_loss: 48.8383 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.36186 to 48.83829, saving model to ./mod3.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 47.6640 - accuracy: 0.9146 - val_loss: 43.6912 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.83829 to 43.69125, saving model to ./mod3.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 42.5816 - accuracy: 0.9268 - val_loss: 38.8669 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.69125 to 38.86693, saving model to ./mod3.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 37.7900 - accuracy: 0.9634 - val_loss: 34.2915 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.86693 to 34.29155, saving model to ./mod3.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 33.3188 - accuracy: 0.9390 - val_loss: 30.0885 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.29155 to 30.08851, saving model to ./mod3.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 29.1226 - accuracy: 0.9878 - val_loss: 26.0641 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.08851 to 26.06406, saving model to ./mod3.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 25.2152 - accuracy: 0.9878 - val_loss: 22.4542 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.06406 to 22.45418, saving model to ./mod3.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 21.6359 - accuracy: 1.0000 - val_loss: 19.0836 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.45418 to 19.08357, saving model to ./mod3.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 18.3603 - accuracy: 1.0000 - val_loss: 16.0575 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.08357 to 16.05753, saving model to ./mod3.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 15.3654 - accuracy: 1.0000 - val_loss: 13.2683 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.05753 to 13.26833, saving model to ./mod3.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 12.7242 - accuracy: 0.9634 - val_loss: 10.8811 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.26833 to 10.88106, saving model to ./mod3.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 10.4162 - accuracy: 0.9512 - val_loss: 8.7460 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 10.88106 to 8.74596, saving model to ./mod3.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 8.5151 - accuracy: 0.8415 - val_loss: 7.0129 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.74596 to 7.01292, saving model to ./mod3.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 6.6706 - accuracy: 0.9634 - val_loss: 5.5070 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.01292 to 5.50701, saving model to ./mod3.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 5.1489 - accuracy: 0.9878 - val_loss: 4.2456 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.50701 to 4.24560, saving model to ./mod3.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 4.0008 - accuracy: 0.9878 - val_loss: 3.2496 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.24560 to 3.24957, saving model to ./mod3.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 3.0717 - accuracy: 1.0000 - val_loss: 2.5887 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.24957 to 2.58867, saving model to ./mod3.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 2.4435 - accuracy: 1.0000 - val_loss: 2.1478 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.58867 to 2.14781, saving model to ./mod3.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 2.0941 - accuracy: 1.0000 - val_loss: 2.0625 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.14781 to 2.06248, saving model to ./mod3.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.9335 - accuracy: 1.0000 - val_loss: 1.7526 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.06248 to 1.75260, saving model to ./mod3.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.6947 - accuracy: 1.0000 - val_loss: 1.5938 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.75260 to 1.59384, saving model to ./mod3.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.4609 - accuracy: 0.9756 - val_loss: 1.2610 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.59384 to 1.26095, saving model to ./mod3.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 1.2060 - accuracy: 1.0000 - val_loss: 1.1160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.26095 to 1.11605, saving model to ./mod3.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 1.0616 - accuracy: 1.0000 - val_loss: 0.9803 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.11605 to 0.98026, saving model to ./mod3.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.9736 - accuracy: 1.0000 - val_loss: 0.8831 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.98026 to 0.88312, saving model to ./mod3.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.8430 - accuracy: 1.0000 - val_loss: 0.7910 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.88312 to 0.79101, saving model to ./mod3.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.7506 - accuracy: 1.0000 - val_loss: 0.8186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.79101\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.7292 - accuracy: 1.0000 - val_loss: 0.6959 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.79101 to 0.69585, saving model to ./mod3.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.7016 - accuracy: 1.0000 - val_loss: 0.7847 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69585\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.7227 - accuracy: 0.9878 - val_loss: 0.6464 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.69585 to 0.64641, saving model to ./mod3.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.6366 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.64641 to 0.58500, saving model to ./mod3.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5832 - accuracy: 1.0000 - val_loss: 0.6802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.58500\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.5625 - accuracy: 1.0000 - val_loss: 0.5291 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.58500 to 0.52914, saving model to ./mod3.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5271 - accuracy: 1.0000 - val_loss: 0.5704 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.52914\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5021 - accuracy: 1.0000 - val_loss: 0.4842 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.52914 to 0.48422, saving model to ./mod3.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4766 - accuracy: 1.0000 - val_loss: 0.4763 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.48422 to 0.47627, saving model to ./mod3.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4606 - accuracy: 1.0000 - val_loss: 0.4760 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.47627 to 0.47601, saving model to ./mod3.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4482 - accuracy: 1.0000 - val_loss: 0.4498 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.47601 to 0.44976, saving model to ./mod3.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.4331 - accuracy: 1.0000 - val_loss: 0.4823 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.44976\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4288 - accuracy: 1.0000 - val_loss: 0.4326 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.44976 to 0.43264, saving model to ./mod3.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4266 - accuracy: 1.0000 - val_loss: 0.4473 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.43264\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4140 - accuracy: 1.0000 - val_loss: 0.4222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.43264 to 0.42225, saving model to ./mod3.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4059 - accuracy: 1.0000 - val_loss: 0.4412 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.42225\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4023 - accuracy: 1.0000 - val_loss: 0.4103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.42225 to 0.41035, saving model to ./mod3.h5\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3964 - accuracy: 1.0000 - val_loss: 0.4059 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.41035 to 0.40590, saving model to ./mod3.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3885 - accuracy: 1.0000 - val_loss: 0.4002 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.40590 to 0.40023, saving model to ./mod3.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3859 - accuracy: 1.0000 - val_loss: 0.3986 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.40023 to 0.39860, saving model to ./mod3.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3809 - accuracy: 1.0000 - val_loss: 0.3934 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.39860 to 0.39337, saving model to ./mod3.h5\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3779 - accuracy: 1.0000 - val_loss: 0.3919 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.39337 to 0.39194, saving model to ./mod3.h5\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3766 - accuracy: 1.0000 - val_loss: 0.3885 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.39194 to 0.38849, saving model to ./mod3.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3746 - accuracy: 1.0000 - val_loss: 0.3920 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.38849\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3748 - accuracy: 1.0000 - val_loss: 0.3840 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.38849 to 0.38400, saving model to ./mod3.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3715 - accuracy: 1.0000 - val_loss: 0.3866 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.38400\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3699 - accuracy: 1.0000 - val_loss: 0.3773 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.38400 to 0.37734, saving model to ./mod3.h5\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3701 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.37734 to 0.37590, saving model to ./mod3.h5\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3645 - accuracy: 1.0000 - val_loss: 0.3712 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.37590 to 0.37115, saving model to ./mod3.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3605 - accuracy: 1.0000 - val_loss: 0.3712 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.37115\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3718 - accuracy: 1.0000 - val_loss: 0.4186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.37115\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3761 - accuracy: 1.0000 - val_loss: 0.3761 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.37115\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3667 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.37115\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3614 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.37115 to 0.37012, saving model to ./mod3.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3620 - accuracy: 1.0000 - val_loss: 0.3709 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.37012\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3581 - accuracy: 1.0000 - val_loss: 0.3645 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.37012 to 0.36450, saving model to ./mod3.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3686 - accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.36450\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3620 - accuracy: 1.0000 - val_loss: 0.3657 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.36450\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3586 - accuracy: 1.0000 - val_loss: 0.3837 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.36450\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3574 - accuracy: 1.0000 - val_loss: 0.3616 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.36450 to 0.36160, saving model to ./mod3.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3567 - accuracy: 1.0000 - val_loss: 0.3756 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.36160\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3513 - accuracy: 1.0000 - val_loss: 0.3568 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.36160 to 0.35681, saving model to ./mod3.h5\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3507 - accuracy: 1.0000 - val_loss: 0.3572 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.35681\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3489 - accuracy: 1.0000 - val_loss: 0.3543 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.35681 to 0.35431, saving model to ./mod3.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3460 - accuracy: 1.0000 - val_loss: 0.3556 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.35431\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3427 - accuracy: 1.0000 - val_loss: 0.3504 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.35431 to 0.35036, saving model to ./mod3.h5\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3447 - accuracy: 1.0000 - val_loss: 0.3604 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.35036\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3493 - accuracy: 1.0000 - val_loss: 0.3569 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.35036\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3489 - accuracy: 1.0000 - val_loss: 0.3661 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.35036\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3469 - accuracy: 1.0000 - val_loss: 0.3548 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.35036\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3519 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.35036\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3507 - accuracy: 1.0000 - val_loss: 0.3515 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.35036\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3440 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.35036\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3418 - accuracy: 1.0000 - val_loss: 0.3511 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.35036\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3388 - accuracy: 1.0000 - val_loss: 0.3448 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.35036 to 0.34481, saving model to ./mod3.h5\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3388 - accuracy: 1.0000 - val_loss: 0.3441 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.34481 to 0.34409, saving model to ./mod3.h5\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3386 - accuracy: 1.0000 - val_loss: 0.3465 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.34409\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3385 - accuracy: 1.0000 - val_loss: 0.3420 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.34409 to 0.34197, saving model to ./mod3.h5\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3368 - accuracy: 1.0000 - val_loss: 0.3557 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.34197\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3410 - accuracy: 1.0000 - val_loss: 0.3451 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.34197\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3423 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.34197\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3410 - accuracy: 1.0000 - val_loss: 0.3430 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.34197\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3403 - accuracy: 1.0000 - val_loss: 0.3466 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.34197\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3376 - accuracy: 1.0000 - val_loss: 0.3406 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.34197 to 0.34064, saving model to ./mod3.h5\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3404 - accuracy: 1.0000 - val_loss: 0.3384 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.34064 to 0.33839, saving model to ./mod3.h5\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3348 - accuracy: 1.0000 - val_loss: 0.3420 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.33839\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3337 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.33839\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3323 - accuracy: 1.0000 - val_loss: 0.3381 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.33839 to 0.33811, saving model to ./mod3.h5\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3318 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.33811 to 0.33702, saving model to ./mod3.h5\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3326 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.33702 to 0.33615, saving model to ./mod3.h5\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3320 - accuracy: 1.0000 - val_loss: 0.3394 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.33615\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3303 - accuracy: 1.0000 - val_loss: 0.3354 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.33615 to 0.33544, saving model to ./mod3.h5\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3328 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.33544 to 0.33284, saving model to ./mod3.h5\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3269 - accuracy: 1.0000 - val_loss: 0.3344 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.33284\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.33284 to 0.33253, saving model to ./mod3.h5\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3276 - accuracy: 1.0000 - val_loss: 0.3354 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.33253\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3276 - accuracy: 1.0000 - val_loss: 0.3329 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.33253\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3271 - accuracy: 1.0000 - val_loss: 0.3315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.33253 to 0.33153, saving model to ./mod3.h5\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3280 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.33153\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3280 - accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.33153\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3276 - accuracy: 1.0000 - val_loss: 0.3343 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.33153\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3270 - accuracy: 1.0000 - val_loss: 0.3320 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.33153\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3258 - accuracy: 1.0000 - val_loss: 0.3304 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.33153 to 0.33042, saving model to ./mod3.h5\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3265 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.33042\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3262 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.33042 to 0.33001, saving model to ./mod3.h5\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.3326 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.33001\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3242 - accuracy: 1.0000 - val_loss: 0.3311 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.33001\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3247 - accuracy: 1.0000 - val_loss: 0.3314 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.33001\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.33001 to 0.32997, saving model to ./mod3.h5\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3227 - accuracy: 1.0000 - val_loss: 0.3279 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.32997 to 0.32788, saving model to ./mod3.h5\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3242 - accuracy: 1.0000 - val_loss: 0.3489 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.32788\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.32788\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3281 - accuracy: 1.0000 - val_loss: 0.3295 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.32788\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3256 - accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.32788\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.3280 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.32788\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3246 - accuracy: 1.0000 - val_loss: 0.3307 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.32788\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.3335 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.32788\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3258 - accuracy: 1.0000 - val_loss: 0.3278 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.32788 to 0.32778, saving model to ./mod3.h5\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3248 - accuracy: 1.0000 - val_loss: 0.3281 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.32778\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3244 - accuracy: 1.0000 - val_loss: 0.3288 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.32778\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3239 - accuracy: 1.0000 - val_loss: 0.3271 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.32778 to 0.32709, saving model to ./mod3.h5\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 0.3266 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.32709 to 0.32660, saving model to ./mod3.h5\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3221 - accuracy: 1.0000 - val_loss: 0.3247 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.32660 to 0.32470, saving model to ./mod3.h5\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3222 - accuracy: 1.0000 - val_loss: 0.3390 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.32470\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.3250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.32470\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3209 - accuracy: 1.0000 - val_loss: 0.3311 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.32470\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.3266 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.32470\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3259 - accuracy: 1.0000 - val_loss: 0.3259 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.32470\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3236 - accuracy: 1.0000 - val_loss: 0.3292 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.32470\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3231 - accuracy: 1.0000 - val_loss: 0.3249 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.32470\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3220 - accuracy: 1.0000 - val_loss: 0.3257 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.32470\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3213 - accuracy: 1.0000 - val_loss: 0.3231 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.32470 to 0.32312, saving model to ./mod3.h5\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3206 - accuracy: 1.0000 - val_loss: 0.3232 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.32312\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3186 - accuracy: 1.0000 - val_loss: 0.3227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.32312 to 0.32267, saving model to ./mod3.h5\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.32267\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3189 - accuracy: 1.0000 - val_loss: 0.3246 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.32267\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3212 - accuracy: 1.0000 - val_loss: 0.3321 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.32267\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3245 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.32267\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3214 - accuracy: 1.0000 - val_loss: 0.3248 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.32267\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3207 - accuracy: 1.0000 - val_loss: 0.3263 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.32267\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3236 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.32267\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3204 - accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.32267\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3200 - accuracy: 1.0000 - val_loss: 0.3233 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.32267\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3230 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.32267\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3184 - accuracy: 1.0000 - val_loss: 0.3218 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.32267 to 0.32175, saving model to ./mod3.h5\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3208 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.32175 to 0.32075, saving model to ./mod3.h5\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 0.3213 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.32075\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.32075\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3197 - accuracy: 1.0000 - val_loss: 0.3217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.32075\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3200 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.32075\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3196 - accuracy: 1.0000 - val_loss: 0.3238 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.32075\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3195 - accuracy: 1.0000 - val_loss: 0.3234 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.32075\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3195 - accuracy: 1.0000 - val_loss: 0.3244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.32075\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3218 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.32075\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3185 - accuracy: 1.0000 - val_loss: 0.3210 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.32075\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3175 - accuracy: 1.0000 - val_loss: 0.3202 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.32075 to 0.32021, saving model to ./mod3.h5\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3167 - accuracy: 1.0000 - val_loss: 0.3183 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.32021 to 0.31832, saving model to ./mod3.h5\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3189 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.31832\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3185 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.31832\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3188 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.31832\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3153 - accuracy: 1.0000 - val_loss: 0.3184 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.31832\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3157 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.31832\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3193 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.31832\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3157 - accuracy: 1.0000 - val_loss: 0.3187 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.31832\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3159 - accuracy: 1.0000 - val_loss: 0.3185 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.31832\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3169 - accuracy: 1.0000 - val_loss: 0.3190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.31832\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3164 - accuracy: 1.0000 - val_loss: 0.3190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.31832\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.3206 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.31832\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3165 - accuracy: 1.0000 - val_loss: 0.3177 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss improved from 0.31832 to 0.31775, saving model to ./mod3.h5\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3167 - accuracy: 1.0000 - val_loss: 0.3175 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.31775 to 0.31749, saving model to ./mod3.h5\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3187 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.31749\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3145 - accuracy: 1.0000 - val_loss: 0.3179 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.31749\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3150 - accuracy: 1.0000 - val_loss: 0.3173 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.31749 to 0.31732, saving model to ./mod3.h5\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.31732\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.3160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.31732 to 0.31598, saving model to ./mod3.h5\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3144 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.31598\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3139 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.31598\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3139 - accuracy: 1.0000 - val_loss: 0.3167 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.31598\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3140 - accuracy: 1.0000 - val_loss: 0.3170 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.31598\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3134 - accuracy: 1.0000 - val_loss: 0.3179 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.31598\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3144 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.31598\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3137 - accuracy: 1.0000 - val_loss: 0.3166 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.31598\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3146 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.31598\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3143 - accuracy: 1.0000 - val_loss: 0.3169 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.31598\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.3158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss improved from 0.31598 to 0.31582, saving model to ./mod3.h5\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3136 - accuracy: 1.0000 - val_loss: 0.3149 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.31582 to 0.31485, saving model to ./mod3.h5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 12ms/step - loss: 0.3123 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.5819 - accuracy: 0.9200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3149 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_217 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_219 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_221 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_223 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_225 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_227 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_229 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_231 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_233 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_235 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_237 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_239 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_241 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_243 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_245 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_247 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_249 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_251 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_253 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_255 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_257 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_259 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_261 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_263 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_265 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_267 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_269 (Lambda)             (None, 19, 3, 7, 1)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_216 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_218 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_220 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_222 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_224 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_226 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_228 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_230 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_232 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_234 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_236 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_238 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_240 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_242 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_244 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_246 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_248 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_250 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_252 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_254 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_256 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_258 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_260 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_262 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_264 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_266 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_268 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_108 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_109 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_110 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_111 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_112 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_113 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_114 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_115 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_116 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_117 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_118 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_119 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_120 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_121 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_122 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_123 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_124 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_125 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_126 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_127 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_128 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_129 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_130 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_131 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_132 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_133 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_134 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_108 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_109 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_110 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_111 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_114 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_115 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_116 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_117 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_118 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_119 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_120 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_121 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_122 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_123 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_124 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_125 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_126 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_127 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_128 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_129 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_130 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_131 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_132 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_133 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_134 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_108 (G (None, 8)            0           dropout_108[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_109 (G (None, 8)            0           dropout_109[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_110 (G (None, 8)            0           dropout_110[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_111 (G (None, 8)            0           dropout_111[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_112 (G (None, 8)            0           dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_113 (G (None, 8)            0           dropout_113[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_114 (G (None, 8)            0           dropout_114[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_115 (G (None, 8)            0           dropout_115[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_116 (G (None, 8)            0           dropout_116[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_117 (G (None, 8)            0           dropout_117[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_118 (G (None, 8)            0           dropout_118[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_119 (G (None, 8)            0           dropout_119[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_120 (G (None, 8)            0           dropout_120[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_121 (G (None, 8)            0           dropout_121[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_122 (G (None, 8)            0           dropout_122[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_123 (G (None, 8)            0           dropout_123[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_124 (G (None, 8)            0           dropout_124[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_125 (G (None, 8)            0           dropout_125[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_126 (G (None, 8)            0           dropout_126[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_127 (G (None, 8)            0           dropout_127[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_128 (G (None, 8)            0           dropout_128[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_129 (G (None, 8)            0           dropout_129[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_130 (G (None, 8)            0           dropout_130[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_131 (G (None, 8)            0           dropout_131[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_132 (G (None, 8)            0           dropout_132[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_133 (G (None, 8)            0           dropout_133[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_134 (G (None, 8)            0           dropout_134[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 216)          0           global_average_pooling3d_108[0][0\n",
            "                                                                 global_average_pooling3d_109[0][0\n",
            "                                                                 global_average_pooling3d_110[0][0\n",
            "                                                                 global_average_pooling3d_111[0][0\n",
            "                                                                 global_average_pooling3d_112[0][0\n",
            "                                                                 global_average_pooling3d_113[0][0\n",
            "                                                                 global_average_pooling3d_114[0][0\n",
            "                                                                 global_average_pooling3d_115[0][0\n",
            "                                                                 global_average_pooling3d_116[0][0\n",
            "                                                                 global_average_pooling3d_117[0][0\n",
            "                                                                 global_average_pooling3d_118[0][0\n",
            "                                                                 global_average_pooling3d_119[0][0\n",
            "                                                                 global_average_pooling3d_120[0][0\n",
            "                                                                 global_average_pooling3d_121[0][0\n",
            "                                                                 global_average_pooling3d_122[0][0\n",
            "                                                                 global_average_pooling3d_123[0][0\n",
            "                                                                 global_average_pooling3d_124[0][0\n",
            "                                                                 global_average_pooling3d_125[0][0\n",
            "                                                                 global_average_pooling3d_126[0][0\n",
            "                                                                 global_average_pooling3d_127[0][0\n",
            "                                                                 global_average_pooling3d_128[0][0\n",
            "                                                                 global_average_pooling3d_129[0][0\n",
            "                                                                 global_average_pooling3d_130[0][0\n",
            "                                                                 global_average_pooling3d_131[0][0\n",
            "                                                                 global_average_pooling3d_132[0][0\n",
            "                                                                 global_average_pooling3d_133[0][0\n",
            "                                                                 global_average_pooling3d_134[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 512)          111104      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 512)          262656      dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 512)          262656      dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 1)            513         dense_18[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 372ms/step - loss: 99.2574 - accuracy: 0.5610 - val_loss: 93.3634 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.36339, saving model to ./mod4.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 91.6729 - accuracy: 0.5366 - val_loss: 86.0929 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.36339 to 86.09286, saving model to ./mod4.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 84.4719 - accuracy: 0.6098 - val_loss: 79.1192 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.09286 to 79.11916, saving model to ./mod4.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 77.5543 - accuracy: 0.8293 - val_loss: 72.4670 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.11916 to 72.46701, saving model to ./mod4.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 70.9387 - accuracy: 0.8171 - val_loss: 66.1034 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.46701 to 66.10344, saving model to ./mod4.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 64.6737 - accuracy: 0.8902 - val_loss: 60.0802 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.10344 to 60.08016, saving model to ./mod4.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 58.6780 - accuracy: 0.8659 - val_loss: 54.4303 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.08016 to 54.43034, saving model to ./mod4.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 53.0251 - accuracy: 0.8780 - val_loss: 48.9315 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.43034 to 48.93154, saving model to ./mod4.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 47.7110 - accuracy: 0.8293 - val_loss: 43.8189 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.93154 to 43.81894, saving model to ./mod4.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 42.6463 - accuracy: 0.9024 - val_loss: 39.1364 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.81894 to 39.13642, saving model to ./mod4.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 37.9029 - accuracy: 0.9146 - val_loss: 34.4854 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00011: val_loss improved from 39.13642 to 34.48539, saving model to ./mod4.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 33.4705 - accuracy: 0.8780 - val_loss: 30.2635 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.48539 to 30.26346, saving model to ./mod4.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 29.2818 - accuracy: 0.9634 - val_loss: 26.5742 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.26346 to 26.57424, saving model to ./mod4.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 25.4903 - accuracy: 0.8780 - val_loss: 22.7508 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.57424 to 22.75078, saving model to ./mod4.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 21.8860 - accuracy: 0.9756 - val_loss: 19.3744 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.75078 to 19.37437, saving model to ./mod4.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 18.5876 - accuracy: 0.9390 - val_loss: 16.4254 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.37437 to 16.42537, saving model to ./mod4.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 15.5987 - accuracy: 0.9512 - val_loss: 13.5818 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.42537 to 13.58182, saving model to ./mod4.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 12.9303 - accuracy: 0.9634 - val_loss: 11.1251 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.58182 to 11.12508, saving model to ./mod4.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 10.5074 - accuracy: 1.0000 - val_loss: 9.0146 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.12508 to 9.01455, saving model to ./mod4.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 8.4099 - accuracy: 0.9878 - val_loss: 7.0557 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 9.01455 to 7.05569, saving model to ./mod4.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 6.6178 - accuracy: 1.0000 - val_loss: 5.5376 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.05569 to 5.53761, saving model to ./mod4.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 5.0933 - accuracy: 1.0000 - val_loss: 4.2455 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.53761 to 4.24551, saving model to ./mod4.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 3.8903 - accuracy: 0.9878 - val_loss: 3.1943 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.24551 to 3.19432, saving model to ./mod4.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 2.9973 - accuracy: 1.0000 - val_loss: 2.7226 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.19432 to 2.72262, saving model to ./mod4.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 2.3808 - accuracy: 1.0000 - val_loss: 2.1870 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.72262 to 2.18696, saving model to ./mod4.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 2.0760 - accuracy: 0.9756 - val_loss: 1.9871 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.18696 to 1.98709, saving model to ./mod4.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 1.8908 - accuracy: 1.0000 - val_loss: 1.9430 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.98709 to 1.94301, saving model to ./mod4.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 1.6694 - accuracy: 0.9878 - val_loss: 1.4947 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.94301 to 1.49469, saving model to ./mod4.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.3969 - accuracy: 1.0000 - val_loss: 1.2851 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.49469 to 1.28506, saving model to ./mod4.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 1.2438 - accuracy: 0.9512 - val_loss: 1.0977 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.28506 to 1.09771, saving model to ./mod4.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 1.1533 - accuracy: 0.9634 - val_loss: 1.1917 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.09771\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.1255 - accuracy: 0.9878 - val_loss: 1.2070 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.09771\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 1.0918 - accuracy: 0.9390 - val_loss: 1.3333 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.09771\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 1.0005 - accuracy: 0.9634 - val_loss: 0.9294 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.09771 to 0.92937, saving model to ./mod4.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.8417 - accuracy: 1.0000 - val_loss: 0.8204 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.92937 to 0.82044, saving model to ./mod4.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.7832 - accuracy: 0.9756 - val_loss: 0.8729 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.82044\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.6938 - accuracy: 1.0000 - val_loss: 0.6996 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.82044 to 0.69955, saving model to ./mod4.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.6202 - accuracy: 1.0000 - val_loss: 0.6526 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.69955 to 0.65265, saving model to ./mod4.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5844 - accuracy: 1.0000 - val_loss: 0.6668 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.65265\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.5347 - accuracy: 1.0000 - val_loss: 0.5815 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.65265 to 0.58147, saving model to ./mod4.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5077 - accuracy: 1.0000 - val_loss: 0.5708 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.58147 to 0.57077, saving model to ./mod4.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4799 - accuracy: 1.0000 - val_loss: 0.5158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.57077 to 0.51579, saving model to ./mod4.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4675 - accuracy: 1.0000 - val_loss: 0.5404 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.51579\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4548 - accuracy: 1.0000 - val_loss: 0.5123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.51579 to 0.51228, saving model to ./mod4.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4343 - accuracy: 1.0000 - val_loss: 0.4862 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.51228 to 0.48618, saving model to ./mod4.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4282 - accuracy: 1.0000 - val_loss: 0.5006 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.48618\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4128 - accuracy: 1.0000 - val_loss: 0.4527 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.48618 to 0.45268, saving model to ./mod4.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4113 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.45268\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4007 - accuracy: 1.0000 - val_loss: 0.4502 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.45268 to 0.45019, saving model to ./mod4.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3956 - accuracy: 1.0000 - val_loss: 0.4586 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.45019\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3891 - accuracy: 1.0000 - val_loss: 0.4549 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.45019\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3820 - accuracy: 1.0000 - val_loss: 0.4553 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.45019\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3823 - accuracy: 1.0000 - val_loss: 0.4302 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.45019 to 0.43025, saving model to ./mod4.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3753 - accuracy: 1.0000 - val_loss: 0.4732 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.43025\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3716 - accuracy: 1.0000 - val_loss: 0.4127 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.43025 to 0.41272, saving model to ./mod4.h5\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3715 - accuracy: 1.0000 - val_loss: 0.4375 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.41272\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3685 - accuracy: 1.0000 - val_loss: 0.4124 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.41272 to 0.41240, saving model to ./mod4.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3691 - accuracy: 1.0000 - val_loss: 0.4058 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.41240 to 0.40583, saving model to ./mod4.h5\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3659 - accuracy: 1.0000 - val_loss: 0.3926 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.40583 to 0.39261, saving model to ./mod4.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3619 - accuracy: 1.0000 - val_loss: 0.4250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.39261\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3596 - accuracy: 1.0000 - val_loss: 0.4082 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.39261\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3614 - accuracy: 1.0000 - val_loss: 0.4234 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.39261\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3599 - accuracy: 1.0000 - val_loss: 0.4515 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.39261\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3630 - accuracy: 1.0000 - val_loss: 0.3907 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.39261 to 0.39071, saving model to ./mod4.h5\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3601 - accuracy: 1.0000 - val_loss: 0.4732 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.39071\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3611 - accuracy: 1.0000 - val_loss: 0.3895 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.39071 to 0.38955, saving model to ./mod4.h5\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3597 - accuracy: 1.0000 - val_loss: 0.4499 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.38955\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3558 - accuracy: 1.0000 - val_loss: 0.3950 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.38955\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3526 - accuracy: 1.0000 - val_loss: 0.4395 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.38955\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3488 - accuracy: 1.0000 - val_loss: 0.3741 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.38955 to 0.37409, saving model to ./mod4.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3765 - accuracy: 0.9878 - val_loss: 0.5268 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.37409\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3722 - accuracy: 1.0000 - val_loss: 0.4411 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.37409\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3603 - accuracy: 1.0000 - val_loss: 0.3868 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.37409\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3585 - accuracy: 1.0000 - val_loss: 0.3942 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.37409\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3513 - accuracy: 1.0000 - val_loss: 0.3763 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.37409\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3484 - accuracy: 1.0000 - val_loss: 0.3821 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.37409\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3451 - accuracy: 1.0000 - val_loss: 0.3680 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.37409 to 0.36800, saving model to ./mod4.h5\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3430 - accuracy: 1.0000 - val_loss: 0.3860 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.36800\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3410 - accuracy: 1.0000 - val_loss: 0.3643 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.36800 to 0.36429, saving model to ./mod4.h5\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3445 - accuracy: 1.0000 - val_loss: 0.4115 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.36429\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3445 - accuracy: 1.0000 - val_loss: 0.3726 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.36429\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3427 - accuracy: 1.0000 - val_loss: 0.4076 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.36429\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3407 - accuracy: 1.0000 - val_loss: 0.3909 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.36429\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3396 - accuracy: 1.0000 - val_loss: 0.3713 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.36429\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3399 - accuracy: 1.0000 - val_loss: 0.4045 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.36429\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3380 - accuracy: 1.0000 - val_loss: 0.3590 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.36429 to 0.35896, saving model to ./mod4.h5\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3404 - accuracy: 1.0000 - val_loss: 0.3950 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.35896\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3372 - accuracy: 1.0000 - val_loss: 0.3744 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.35896\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3353 - accuracy: 1.0000 - val_loss: 0.3794 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.35896\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3364 - accuracy: 1.0000 - val_loss: 0.3670 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.35896\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3336 - accuracy: 1.0000 - val_loss: 0.3602 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.35896\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3327 - accuracy: 1.0000 - val_loss: 0.3788 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.35896\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3314 - accuracy: 1.0000 - val_loss: 0.3692 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.35896\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3295 - accuracy: 1.0000 - val_loss: 0.3761 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.35896\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3303 - accuracy: 1.0000 - val_loss: 0.3878 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.35896\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3297 - accuracy: 1.0000 - val_loss: 0.3926 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.35896\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3309 - accuracy: 1.0000 - val_loss: 0.3745 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.35896\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3289 - accuracy: 1.0000 - val_loss: 0.3832 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.35896\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3294 - accuracy: 1.0000 - val_loss: 0.3607 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.35896\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3278 - accuracy: 1.0000 - val_loss: 0.3718 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.35896\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3278 - accuracy: 1.0000 - val_loss: 0.3605 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.35896\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3288 - accuracy: 1.0000 - val_loss: 0.3703 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.35896\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3275 - accuracy: 1.0000 - val_loss: 0.3605 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.35896\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3279 - accuracy: 1.0000 - val_loss: 0.3734 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.35896\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3285 - accuracy: 1.0000 - val_loss: 0.3637 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.35896\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3275 - accuracy: 1.0000 - val_loss: 0.4016 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.35896\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3275 - accuracy: 1.0000 - val_loss: 0.3690 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.35896\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3265 - accuracy: 1.0000 - val_loss: 0.3896 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.35896\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3268 - accuracy: 1.0000 - val_loss: 0.3682 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.35896\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3262 - accuracy: 1.0000 - val_loss: 0.3718 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.35896\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3254 - accuracy: 1.0000 - val_loss: 0.3738 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.35896\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3260 - accuracy: 1.0000 - val_loss: 0.3508 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.35896 to 0.35084, saving model to ./mod4.h5\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3255 - accuracy: 1.0000 - val_loss: 0.3799 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.35084\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3253 - accuracy: 1.0000 - val_loss: 0.3523 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.35084\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.4413 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.35084\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.3324 - accuracy: 1.0000 - val_loss: 0.3478 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.35084 to 0.34777, saving model to ./mod4.h5\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3299 - accuracy: 1.0000 - val_loss: 0.3736 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.34777\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3266 - accuracy: 1.0000 - val_loss: 0.3521 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.34777\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3266 - accuracy: 1.0000 - val_loss: 0.3671 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.34777\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3284 - accuracy: 1.0000 - val_loss: 0.3400 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.34777 to 0.33996, saving model to ./mod4.h5\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3271 - accuracy: 1.0000 - val_loss: 0.3753 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.33996\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3264 - accuracy: 1.0000 - val_loss: 0.3540 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.33996\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3244 - accuracy: 1.0000 - val_loss: 0.3639 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.33996\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3219 - accuracy: 1.0000 - val_loss: 0.3648 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.33996\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3230 - accuracy: 1.0000 - val_loss: 0.3471 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.33996\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3224 - accuracy: 1.0000 - val_loss: 0.3726 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.33996\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3658 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.33996\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3230 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.33996\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.3550 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.33996\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3213 - accuracy: 1.0000 - val_loss: 0.3623 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.33996\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3203 - accuracy: 1.0000 - val_loss: 0.3521 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.33996\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3209 - accuracy: 1.0000 - val_loss: 0.3589 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.33996\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3213 - accuracy: 1.0000 - val_loss: 0.3469 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.33996\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3202 - accuracy: 1.0000 - val_loss: 0.3580 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.33996\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.33996\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3196 - accuracy: 1.0000 - val_loss: 0.3486 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.33996\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3653 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.33996\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3184 - accuracy: 1.0000 - val_loss: 0.3637 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.33996\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3191 - accuracy: 1.0000 - val_loss: 0.3620 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.33996\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3180 - accuracy: 1.0000 - val_loss: 0.3679 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.33996\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3651 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.33996\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3181 - accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.33996\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3170 - accuracy: 1.0000 - val_loss: 0.3628 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.33996\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3171 - accuracy: 1.0000 - val_loss: 0.3753 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.33996\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3181 - accuracy: 1.0000 - val_loss: 0.3595 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.33996\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3174 - accuracy: 1.0000 - val_loss: 0.3516 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.33996\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3169 - accuracy: 1.0000 - val_loss: 0.3545 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.33996\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3168 - accuracy: 1.0000 - val_loss: 0.3495 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.33996\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3180 - accuracy: 1.0000 - val_loss: 0.3339 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.33996 to 0.33391, saving model to ./mod4.h5\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3211 - accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.33391\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3224 - accuracy: 1.0000 - val_loss: 0.3579 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.33391\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.3522 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.33391\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3197 - accuracy: 1.0000 - val_loss: 0.3563 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.33391\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3181 - accuracy: 1.0000 - val_loss: 0.3524 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.33391\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3165 - accuracy: 1.0000 - val_loss: 0.3610 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.33391\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3164 - accuracy: 1.0000 - val_loss: 0.3404 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.33391\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.3456 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.33391\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.3473 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.33391\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.3740 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.33391\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3171 - accuracy: 1.0000 - val_loss: 0.3463 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.33391\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3158 - accuracy: 1.0000 - val_loss: 0.3473 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.33391\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3421 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.33391\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3498 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.33391\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3143 - accuracy: 1.0000 - val_loss: 0.3423 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.33391\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.3502 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.33391\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.33391 to 0.33249, saving model to ./mod4.h5\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3171 - accuracy: 1.0000 - val_loss: 0.3409 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.33249\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3171 - accuracy: 1.0000 - val_loss: 0.3551 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.33249\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3167 - accuracy: 1.0000 - val_loss: 0.3307 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.33249 to 0.33071, saving model to ./mod4.h5\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3171 - accuracy: 1.0000 - val_loss: 0.3602 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.33071\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.3349 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.33071\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3140 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.33071\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3139 - accuracy: 1.0000 - val_loss: 0.3409 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.33071\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3131 - accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.33071\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3128 - accuracy: 1.0000 - val_loss: 0.3516 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.33071\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3142 - accuracy: 1.0000 - val_loss: 0.3307 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.33071 to 0.33066, saving model to ./mod4.h5\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3143 - accuracy: 1.0000 - val_loss: 0.3410 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.33066\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 0.3690 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.33066\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3156 - accuracy: 1.0000 - val_loss: 0.3407 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.33066\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3144 - accuracy: 1.0000 - val_loss: 0.3505 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.33066\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3137 - accuracy: 1.0000 - val_loss: 0.3411 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.33066\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3130 - accuracy: 1.0000 - val_loss: 0.3457 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.33066\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3129 - accuracy: 1.0000 - val_loss: 0.3438 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.33066\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3120 - accuracy: 1.0000 - val_loss: 0.3401 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.33066\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3122 - accuracy: 1.0000 - val_loss: 0.3360 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.33066\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3120 - accuracy: 1.0000 - val_loss: 0.3454 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.33066\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3122 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.33066\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3120 - accuracy: 1.0000 - val_loss: 0.3389 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.33066\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3129 - accuracy: 1.0000 - val_loss: 0.3385 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.33066\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3130 - accuracy: 1.0000 - val_loss: 0.3446 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.33066\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3124 - accuracy: 1.0000 - val_loss: 0.3334 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.33066\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3132 - accuracy: 1.0000 - val_loss: 0.3426 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.33066\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3125 - accuracy: 1.0000 - val_loss: 0.3372 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.33066\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3123 - accuracy: 1.0000 - val_loss: 0.3350 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.33066\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3130 - accuracy: 1.0000 - val_loss: 0.3367 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.33066\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3115 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.33066\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3129 - accuracy: 1.0000 - val_loss: 0.3399 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.33066\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3119 - accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.33066\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3118 - accuracy: 1.0000 - val_loss: 0.3547 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.33066\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3112 - accuracy: 1.0000 - val_loss: 0.3315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.33066\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 13ms/step - loss: 0.3141 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4198 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3307 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_271 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_273 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_275 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_277 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_279 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_281 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_283 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_285 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_287 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_289 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_291 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_293 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_295 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_297 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_299 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_301 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_303 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_305 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_307 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_309 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_311 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_313 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_315 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_317 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_319 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_321 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_323 (Lambda)             (None, 19, 3, 7, 1)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_270 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_272 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_274 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_276 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_278 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_280 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_282 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_283[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_284 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_285[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_286 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_287[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_288 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_289[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_290 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_291[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_292 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_293[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_294 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_295[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_296 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_297[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_298 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_299[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_300 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_301[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_302 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_303[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_304 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_305[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_306 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_307[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_308 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_309[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_310 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_311[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_312 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_313[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_314 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_315[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_316 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_317[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_318 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_319[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_320 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_321[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_322 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_323[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_135 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_136 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_137 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_138 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_139 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_140 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_141 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_282[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_142 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_284[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_143 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_286[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_144 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_288[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_145 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_290[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_146 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_292[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_147 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_294[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_148 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_296[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_149 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_298[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_150 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_300[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_151 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_302[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_152 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_304[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_153 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_306[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_154 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_308[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_155 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_310[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_156 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_312[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_157 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_314[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_158 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_316[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_159 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_318[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_160 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_320[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_161 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_322[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_135 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_136 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_137 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_138 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_139 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_140 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_141 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_142 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_143 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_144 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_145 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_146 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_147 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_148 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_149 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_150 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_151 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_152 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_153 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_154 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_155 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_156 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_157 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_158 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_159 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_160 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_161 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_135 (G (None, 8)            0           dropout_135[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_136 (G (None, 8)            0           dropout_136[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_137 (G (None, 8)            0           dropout_137[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_138 (G (None, 8)            0           dropout_138[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_139 (G (None, 8)            0           dropout_139[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_140 (G (None, 8)            0           dropout_140[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_141 (G (None, 8)            0           dropout_141[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_142 (G (None, 8)            0           dropout_142[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_143 (G (None, 8)            0           dropout_143[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_144 (G (None, 8)            0           dropout_144[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_145 (G (None, 8)            0           dropout_145[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_146 (G (None, 8)            0           dropout_146[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_147 (G (None, 8)            0           dropout_147[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_148 (G (None, 8)            0           dropout_148[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_149 (G (None, 8)            0           dropout_149[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_150 (G (None, 8)            0           dropout_150[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_151 (G (None, 8)            0           dropout_151[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_152 (G (None, 8)            0           dropout_152[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_153 (G (None, 8)            0           dropout_153[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_154 (G (None, 8)            0           dropout_154[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_155 (G (None, 8)            0           dropout_155[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_156 (G (None, 8)            0           dropout_156[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_157 (G (None, 8)            0           dropout_157[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_158 (G (None, 8)            0           dropout_158[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_159 (G (None, 8)            0           dropout_159[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_160 (G (None, 8)            0           dropout_160[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_161 (G (None, 8)            0           dropout_161[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 216)          0           global_average_pooling3d_135[0][0\n",
            "                                                                 global_average_pooling3d_136[0][0\n",
            "                                                                 global_average_pooling3d_137[0][0\n",
            "                                                                 global_average_pooling3d_138[0][0\n",
            "                                                                 global_average_pooling3d_139[0][0\n",
            "                                                                 global_average_pooling3d_140[0][0\n",
            "                                                                 global_average_pooling3d_141[0][0\n",
            "                                                                 global_average_pooling3d_142[0][0\n",
            "                                                                 global_average_pooling3d_143[0][0\n",
            "                                                                 global_average_pooling3d_144[0][0\n",
            "                                                                 global_average_pooling3d_145[0][0\n",
            "                                                                 global_average_pooling3d_146[0][0\n",
            "                                                                 global_average_pooling3d_147[0][0\n",
            "                                                                 global_average_pooling3d_148[0][0\n",
            "                                                                 global_average_pooling3d_149[0][0\n",
            "                                                                 global_average_pooling3d_150[0][0\n",
            "                                                                 global_average_pooling3d_151[0][0\n",
            "                                                                 global_average_pooling3d_152[0][0\n",
            "                                                                 global_average_pooling3d_153[0][0\n",
            "                                                                 global_average_pooling3d_154[0][0\n",
            "                                                                 global_average_pooling3d_155[0][0\n",
            "                                                                 global_average_pooling3d_156[0][0\n",
            "                                                                 global_average_pooling3d_157[0][0\n",
            "                                                                 global_average_pooling3d_158[0][0\n",
            "                                                                 global_average_pooling3d_159[0][0\n",
            "                                                                 global_average_pooling3d_160[0][0\n",
            "                                                                 global_average_pooling3d_161[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 512)          111104      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 512)          262656      dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 512)          262656      dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 1)            513         dense_22[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 384ms/step - loss: 99.1875 - accuracy: 0.4940 - val_loss: 93.2709 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.27088, saving model to ./mod5.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 91.5219 - accuracy: 0.7590 - val_loss: 85.8712 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.27088 to 85.87120, saving model to ./mod5.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 84.2305 - accuracy: 0.7952 - val_loss: 78.8403 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00003: val_loss improved from 85.87120 to 78.84032, saving model to ./mod5.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 77.2798 - accuracy: 0.8554 - val_loss: 72.1366 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00004: val_loss improved from 78.84032 to 72.13658, saving model to ./mod5.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 70.6719 - accuracy: 0.8675 - val_loss: 65.7978 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.13658 to 65.79782, saving model to ./mod5.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 64.3580 - accuracy: 0.9277 - val_loss: 59.7681 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00006: val_loss improved from 65.79782 to 59.76813, saving model to ./mod5.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 58.4191 - accuracy: 0.9036 - val_loss: 54.0694 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00007: val_loss improved from 59.76813 to 54.06937, saving model to ./mod5.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 52.7522 - accuracy: 0.9639 - val_loss: 48.7427 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.06937 to 48.74268, saving model to ./mod5.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 47.4783 - accuracy: 0.9277 - val_loss: 43.6652 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.74268 to 43.66518, saving model to ./mod5.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 42.5198 - accuracy: 0.9036 - val_loss: 38.8381 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.66518 to 38.83809, saving model to ./mod5.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 37.7692 - accuracy: 0.9759 - val_loss: 34.3688 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.83809 to 34.36878, saving model to ./mod5.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 33.3099 - accuracy: 0.9759 - val_loss: 30.1688 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.36878 to 30.16885, saving model to ./mod5.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 29.1894 - accuracy: 0.9880 - val_loss: 26.2452 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.16885 to 26.24519, saving model to ./mod5.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 25.3316 - accuracy: 0.9880 - val_loss: 22.6179 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.24519 to 22.61786, saving model to ./mod5.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 21.7735 - accuracy: 1.0000 - val_loss: 19.2884 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.61786 to 19.28836, saving model to ./mod5.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 18.5350 - accuracy: 0.9880 - val_loss: 16.2957 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.28836 to 16.29573, saving model to ./mod5.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 15.5678 - accuracy: 0.9880 - val_loss: 13.5039 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.29573 to 13.50388, saving model to ./mod5.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 12.8863 - accuracy: 1.0000 - val_loss: 11.0960 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.50388 to 11.09596, saving model to ./mod5.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 10.5064 - accuracy: 1.0000 - val_loss: 8.8985 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.09596 to 8.89849, saving model to ./mod5.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 8.4322 - accuracy: 0.9880 - val_loss: 7.0807 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.89849 to 7.08066, saving model to ./mod5.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 6.6876 - accuracy: 0.9759 - val_loss: 5.4965 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.08066 to 5.49651, saving model to ./mod5.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 5.1590 - accuracy: 1.0000 - val_loss: 4.2195 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.49651 to 4.21947, saving model to ./mod5.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 3.9362 - accuracy: 1.0000 - val_loss: 3.2460 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.21947 to 3.24604, saving model to ./mod5.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 3.0135 - accuracy: 1.0000 - val_loss: 2.5611 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.24604 to 2.56106, saving model to ./mod5.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 2.4178 - accuracy: 0.9880 - val_loss: 2.2033 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.56106 to 2.20329, saving model to ./mod5.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 2.0827 - accuracy: 1.0000 - val_loss: 2.0669 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.20329 to 2.06692, saving model to ./mod5.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.9710 - accuracy: 0.9880 - val_loss: 1.7909 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.06692 to 1.79093, saving model to ./mod5.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 1.7078 - accuracy: 1.0000 - val_loss: 1.5842 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.79093 to 1.58418, saving model to ./mod5.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 1.4312 - accuracy: 0.9880 - val_loss: 1.2467 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.58418 to 1.24671, saving model to ./mod5.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 1.1803 - accuracy: 1.0000 - val_loss: 1.1604 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.24671 to 1.16037, saving model to ./mod5.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 1.1061 - accuracy: 0.9880 - val_loss: 1.1864 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.16037\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 1.0511 - accuracy: 0.9880 - val_loss: 0.9474 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.16037 to 0.94740, saving model to ./mod5.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.8912 - accuracy: 1.0000 - val_loss: 0.8323 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.94740 to 0.83230, saving model to ./mod5.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.7686 - accuracy: 1.0000 - val_loss: 0.7376 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.83230 to 0.73764, saving model to ./mod5.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.6984 - accuracy: 1.0000 - val_loss: 0.7636 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.73764\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.6670 - accuracy: 1.0000 - val_loss: 0.6440 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.73764 to 0.64401, saving model to ./mod5.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.6149 - accuracy: 1.0000 - val_loss: 0.6123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.64401 to 0.61227, saving model to ./mod5.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.5739 - accuracy: 1.0000 - val_loss: 0.5649 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.61227 to 0.56487, saving model to ./mod5.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.5451 - accuracy: 1.0000 - val_loss: 0.5426 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.56487 to 0.54262, saving model to ./mod5.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.5105 - accuracy: 1.0000 - val_loss: 0.5238 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.54262 to 0.52379, saving model to ./mod5.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4854 - accuracy: 1.0000 - val_loss: 0.4951 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.52379 to 0.49513, saving model to ./mod5.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4704 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.49513 to 0.48502, saving model to ./mod5.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4590 - accuracy: 1.0000 - val_loss: 0.4731 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.48502 to 0.47312, saving model to ./mod5.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4420 - accuracy: 1.0000 - val_loss: 0.4930 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.47312\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.4313 - accuracy: 1.0000 - val_loss: 0.4470 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.47312 to 0.44700, saving model to ./mod5.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4240 - accuracy: 1.0000 - val_loss: 0.4412 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.44700 to 0.44125, saving model to ./mod5.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.4229 - accuracy: 1.0000 - val_loss: 0.5120 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.44125\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4287 - accuracy: 1.0000 - val_loss: 0.4861 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.44125\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.4386 - accuracy: 1.0000 - val_loss: 0.5286 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.44125\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4288 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.44125\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4522 - accuracy: 1.0000 - val_loss: 0.4515 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.44125\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.4367 - accuracy: 1.0000 - val_loss: 0.4430 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.44125\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4109 - accuracy: 1.0000 - val_loss: 0.4211 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.44125 to 0.42110, saving model to ./mod5.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4084 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.42110\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3982 - accuracy: 1.0000 - val_loss: 0.4060 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.42110 to 0.40599, saving model to ./mod5.h5\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3835 - accuracy: 1.0000 - val_loss: 0.4163 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.40599\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3807 - accuracy: 1.0000 - val_loss: 0.4044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.40599 to 0.40437, saving model to ./mod5.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3779 - accuracy: 1.0000 - val_loss: 0.4043 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.40437 to 0.40432, saving model to ./mod5.h5\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3765 - accuracy: 1.0000 - val_loss: 0.3945 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.40432 to 0.39447, saving model to ./mod5.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3741 - accuracy: 1.0000 - val_loss: 0.3871 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.39447 to 0.38712, saving model to ./mod5.h5\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3725 - accuracy: 1.0000 - val_loss: 0.3986 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.38712\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3653 - accuracy: 1.0000 - val_loss: 0.3848 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.38712 to 0.38482, saving model to ./mod5.h5\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3645 - accuracy: 1.0000 - val_loss: 0.3927 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.38482\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3602 - accuracy: 1.0000 - val_loss: 0.3931 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.38482\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3589 - accuracy: 1.0000 - val_loss: 0.3776 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.38482 to 0.37764, saving model to ./mod5.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3618 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.37764\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3588 - accuracy: 1.0000 - val_loss: 0.3793 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.37764\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3542 - accuracy: 1.0000 - val_loss: 0.3852 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.37764\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3549 - accuracy: 1.0000 - val_loss: 0.3965 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.37764\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3543 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.37764\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3522 - accuracy: 1.0000 - val_loss: 0.3705 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.37764 to 0.37046, saving model to ./mod5.h5\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3518 - accuracy: 1.0000 - val_loss: 0.3782 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.37046\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3490 - accuracy: 1.0000 - val_loss: 0.4032 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.37046\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3505 - accuracy: 1.0000 - val_loss: 0.3665 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.37046 to 0.36650, saving model to ./mod5.h5\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3490 - accuracy: 1.0000 - val_loss: 0.4388 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.36650\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3506 - accuracy: 1.0000 - val_loss: 0.3655 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.36650 to 0.36554, saving model to ./mod5.h5\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3514 - accuracy: 1.0000 - val_loss: 0.4309 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.36554\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3547 - accuracy: 1.0000 - val_loss: 0.3655 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.36554\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3520 - accuracy: 1.0000 - val_loss: 0.3950 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.36554\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3490 - accuracy: 1.0000 - val_loss: 0.3623 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.36554 to 0.36226, saving model to ./mod5.h5\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3525 - accuracy: 1.0000 - val_loss: 0.4365 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.36226\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3549 - accuracy: 1.0000 - val_loss: 0.3616 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.36226 to 0.36161, saving model to ./mod5.h5\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3489 - accuracy: 1.0000 - val_loss: 0.3900 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.36161\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3452 - accuracy: 1.0000 - val_loss: 0.3603 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.36161 to 0.36026, saving model to ./mod5.h5\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3478 - accuracy: 1.0000 - val_loss: 0.3941 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.36026\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3464 - accuracy: 1.0000 - val_loss: 0.3595 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.36026 to 0.35950, saving model to ./mod5.h5\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3434 - accuracy: 1.0000 - val_loss: 0.3650 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.35950\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3416 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.35950\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3400 - accuracy: 1.0000 - val_loss: 0.3652 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.35950\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3379 - accuracy: 1.0000 - val_loss: 0.3576 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.35950 to 0.35760, saving model to ./mod5.h5\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3371 - accuracy: 1.0000 - val_loss: 0.3709 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.35760\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3385 - accuracy: 1.0000 - val_loss: 0.3554 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.35760 to 0.35537, saving model to ./mod5.h5\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3366 - accuracy: 1.0000 - val_loss: 0.3600 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.35537\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3368 - accuracy: 1.0000 - val_loss: 0.3551 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.35537 to 0.35507, saving model to ./mod5.h5\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3355 - accuracy: 1.0000 - val_loss: 0.3624 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.35507\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3335 - accuracy: 1.0000 - val_loss: 0.3514 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.35507 to 0.35145, saving model to ./mod5.h5\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3335 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.35145\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3336 - accuracy: 1.0000 - val_loss: 0.3494 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.35145 to 0.34942, saving model to ./mod5.h5\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3343 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.34942\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3349 - accuracy: 1.0000 - val_loss: 0.3453 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.34942 to 0.34529, saving model to ./mod5.h5\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3337 - accuracy: 1.0000 - val_loss: 0.4076 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.34529\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3372 - accuracy: 1.0000 - val_loss: 0.3480 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.34529\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3337 - accuracy: 1.0000 - val_loss: 0.3562 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.34529\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3338 - accuracy: 1.0000 - val_loss: 0.3537 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.34529\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3331 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.34529\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3413 - accuracy: 1.0000 - val_loss: 0.3511 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.34529\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3458 - accuracy: 1.0000 - val_loss: 0.3516 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.34529\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3372 - accuracy: 1.0000 - val_loss: 0.3595 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.34529\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3375 - accuracy: 1.0000 - val_loss: 0.3522 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.34529\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3336 - accuracy: 1.0000 - val_loss: 0.3610 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.34529\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3328 - accuracy: 1.0000 - val_loss: 0.3467 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.34529\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3314 - accuracy: 1.0000 - val_loss: 0.3447 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.34529 to 0.34467, saving model to ./mod5.h5\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3310 - accuracy: 1.0000 - val_loss: 0.3618 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.34467\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3297 - accuracy: 1.0000 - val_loss: 0.3435 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.34467 to 0.34345, saving model to ./mod5.h5\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3293 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.34345\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3280 - accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.34345\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3290 - accuracy: 1.0000 - val_loss: 0.3683 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.34345\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3281 - accuracy: 1.0000 - val_loss: 0.3442 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.34345\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3270 - accuracy: 1.0000 - val_loss: 0.3588 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.34345\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3269 - accuracy: 1.0000 - val_loss: 0.3556 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.34345\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3263 - accuracy: 1.0000 - val_loss: 0.3565 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.34345\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3267 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.34345\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3274 - accuracy: 1.0000 - val_loss: 0.3496 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.34345\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3264 - accuracy: 1.0000 - val_loss: 0.3476 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.34345\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3277 - accuracy: 1.0000 - val_loss: 0.3501 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.34345\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3255 - accuracy: 1.0000 - val_loss: 0.3538 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.34345\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3286 - accuracy: 1.0000 - val_loss: 0.3390 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.34345 to 0.33901, saving model to ./mod5.h5\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.33901\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3255 - accuracy: 1.0000 - val_loss: 0.3478 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.33901\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3264 - accuracy: 1.0000 - val_loss: 0.3544 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.33901\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3262 - accuracy: 1.0000 - val_loss: 0.5383 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.33901\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3501 - accuracy: 1.0000 - val_loss: 0.3598 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.33901\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3557 - accuracy: 1.0000 - val_loss: 0.3460 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.33901\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3438 - accuracy: 1.0000 - val_loss: 0.3518 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.33901\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3398 - accuracy: 1.0000 - val_loss: 0.3441 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.33901\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3353 - accuracy: 1.0000 - val_loss: 0.3712 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.33901\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3348 - accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.33901\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3308 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.33901 to 0.33701, saving model to ./mod5.h5\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3296 - accuracy: 1.0000 - val_loss: 0.3439 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.33701\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3264 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.33701 to 0.33565, saving model to ./mod5.h5\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3267 - accuracy: 1.0000 - val_loss: 0.3403 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.33565\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3254 - accuracy: 1.0000 - val_loss: 0.3535 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.33565\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3263 - accuracy: 1.0000 - val_loss: 0.3389 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.33565\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3249 - accuracy: 1.0000 - val_loss: 0.3346 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.33565 to 0.33458, saving model to ./mod5.h5\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3237 - accuracy: 1.0000 - val_loss: 0.3530 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.33458\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3252 - accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.33458 to 0.33015, saving model to ./mod5.h5\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3248 - accuracy: 1.0000 - val_loss: 0.3401 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.33015\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.3334 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.33015\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3222 - accuracy: 1.0000 - val_loss: 0.3351 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.33015\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3235 - accuracy: 1.0000 - val_loss: 0.3334 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.33015\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3232 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.33015\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3222 - accuracy: 1.0000 - val_loss: 0.3318 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.33015\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3238 - accuracy: 1.0000 - val_loss: 0.3453 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.33015\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3236 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.33015\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3231 - accuracy: 1.0000 - val_loss: 0.3408 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.33015\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3215 - accuracy: 1.0000 - val_loss: 0.3312 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.33015\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.3463 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.33015\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3227 - accuracy: 1.0000 - val_loss: 0.3315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.33015\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3216 - accuracy: 1.0000 - val_loss: 0.3329 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.33015\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3200 - accuracy: 1.0000 - val_loss: 0.3277 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.33015 to 0.32765, saving model to ./mod5.h5\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3204 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.32765\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3191 - accuracy: 1.0000 - val_loss: 0.3287 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.32765\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3193 - accuracy: 1.0000 - val_loss: 0.3301 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.32765\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 0.3278 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.32765\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3187 - accuracy: 1.0000 - val_loss: 0.3317 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.32765\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3180 - accuracy: 1.0000 - val_loss: 0.3311 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.32765\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3184 - accuracy: 1.0000 - val_loss: 0.3286 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.32765\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3178 - accuracy: 1.0000 - val_loss: 0.3306 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.32765\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3351 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.32765\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.3292 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.32765\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3187 - accuracy: 1.0000 - val_loss: 0.3328 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.32765\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3328 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.32765\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3183 - accuracy: 1.0000 - val_loss: 0.3276 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.32765 to 0.32755, saving model to ./mod5.h5\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3172 - accuracy: 1.0000 - val_loss: 0.3282 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.32755\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3186 - accuracy: 1.0000 - val_loss: 0.3284 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.32755\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3178 - accuracy: 1.0000 - val_loss: 0.3275 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.32755 to 0.32752, saving model to ./mod5.h5\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3178 - accuracy: 1.0000 - val_loss: 0.3291 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.32752\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3183 - accuracy: 1.0000 - val_loss: 0.3256 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.32752 to 0.32563, saving model to ./mod5.h5\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3187 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.32563\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3187 - accuracy: 1.0000 - val_loss: 0.3255 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss improved from 0.32563 to 0.32555, saving model to ./mod5.h5\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3200 - accuracy: 1.0000 - val_loss: 0.3492 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.32555\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3205 - accuracy: 1.0000 - val_loss: 0.3290 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.32555\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3203 - accuracy: 1.0000 - val_loss: 0.3322 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.32555\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3198 - accuracy: 1.0000 - val_loss: 0.3372 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.32555\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 0.3261 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.32555\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3184 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.32555\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3263 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.32555\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3168 - accuracy: 1.0000 - val_loss: 0.3335 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.32555\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3174 - accuracy: 1.0000 - val_loss: 0.3219 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.32555 to 0.32193, saving model to ./mod5.h5\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 0.3284 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.32193\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3187 - accuracy: 1.0000 - val_loss: 0.3239 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.32193\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3194 - accuracy: 1.0000 - val_loss: 0.3270 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.32193\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 0.3259 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.32193\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3196 - accuracy: 1.0000 - val_loss: 0.3385 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.32193\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3236 - accuracy: 1.0000 - val_loss: 0.3297 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.32193\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3207 - accuracy: 1.0000 - val_loss: 0.3346 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.32193\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3201 - accuracy: 1.0000 - val_loss: 0.3320 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.32193\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3200 - accuracy: 1.0000 - val_loss: 0.3268 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.32193\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3274 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.32193\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.3259 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.32193\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 14ms/step - loss: 0.3176 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.9995 - accuracy: 0.8000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3219 - accuracy: 1.0000\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "(None, 19, 3, 3, 1)\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 19, 19, 7, 1 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_325 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_327 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_329 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_331 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_333 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_335 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_337 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_339 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_341 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_343 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_345 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_347 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_349 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_351 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_353 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_355 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_357 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_359 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_361 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_363 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_365 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_367 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_369 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_371 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_373 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_375 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_377 (Lambda)             (None, 19, 3, 7, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_324 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_325[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_326 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_327[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_328 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_329[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_330 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_331[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_332 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_333[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_334 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_335[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_336 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_337[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_338 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_339[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_340 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_341[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_342 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_343[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_344 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_345[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_346 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_347[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_348 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_349[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_350 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_351[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_352 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_353[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_354 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_355[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_356 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_357[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_358 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_359[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_360 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_361[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_362 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_363[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_364 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_365[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_366 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_368 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_369[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_370 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_372 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_374 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_376 (Lambda)             (None, 19, 3, 3, 1)  0           lambda_377[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_162 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_324[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_163 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_326[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_164 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_328[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_165 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_330[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_166 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_332[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_167 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_334[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_168 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_336[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_169 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_338[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_170 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_340[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_171 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_342[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_172 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_344[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_173 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_346[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_174 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_348[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_175 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_350[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_176 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_352[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_177 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_354[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_178 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_356[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_179 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_358[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_180 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_360[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_181 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_362[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_182 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_364[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_183 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_366[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_184 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_185 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_186 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_187 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_188 (Conv3D)             (None, 17, 1, 1, 8)  224         lambda_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_162 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_163 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_164 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_165 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_166 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_167 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_168 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_169 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_170 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_171 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_172 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_173 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_174 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_175 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_176 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_177 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_178 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_179 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_180 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_181 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_182 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_183 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_184 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_185 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_186 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_187 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_188 (Dropout)           (None, 17, 1, 1, 8)  0           conv3d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_162 (G (None, 8)            0           dropout_162[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_163 (G (None, 8)            0           dropout_163[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_164 (G (None, 8)            0           dropout_164[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_165 (G (None, 8)            0           dropout_165[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_166 (G (None, 8)            0           dropout_166[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_167 (G (None, 8)            0           dropout_167[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_168 (G (None, 8)            0           dropout_168[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_169 (G (None, 8)            0           dropout_169[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_170 (G (None, 8)            0           dropout_170[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_171 (G (None, 8)            0           dropout_171[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_172 (G (None, 8)            0           dropout_172[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_173 (G (None, 8)            0           dropout_173[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_174 (G (None, 8)            0           dropout_174[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_175 (G (None, 8)            0           dropout_175[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_176 (G (None, 8)            0           dropout_176[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_177 (G (None, 8)            0           dropout_177[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_178 (G (None, 8)            0           dropout_178[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_179 (G (None, 8)            0           dropout_179[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_180 (G (None, 8)            0           dropout_180[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_181 (G (None, 8)            0           dropout_181[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_182 (G (None, 8)            0           dropout_182[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_183 (G (None, 8)            0           dropout_183[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_184 (G (None, 8)            0           dropout_184[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_185 (G (None, 8)            0           dropout_185[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_186 (G (None, 8)            0           dropout_186[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_187 (G (None, 8)            0           dropout_187[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_188 (G (None, 8)            0           dropout_188[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 216)          0           global_average_pooling3d_162[0][0\n",
            "                                                                 global_average_pooling3d_163[0][0\n",
            "                                                                 global_average_pooling3d_164[0][0\n",
            "                                                                 global_average_pooling3d_165[0][0\n",
            "                                                                 global_average_pooling3d_166[0][0\n",
            "                                                                 global_average_pooling3d_167[0][0\n",
            "                                                                 global_average_pooling3d_168[0][0\n",
            "                                                                 global_average_pooling3d_169[0][0\n",
            "                                                                 global_average_pooling3d_170[0][0\n",
            "                                                                 global_average_pooling3d_171[0][0\n",
            "                                                                 global_average_pooling3d_172[0][0\n",
            "                                                                 global_average_pooling3d_173[0][0\n",
            "                                                                 global_average_pooling3d_174[0][0\n",
            "                                                                 global_average_pooling3d_175[0][0\n",
            "                                                                 global_average_pooling3d_176[0][0\n",
            "                                                                 global_average_pooling3d_177[0][0\n",
            "                                                                 global_average_pooling3d_178[0][0\n",
            "                                                                 global_average_pooling3d_179[0][0\n",
            "                                                                 global_average_pooling3d_180[0][0\n",
            "                                                                 global_average_pooling3d_181[0][0\n",
            "                                                                 global_average_pooling3d_182[0][0\n",
            "                                                                 global_average_pooling3d_183[0][0\n",
            "                                                                 global_average_pooling3d_184[0][0\n",
            "                                                                 global_average_pooling3d_185[0][0\n",
            "                                                                 global_average_pooling3d_186[0][0\n",
            "                                                                 global_average_pooling3d_187[0][0\n",
            "                                                                 global_average_pooling3d_188[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 512)          111104      concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 512)          262656      dense_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 512)          262656      dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 1)            513         dense_26[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 642,977\n",
            "Trainable params: 642,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 5s 565ms/step - loss: 99.1221 - accuracy: 0.4578 - val_loss: 93.2622 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.26224, saving model to ./mod6.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 91.4762 - accuracy: 0.6988 - val_loss: 85.8967 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.26224 to 85.89673, saving model to ./mod6.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 84.2070 - accuracy: 0.8795 - val_loss: 79.0018 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00003: val_loss improved from 85.89673 to 79.00184, saving model to ./mod6.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 77.3027 - accuracy: 0.7711 - val_loss: 72.3357 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.00184 to 72.33573, saving model to ./mod6.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 70.6652 - accuracy: 0.8795 - val_loss: 65.9808 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.33573 to 65.98076, saving model to ./mod6.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 64.4240 - accuracy: 0.8434 - val_loss: 60.0433 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00006: val_loss improved from 65.98076 to 60.04327, saving model to ./mod6.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 58.4304 - accuracy: 0.9157 - val_loss: 54.3606 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.04327 to 54.36062, saving model to ./mod6.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 52.7625 - accuracy: 0.9157 - val_loss: 48.9253 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.36062 to 48.92529, saving model to ./mod6.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 47.3897 - accuracy: 0.9398 - val_loss: 43.9154 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.92529 to 43.91539, saving model to ./mod6.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 42.3398 - accuracy: 0.9518 - val_loss: 38.9877 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.91539 to 38.98773, saving model to ./mod6.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 37.5773 - accuracy: 0.9398 - val_loss: 34.4324 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.98773 to 34.43240, saving model to ./mod6.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 33.0891 - accuracy: 0.9518 - val_loss: 30.2658 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.43240 to 30.26580, saving model to ./mod6.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 28.9192 - accuracy: 0.9639 - val_loss: 26.1873 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.26580 to 26.18728, saving model to ./mod6.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 25.0560 - accuracy: 0.9518 - val_loss: 22.5825 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.18728 to 22.58254, saving model to ./mod6.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 21.5382 - accuracy: 0.9277 - val_loss: 19.1726 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.58254 to 19.17261, saving model to ./mod6.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 18.2106 - accuracy: 0.9880 - val_loss: 16.1189 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.17261 to 16.11888, saving model to ./mod6.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 15.2231 - accuracy: 1.0000 - val_loss: 13.4493 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.11888 to 13.44929, saving model to ./mod6.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 12.5686 - accuracy: 1.0000 - val_loss: 10.9351 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.44929 to 10.93510, saving model to ./mod6.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 10.2186 - accuracy: 0.9759 - val_loss: 8.8032 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00019: val_loss improved from 10.93510 to 8.80321, saving model to ./mod6.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 8.1391 - accuracy: 1.0000 - val_loss: 6.9609 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.80321 to 6.96094, saving model to ./mod6.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 6.3686 - accuracy: 1.0000 - val_loss: 5.4086 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00021: val_loss improved from 6.96094 to 5.40865, saving model to ./mod6.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 4.8774 - accuracy: 1.0000 - val_loss: 4.1004 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.40865 to 4.10041, saving model to ./mod6.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 3.7040 - accuracy: 1.0000 - val_loss: 3.1631 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.10041 to 3.16309, saving model to ./mod6.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 2.8192 - accuracy: 1.0000 - val_loss: 2.5197 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.16309 to 2.51974, saving model to ./mod6.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 2.2307 - accuracy: 1.0000 - val_loss: 2.1648 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.51974 to 2.16480, saving model to ./mod6.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 1.9523 - accuracy: 0.9880 - val_loss: 2.0968 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.16480 to 2.09677, saving model to ./mod6.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 1.8445 - accuracy: 1.0000 - val_loss: 1.9041 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.09677 to 1.90412, saving model to ./mod6.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 1.6182 - accuracy: 1.0000 - val_loss: 1.8214 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.90412 to 1.82136, saving model to ./mod6.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.3896 - accuracy: 0.9759 - val_loss: 1.3461 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.82136 to 1.34608, saving model to ./mod6.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.1109 - accuracy: 1.0000 - val_loss: 1.1363 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.34608 to 1.13630, saving model to ./mod6.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.0825 - accuracy: 0.9398 - val_loss: 1.4501 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.13630\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 1.3315 - accuracy: 0.8554 - val_loss: 1.8701 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.13630\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 1.2546 - accuracy: 0.9759 - val_loss: 1.4634 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.13630\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 1.2095 - accuracy: 0.9518 - val_loss: 1.3496 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.13630\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.0298 - accuracy: 0.9880 - val_loss: 1.1346 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.13630 to 1.13455, saving model to ./mod6.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.8475 - accuracy: 1.0000 - val_loss: 1.0105 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.13455 to 1.01045, saving model to ./mod6.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.7358 - accuracy: 1.0000 - val_loss: 0.8859 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.01045 to 0.88588, saving model to ./mod6.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.6627 - accuracy: 1.0000 - val_loss: 0.8100 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.88588 to 0.80998, saving model to ./mod6.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.6126 - accuracy: 1.0000 - val_loss: 0.7708 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.80998 to 0.77077, saving model to ./mod6.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.5709 - accuracy: 1.0000 - val_loss: 0.7373 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.77077 to 0.73725, saving model to ./mod6.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.5313 - accuracy: 1.0000 - val_loss: 0.7013 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.73725 to 0.70129, saving model to ./mod6.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.5018 - accuracy: 1.0000 - val_loss: 0.6704 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.70129 to 0.67040, saving model to ./mod6.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4678 - accuracy: 1.0000 - val_loss: 0.6372 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.67040 to 0.63725, saving model to ./mod6.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4653 - accuracy: 1.0000 - val_loss: 0.6629 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.63725\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4431 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.63725 to 0.61430, saving model to ./mod6.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.4303 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.61430\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4203 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.61430 to 0.59768, saving model to ./mod6.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4085 - accuracy: 1.0000 - val_loss: 0.5848 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.59768 to 0.58476, saving model to ./mod6.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3982 - accuracy: 1.0000 - val_loss: 0.5674 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.58476 to 0.56744, saving model to ./mod6.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3965 - accuracy: 1.0000 - val_loss: 0.5658 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.56744 to 0.56581, saving model to ./mod6.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3942 - accuracy: 1.0000 - val_loss: 0.5730 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.56581\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3847 - accuracy: 1.0000 - val_loss: 0.5586 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.56581 to 0.55863, saving model to ./mod6.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3782 - accuracy: 1.0000 - val_loss: 0.5540 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.55863 to 0.55397, saving model to ./mod6.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3755 - accuracy: 1.0000 - val_loss: 0.5551 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.55397\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3740 - accuracy: 1.0000 - val_loss: 0.5567 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.55397\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3698 - accuracy: 1.0000 - val_loss: 0.5446 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.55397 to 0.54460, saving model to ./mod6.h5\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3716 - accuracy: 1.0000 - val_loss: 0.5579 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.54460\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3718 - accuracy: 1.0000 - val_loss: 0.5466 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.54460\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3710 - accuracy: 1.0000 - val_loss: 0.6914 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.54460\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.4058 - accuracy: 0.9880 - val_loss: 0.6099 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.54460\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4823 - accuracy: 0.9759 - val_loss: 0.9206 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.54460\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.5228 - accuracy: 1.0000 - val_loss: 1.1557 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.54460\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.5298 - accuracy: 1.0000 - val_loss: 0.8867 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.54460\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.5003 - accuracy: 1.0000 - val_loss: 0.6748 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.54460\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4602 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.54460\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4392 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.54460\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4147 - accuracy: 1.0000 - val_loss: 0.5596 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.54460\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3971 - accuracy: 1.0000 - val_loss: 0.5430 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.54460 to 0.54299, saving model to ./mod6.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3817 - accuracy: 1.0000 - val_loss: 0.5243 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.54299 to 0.52426, saving model to ./mod6.h5\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3729 - accuracy: 1.0000 - val_loss: 0.5137 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.52426 to 0.51373, saving model to ./mod6.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3637 - accuracy: 1.0000 - val_loss: 0.5134 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.51373 to 0.51339, saving model to ./mod6.h5\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3587 - accuracy: 1.0000 - val_loss: 0.4993 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.51339 to 0.49928, saving model to ./mod6.h5\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3524 - accuracy: 1.0000 - val_loss: 0.4908 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.49928 to 0.49081, saving model to ./mod6.h5\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3509 - accuracy: 1.0000 - val_loss: 0.5042 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.49081\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3477 - accuracy: 1.0000 - val_loss: 0.4849 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.49081 to 0.48487, saving model to ./mod6.h5\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3512 - accuracy: 1.0000 - val_loss: 0.4978 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.48487\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3455 - accuracy: 1.0000 - val_loss: 0.4860 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.48487\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3423 - accuracy: 1.0000 - val_loss: 0.4976 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.48487\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3416 - accuracy: 1.0000 - val_loss: 0.4766 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.48487 to 0.47655, saving model to ./mod6.h5\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3418 - accuracy: 1.0000 - val_loss: 0.5030 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.47655\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3428 - accuracy: 1.0000 - val_loss: 0.4989 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.47655\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3420 - accuracy: 1.0000 - val_loss: 0.4778 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.47655\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3412 - accuracy: 1.0000 - val_loss: 0.4758 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.47655 to 0.47577, saving model to ./mod6.h5\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3407 - accuracy: 1.0000 - val_loss: 0.4778 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.47577\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3396 - accuracy: 1.0000 - val_loss: 0.4956 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.47577\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3371 - accuracy: 1.0000 - val_loss: 0.4656 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.47577 to 0.46560, saving model to ./mod6.h5\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3359 - accuracy: 1.0000 - val_loss: 0.4773 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.46560\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3364 - accuracy: 1.0000 - val_loss: 0.4639 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.46560 to 0.46386, saving model to ./mod6.h5\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3383 - accuracy: 1.0000 - val_loss: 0.4649 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.46386\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3347 - accuracy: 1.0000 - val_loss: 0.4752 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.46386\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3323 - accuracy: 1.0000 - val_loss: 0.4603 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.46386 to 0.46035, saving model to ./mod6.h5\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3317 - accuracy: 1.0000 - val_loss: 0.4753 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.46035\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3300 - accuracy: 1.0000 - val_loss: 0.4614 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.46035\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3317 - accuracy: 1.0000 - val_loss: 0.4889 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.46035\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3318 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.46035\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3292 - accuracy: 1.0000 - val_loss: 0.4680 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.46035\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3286 - accuracy: 1.0000 - val_loss: 0.4736 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.46035\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3280 - accuracy: 1.0000 - val_loss: 0.4727 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.46035\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3282 - accuracy: 1.0000 - val_loss: 0.4750 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.46035\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3294 - accuracy: 1.0000 - val_loss: 0.4688 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.46035\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3269 - accuracy: 1.0000 - val_loss: 0.4645 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.46035\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3277 - accuracy: 1.0000 - val_loss: 0.4808 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.46035\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3263 - accuracy: 1.0000 - val_loss: 0.4674 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.46035\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3269 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.46035\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.4783 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.46035\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.4503 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.46035 to 0.45033, saving model to ./mod6.h5\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.4564 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.45033\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3216 - accuracy: 1.0000 - val_loss: 0.4542 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.45033\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.4820 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.45033\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3252 - accuracy: 1.0000 - val_loss: 0.4653 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.45033\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.4489 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.45033 to 0.44891, saving model to ./mod6.h5\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3245 - accuracy: 1.0000 - val_loss: 0.4646 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.44891\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3242 - accuracy: 1.0000 - val_loss: 0.4481 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.44891 to 0.44810, saving model to ./mod6.h5\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3239 - accuracy: 1.0000 - val_loss: 0.4442 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.44810 to 0.44417, saving model to ./mod6.h5\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3225 - accuracy: 1.0000 - val_loss: 0.4485 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.44417\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3217 - accuracy: 1.0000 - val_loss: 0.4425 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.44417 to 0.44254, saving model to ./mod6.h5\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 0.4751 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.44254\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3231 - accuracy: 1.0000 - val_loss: 0.4578 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.44254\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 0.4415 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.44254 to 0.44145, saving model to ./mod6.h5\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3221 - accuracy: 1.0000 - val_loss: 0.4599 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.44145\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3218 - accuracy: 1.0000 - val_loss: 0.4552 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.44145\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3214 - accuracy: 1.0000 - val_loss: 0.4526 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.44145\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3222 - accuracy: 1.0000 - val_loss: 0.4374 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.44145 to 0.43743, saving model to ./mod6.h5\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3212 - accuracy: 1.0000 - val_loss: 0.4480 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.43743\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3206 - accuracy: 1.0000 - val_loss: 0.4350 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.43743 to 0.43496, saving model to ./mod6.h5\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3205 - accuracy: 1.0000 - val_loss: 0.4403 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.43496\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3198 - accuracy: 1.0000 - val_loss: 0.4409 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.43496\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3196 - accuracy: 1.0000 - val_loss: 0.4429 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.43496\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3195 - accuracy: 1.0000 - val_loss: 0.4429 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.43496\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3194 - accuracy: 1.0000 - val_loss: 0.4415 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.43496\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3196 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.43496\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3208 - accuracy: 1.0000 - val_loss: 0.4463 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.43496\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3183 - accuracy: 1.0000 - val_loss: 0.4499 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.43496\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 0.4351 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.43496\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3194 - accuracy: 1.0000 - val_loss: 0.4338 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.43496 to 0.43384, saving model to ./mod6.h5\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3175 - accuracy: 1.0000 - val_loss: 0.4267 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.43384 to 0.42671, saving model to ./mod6.h5\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3192 - accuracy: 1.0000 - val_loss: 0.4608 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.42671\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3213 - accuracy: 1.0000 - val_loss: 0.4499 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.42671\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3199 - accuracy: 1.0000 - val_loss: 0.4361 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.42671\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3191 - accuracy: 1.0000 - val_loss: 0.4444 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.42671\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3185 - accuracy: 1.0000 - val_loss: 0.4377 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.42671\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3186 - accuracy: 1.0000 - val_loss: 0.4462 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.42671\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3172 - accuracy: 1.0000 - val_loss: 0.4333 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.42671\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3171 - accuracy: 1.0000 - val_loss: 0.4339 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.42671\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3164 - accuracy: 1.0000 - val_loss: 0.4282 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.42671\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.4414 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.42671\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3165 - accuracy: 1.0000 - val_loss: 0.4297 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.42671\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3151 - accuracy: 1.0000 - val_loss: 0.4297 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.42671\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3156 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.42671\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.4393 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.42671\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.4307 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.42671\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.4467 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.42671\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.4187 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.42671 to 0.41871, saving model to ./mod6.h5\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3168 - accuracy: 1.0000 - val_loss: 0.4460 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.41871\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3166 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.41871\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3144 - accuracy: 1.0000 - val_loss: 0.4233 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.41871\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3148 - accuracy: 1.0000 - val_loss: 0.4235 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.41871\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3163 - accuracy: 1.0000 - val_loss: 0.4464 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.41871\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3156 - accuracy: 1.0000 - val_loss: 0.4237 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.41871\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3144 - accuracy: 1.0000 - val_loss: 0.4409 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.41871\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.4283 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.41871\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3143 - accuracy: 1.0000 - val_loss: 0.4364 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.41871\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3137 - accuracy: 1.0000 - val_loss: 0.4221 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.41871\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3125 - accuracy: 1.0000 - val_loss: 0.4245 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.41871\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3138 - accuracy: 1.0000 - val_loss: 0.4192 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.41871\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3133 - accuracy: 1.0000 - val_loss: 0.4195 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.41871\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3144 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.41871 to 0.41500, saving model to ./mod6.h5\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3139 - accuracy: 1.0000 - val_loss: 0.4201 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.41500\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3139 - accuracy: 1.0000 - val_loss: 0.4214 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.41500\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3145 - accuracy: 1.0000 - val_loss: 0.4245 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.41500\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3148 - accuracy: 1.0000 - val_loss: 0.4250 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.41500\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3139 - accuracy: 1.0000 - val_loss: 0.4201 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.41500\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3130 - accuracy: 1.0000 - val_loss: 0.4238 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.41500\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3131 - accuracy: 1.0000 - val_loss: 0.4310 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.41500\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3140 - accuracy: 1.0000 - val_loss: 0.4105 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.41500 to 0.41052, saving model to ./mod6.h5\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3149 - accuracy: 1.0000 - val_loss: 0.4725 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.41052\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3176 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.41052\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3175 - accuracy: 1.0000 - val_loss: 0.4046 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.41052 to 0.40459, saving model to ./mod6.h5\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3163 - accuracy: 1.0000 - val_loss: 0.4167 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.40459\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.4028 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00180: val_loss improved from 0.40459 to 0.40282, saving model to ./mod6.h5\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3143 - accuracy: 1.0000 - val_loss: 0.4154 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.40282\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3125 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00182: val_loss improved from 0.40282 to 0.39837, saving model to ./mod6.h5\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3120 - accuracy: 1.0000 - val_loss: 0.3998 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.39837\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3107 - accuracy: 1.0000 - val_loss: 0.4040 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.39837\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3109 - accuracy: 1.0000 - val_loss: 0.4070 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.39837\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3108 - accuracy: 1.0000 - val_loss: 0.4081 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.39837\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3107 - accuracy: 1.0000 - val_loss: 0.4151 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.39837\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3115 - accuracy: 1.0000 - val_loss: 0.4069 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.39837\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3128 - accuracy: 1.0000 - val_loss: 0.4070 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.39837\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3129 - accuracy: 1.0000 - val_loss: 0.4096 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.39837\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3127 - accuracy: 1.0000 - val_loss: 0.4057 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.39837\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3122 - accuracy: 1.0000 - val_loss: 0.4111 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.39837\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3123 - accuracy: 1.0000 - val_loss: 0.4289 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.39837\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3119 - accuracy: 1.0000 - val_loss: 0.4075 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.39837\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3129 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.39837\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3121 - accuracy: 1.0000 - val_loss: 0.4134 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.39837\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3131 - accuracy: 1.0000 - val_loss: 0.4034 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.39837\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3126 - accuracy: 1.0000 - val_loss: 0.3982 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss improved from 0.39837 to 0.39822, saving model to ./mod6.h5\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3115 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.39822\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3105 - accuracy: 1.0000 - val_loss: 0.4029 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.39822\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 16ms/step - loss: 0.3114 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5411 - accuracy: 0.9200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3982 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVEPB4XHLzU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KeWBKywEp6Ub",
        "outputId": "1862dc63-d68b-479d-d7a0-cb3994225a23"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(7):\n",
        "  y_test_pred=load_model(f'./mod{i}.h5')\n",
        "  y_test_pred=y_test_pred.predict(x_test)\n",
        "\n",
        "  y_pred=y_test_pred.flatten()\n",
        "\n",
        "  y_test_pred=np.where(y_pred<0.5, 0,1)\n",
        "\n",
        "  c_matrix=confusion_matrix(y_test,y_test_pred)\n",
        "  ax=sns.heatmap(c_matrix,annot=True, xticklabels=['No ADHD','ADHD'],yticklabels=['No ADHD','ADHD'],cbar=False,cmap='Blues')\n",
        "  ax.set_xlabel('Prediction')\n",
        "  ax.set_ylabel('Actual')\n",
        "  plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fabf41440e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEGCAYAAABmXi5tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS1ElEQVR4nO3debRdZXnH8e8jUBEkEIZcsAQigwwCZbZWQIaWIkQJg0qAKg6EQUFArXVRFEUrFtEKWBlFoCKoDEsU0VWITGolEAQKNIJBVKYoIQwy5+kfZ99wc73DSXLee27u+/2sddY9e3yfJPv8su979n53ZCaSpLHvVd0uQJI0Mgx8SaqEgS9JlTDwJakSBr4kVWLZbhcwmJWnXuTlQxqVZp11QLdLkAbVM265GGyZZ/iSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEga+JFXCwJekShj4klQJA1+SKmHgS1IlDHxJqoSBL0mVMPAlqRIGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlTDwJakSBr4kVcLAl6RKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEga+JFXCwJekShj4klSJIoEfEbtExOUR8b/N63sRsXOJtiRJ7el44EfEXsA3gKuAA4GDgKuBb0TEnp1uT5LUnmUL7PPjwJTM/FWfebdHxAzgdFrhL0kaYSW6dNbsF/YAZOYdQE+B9iRJbSgR+M8s5jJJUkElunTWj4jvDzA/gPUKtCdJakOJwN97iGVfKtCeJKkNHQ/8zLy+0/uUJC25jgd+RNwJ5GDLM3OLTrcpSRpeiS6dyc3PAH4IeO29JI0CJbp0ftv7PiKe7zstSeoex9KRpEqU6MPfus/kayJiK1rdOwBk5m2dblOSNLwSffin9nn/CPDlPtMJ7FqgTUnSMEr04e/S6X1KkpZciTN8ImI1WiNlbtzMuge4ODMfL9GeJGl4JYZH3gS4C9gGmAX8GtgOuCsiNh5qW0lSOSXO8E8CPpKZ3+k7MyL2Az4P7FegTfVxxmFvZo+t1mbOk8/x5n++CoApb1qHf9n/b9jodSuz6wlXM/M3/rKl7jr5s//Kz266gfHjV+WCS6/sdjlVKHFZ5ub9wx4gMy8DNivQnvq5+Pr72e/kaxead/fvnuDgL1/Pzfc+2qWqpIXtMXkKp5x2ZrfLqEqJM3yHR+6yn937GOusvuJC82Y99GSXqpEGtuXW2/LwQ3/odhlVKRH4EyLiuAHmB7BGgfYkSW0o0aVzDrDSAK/XAucOtWFETIuIGREx44X7phcoTZLqVeI6/M8Mtiwithtm27OBswFWnnrRoCNuSpIWXZHr8PuKiE2Bqc3rCWDb0m1Kkv5SZHb+RDoiJvFKyL8IrAtsm5kPtLsPz/AX33lH7cAOm/Sw2krL89i8Z/nC9+5g7tPP8++HbMfq45Zn3p9f4M4H5rJvvyt51J5ZZx3Q7RLGhM8c/3Fm3noL8554glVXW433TTuSyXt71faS6hm3XAy2rOOBHxE/B8YBlwCXZOavI2J2Zr5+UfZj4Gu0MvA1mg0V+CW+tH2U1pe0PbxyVY7hLUld1vHAz8wpwObArcCJETEbGB8R23e6LUlS+4p8aZuZ84DzgfMjYgLwLuArEbFOZk4s0aYkaWjFn3iVmY9l5hmZ+RZgh9LtSZIGNqKPOPT5tpLUPT7TVpIqYeBLUiWKBX5ErB0RV0TEnIh4LCIui4i1S7UnSRpayTP884HvA2sBrwOuauZJkrqgZOCvkZnnZ+ZLzeubODyyJHVNycD/U0QcHBHLNK+DgT8VbE+SNISSgf9+WjdcPQI8DOwPvK9ge5KkIRQbHrm55v4dpfYvSVo0HQ/8iPjUEIszM0/qdJuSpOGN1EPMVwQ+AKwGGPiS1AUlHnF4au/7iFgJ+AitvvtLgFMH206SVFaRPvyIWBU4DjgIuADYOjPnlmhLktSeEn34pwD70noY+eaZ+XSn25AkLboSl2V+lNadtf8KPBQRTzavpyLiyQLtSZLaUKIP3wHZJGkUMpwlqRIGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlTDwJakSBr4kVcLAl6RKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEoM+8SoiTgdysOWZeXSRiiRJRQz1iMMZI1aFJKm4QQM/My8YyUIkSWUN+xDziFgD+ASwKbB87/zM3LVgXZKkDmvnS9tvAfcArwc+AzwA3FKwJklSAe0E/mqZeR7wYmZen5nvBzy7l6SlzLBdOsCLzc+HI2Iv4CFg1XIlSZJKaCfwPxcRKwMfBU4HxgHHFq1KktRxwwZ+Zv6geTsP2KVsOZKkUtq5Sud8BrgBq+nLlyQtJdrp0vlBn/fLA/vQ6seXJC1F2unSuazvdER8G7ipWEWSpCIic9DhcgbeIGIj4IeZuUGZklqee2nwcXykbhq/3Ye7XYI0qGdnnhGDLWunD/8pFu7Df4TWnbeSpKVIO106K41EIZKksoa90zYirm1nniRpdBtqPPzlgRWA1SNiPNDbLzQO+OsRqE2S1EFDdekcBhwDvA64lVcC/0ngjMJ1SZI6bKjx8L8KfDUijsrM00ewJklSAe2Mljk/IlbpnYiI8RFxZMGaJEkFtBP4h2bmE70TmTkXOLRcSZKkEtoJ/GUiYsGF/BGxDPBX5UqSJJXQzlg61wCXRsRZzfRhwI/KlSRJKqGdwP8EMA04vJm+A1izWEWSpCKG7dLJzPnA/9B6lu32tB5veE/ZsiRJnTbUjVdvAKY2rz8ClwJkpg9BkaSl0FBdOvcCNwKTM/M+gIjw0YaStJQaqktnX+BhYHpEnBMRu/HK3baSpKXMoIGfmVdm5gHAxsB0WsMsTIiIr0fE7iNVoCSpM9r50vaZzLw4M98OrA3MxPHwJWmp086NVwtk5tzMPDszdytVkCSpjEUKfEnS0svAl6RKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEga+JFXCwJekShj4klQJA1+SKmHgS1IlDHxJqoSBL0mVMPAlqRIGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlVi21I4jYhVgw2ZyVmbOK9WWJGl4HQ/8iHg1cBYwBZgNBLBuRFwBHJ6ZL3S6TUnS8Ep06RwPLAdMzMytMnNLYB1a/7mcUKA9SVIbSgT+vsChmflU74zm/ZHAPgXakyS1oUTgz8/MP/efmZlPA1mgPUlSG0p8aZsRMZ5W331/8wu0J0lqQ4nAXxm4lYED3zN8SeqSjgd+Zk7q9D4lSUuuxGWZWw+1PDNv63SbGtzNN97AF0/+PPNfns8++72TDxw6rdslqWJnfvog3rbTZsx5/Cm2fee/AfCpI/di8lu3YH4mcx5/immf/i8enuNtOyVEZmd7WSJiep/JbWh17/TKzNy1nf0895LdP0vq5Zdf5h17/SNnnXM+PT09HPju/Tn5lC+z/gYbdLu0pdr47T7c7RKWWm/Zen2e+fPznHvSexYE/korLs9TzzwHwJFT38rG663F0Z+/pJtlLtWenXnGQN3pQJkunV1630fEzL7TGll33XkHEyeuy9oTJwKwx5578dPp1xr46pqbb7ufddZadaF5vWEPsMJrXk2nT0L1imJDKzT8l+uixx59lDXXWnPB9ISeHu68444uViQN7MQPvZ2DJm/PvKefZY9pp3W7nDFrVA2eFhHTImJGRMw475yzu12OpBFy4teuYsO3ncAlP5rB4e/eqdvljFklvrQ9nVfO7NeOiIX+u87MowfbNjPPBs4G+/A7YUJPD488/MiC6ccefZSenp4uViQN7dKrb+GK04/gc2de3e1SxqQSXToz+ry/ddC1VNwbN9ucBx98gN///nf0TOjhmqt/yBdOObXbZUkLWX+dNbj/wTkATN55C2Y98GiXKxq7Snxpe0Gn96nFs+yyy/LJ4z/FEdM+yPz5LzNln/3YYIMNh99QKuSCLxzCjttsyOqrvJb7rjmJk868mj12eCMbrjuB+fOTBx9+3Ct0Cur4ZZkAEfFe4CPARs2se4DTMvPCdvdhl45GKy/L1Gg2opdlNmF/DHAccButIRa2Bk6JiMzMizrdpiRpeCWu0jkC2Cczp2fmvMx8IjOvA/YDPlSgPUlSG0oE/rjMfKD/zGbeuALtSZLaUCLwn13MZZKkgkpclrlJRAx0O2cA6xVoT5LUhiKBP8C8ACYCnyzQniSpDSWuw/9t7/uI2Ao4EHgnMBu4rNPtSZLaU+KyzDcAU5vXH4FLaV3v76iZktRFJbp07gVuBCZn5n0AEXFsgXYkSYugxFU6+wIPA9Mj4pyI2I2Bn28rSRpBHQ/8zLwyMw8ANgam07rrdkJEfD0idu90e5Kk9hQbDz8zn8nMizPz7cDawEzgE6XakyQNbUQegJKZczPz7MzcbSTakyT9pVH1xCtJUjkGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlTDwJakSBr4kVcLAl6RKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEga+JFXCwJekShj4klQJA1+SKmHgS1IlDHxJqoSBL0mVMPAlqRIGviRVwsCXpEoY+JJUCQNfkiph4EtSJQx8SaqEgS9JlTDwJakSBr4kVcLAl6RKGPiSVAkDX5IqEZnZ7Ro0AiJiWmae3e06pP48NkeOZ/j1mNbtAqRBeGyOEANfkiph4EtSJQz8ethHqtHKY3OE+KWtJFXCM3xJqoSBL0mVMPBHmYjIiDi1z/THIuLExdjPlRHxi37zToyIP0TE7RHx64i4PCI27bP8pxGxbZ/pSRFxV/N+54iYFxEzI+L/IuKGiJi8WH9IjVkRMaU5hjdupidFxLPNcXNPRPwyIg7ps/4hEXFGv30sOA4j4oGIuLN53R0Rn4uI5Uf0DzWGGPijz/PAvhGx+uLuICJWAbYBVo6I9fot/kpmbpmZGwKXAtdFxBpt7vrGzNwqMzcCjgbOiIjdFrdOjUlTgZuan73ub46bTYADgGMi4n2LsM9dMnNzYHtgPeCsjlVbGQN/9HmJ1lULx/Zf0JwtXRcRd0TEtRGxziD72Be4CriE1gdsQJl5KfAT4MBFLTIzbwc+C3x4UbfV2BQRrwV2AD7AIMddZv4GOI7WCcMiycyngcOBKRGx6hKUWi0Df3T6GnBQRKzcb/7pwAWZuQXwLeC0QbafCny7eU0dZJ1etwEb95n+VtPlcztw9SJuq7rtDVyTmbOAP0XENoOs1/+4eXfvMdccd9sOsh2Z+SQwG9iwU0XXxMAfhZqD+kL+8izozcDFzfuLaJ1NLSQiemh9GG5qPngvRsRmQzQX/aYParp8tgT2HKbU/tuqblNp/VZJ83Owk43+x82lvcdcc9zNGKYdj7vFtGy3C9Cg/oPWmdD5i7jdu4DxwOyIABhH64N3/CDrb8XwH7DBbAXcs5jbagxpulh2BTaPiASWAZLWb6v9LfZxExErAZOAWYtXad08wx+lMvNx4Du0+kN7/YxX+kYPAm4cYNOpwB6ZOSkzJ9H68nbA/tSI2A/YnVbXzyKJiC2AExj4A6367A9clJnrNsfeRFpdLxP7rhQRk4Av0eqeXCTNdwT/CVyZmXOXuOIKeYY/up3Kwl+KHgWcHxEfB+YAC13p0HyY1gUWXI6ZmbObyynf1Mw6NiIOBlYE7gJ2zcw5bdazY0TMBFYAHgOOzsxrF/lPpbFoKvDFfvMuAz4JrN8cN8sDTwGnZeY3F2Hf06P16+qrgCuAk5a83Do5tIIkVcIuHUmqhIEvSZUw8CWpEga+JFXCwJekShj4GpMi4uXmVv27IuK7EbHCEuzrmxGxf/P+3L4jjA6w7s4R8Xd9pg+PiPcsbttSJxn4GquebW7V3wx4gdagWwtExGLdg5KZH8zMu4dYZWdgQeBn5pmZeeHitCV1moGvGtwIbNCcfd8YEd8H7o6IZSLilIi4pRmB9DCAaDmjGff/v4EJvTvqN1b7HhFxW0T8qhm9dBKt/1iObX672LF5BsHHmvW3jIhfNG1dERHj++zzi81Y8bMiYscR/dtRNbzTVmNacyb/NuCaZtbWwGbNHcjTgHmZuV1EvBq4OSJ+Qmusl42ATYEe4G7gG/32uwZwDrBTs69VM/PxiDgTeDozv9Ss1/d5ARcCR2Xm9RHxWeDTwDHNsmUzc/uI2LOZ//ed/ruQDHyNVa9phtqF1hn+ebS6Wn6ZmbOb+bsDW/T2zwMr0xppdCfg25n5MvBQRFw3wP7/Frihd1/N2EeDaoa6XiUzr29mXQB8t88qlzc/b6U1OJjUcQa+xqpnm6F2F2hGD32m7yxaZ9w/7rfecMNCl/B88/Nl/FyqEPvwVbMfA0dExHIAEfGGiFgRuIHWQzmWiYi1gF0G2PYXwE4R8fpm294nMD0FrNR/5cycB8zt0z//T8D1/deTSvJMQjU7l1b3yW3NaIxzgCm0RmTclVbf/YPAz/tvmJlzmu8ALo+IV9EaPfQfaD1a8nsRsTet0U37ei9wZnOJ6G/oN9qpVJqjZUpSJezSkaRKGPiSVAkDX5IqYeBLUiUMfEmqhIEvSZUw8CWpEv8PqDioLD2D4b0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fabec283710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
