{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb의 사본의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnQTMxRnGhLO"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "control=[]\n",
        "adhd=[]\n",
        "for i in range(19):\n",
        "    b=[]\n",
        "    for j in os.listdir(f'./drive/MyDrive/ch_corr/ADHD/{i}'):\n",
        "        b.append(pd.read_csv(f'./drive/MyDrive/ch_corr/ADHD/{i}/'+j,index_col='Unnamed: 0').to_numpy())\n",
        "    adhd.append(b)\n",
        "    \n",
        "for i in range(19):\n",
        "    b=[]    \n",
        "    for j in os.listdir(f'./drive/MyDrive/ch_corr/Control/{i}'):\n",
        "        b.append(pd.read_csv(f'./drive/MyDrive/ch_corr/Control/{i}/'+j,index_col='Unnamed: 0').to_numpy())\n",
        "    control.append(b)\n",
        "\n",
        "control= np.array(control).transpose(1,0,2,3)\n",
        "adhd=np.array(adhd).transpose(1,0,2,3)\n",
        "\n",
        "x=np.append(adhd,control,axis=0)\n",
        "y=np.array([1]*61+[0]*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex8uYJ5FHvvZ"
      },
      "source": [
        "def crop(dimension, start, end):\n",
        "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
        "    # example : to crop tensor x[:, :, 5:10]\n",
        "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
        "    def func(x):\n",
        "        if dimension == 0:\n",
        "            return x[start: end]\n",
        "        if dimension == 1:\n",
        "            return x[:, start: end]\n",
        "        if dimension == 2:\n",
        "            return x[:, :, start: end]\n",
        "        if dimension == 3:\n",
        "            return x[:, :, :, start: end]\n",
        "        if dimension == 4:\n",
        "            return x[:, :, :, :, start: end]\n",
        "    return Lambda(func)\n",
        "import math\n",
        "def slice_model(model_input,unit,row_num,col_num,term):\n",
        "  remain=math.ceil(unit/2)\n",
        "  return [crop(3,col_num-(j+unit),col_num-j)(crop(2,row_num-(i+unit),row_num-i)(model_input)) for i in range(0,row_num-unit+1,term) for j in range(0,col_num-unit+1,term)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7qqXdEjg6OP"
      },
      "source": [
        "import sys\n",
        "import sys\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, GlobalMaxPooling3D,Lambda,concatenate,Conv3D, MaxPooling3D,GlobalAveragePooling3D\n",
        "from tensorflow.keras.regularizers import l1,l2\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mk_model(filepath=None):\n",
        "    \n",
        "    FILTER_SIZE=3\n",
        "    NUM_FILTERS=8\n",
        "    INPUT_SIZE=19\n",
        "    MAXPOOL_SIZE=2\n",
        "\n",
        "    densors=[]\n",
        "    model_input=Input(shape=(19,INPUT_SIZE,INPUT_SIZE,1))\n",
        "    print(model_input.shape)\n",
        "    for idx in slice_model(model_input,5,19,19,2):\n",
        "            model_output=Conv3D(NUM_FILTERS, (FILTER_SIZE,FILTER_SIZE,FILTER_SIZE),activation='relu')(idx)\n",
        "            model_output=Dropout(0.5)(model_output)                        \n",
        "            model_output=GlobalAveragePooling3D()(model_output)\n",
        "            densors.append(model_output)\n",
        "\n",
        "    model_output=concatenate(densors)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "    model_output=Dense(units=512,activation='relu')(model_output)\n",
        "    model_output=Dense(units=512,activation='relu',kernel_regularizer=l1(0.01))(model_output)\n",
        "\n",
        "\n",
        "    model_output=Dense(units=1,activation='sigmoid')(model_output)\n",
        "    model = Model(inputs = model_input, outputs = model_output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_Wxk0KWV7Bq",
        "outputId": "4eb53ba4-43e8-464f-ec19-1a6ae89096bc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "x_tra,x_test,y_tra,y_test=train_test_split(x,y,train_size=0.8,stratify=y,random_state=128)\n",
        "\n",
        "kf=KFold(7,True )\n",
        "train_score=[]\n",
        "test_score=[]\n",
        "val_score=[]\n",
        "test_list=[]\n",
        "\n",
        "idx=0\n",
        "\n",
        "val_list=[]\n",
        "\n",
        "for train_index, test_index in kf.split(x_tra):\n",
        "    callback_list = [\n",
        "    EarlyStopping( #성능 향상이 멈추면 훈련을 중지\n",
        "    monitor='val_loss',  #모델 검증 정확도를 모니터링\n",
        "    patience=50        #1 에포크 보다 더 길게(즉, 2에포크 동안 정확도가 향상되지 않으면 훈련 중지\n",
        "),\n",
        "    ModelCheckpoint( #에포크마다 현재 가중치를 저장\n",
        "    filepath=f'./mod{idx}.h5', #모델 파일 경로\n",
        "    monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\n",
        "    save_best_only=True,\n",
        "    mode='auto',\n",
        "    verbose=1\n",
        ")\n",
        "]\n",
        "    x_train,x_val=x_tra[train_index],x_tra[test_index]\n",
        "    y_train,y_val=y_tra[train_index],y_tra[test_index]\n",
        "    \n",
        "    #with strategy.scope():\n",
        "    model=mk_model()\n",
        "    print(model.summary())\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    hist=model.fit(x_train, y_train, epochs=200, validation_data=(x_val, y_val),batch_size=36,callbacks=callback_list)\n",
        "    \n",
        "    \n",
        "    plt.plot(hist.history['loss'],label='train'+str(idx))\n",
        "    plt.plot(hist.history['val_loss'],label='train'+str(idx))\n",
        "    plt.title('loss',fontsize=15)\n",
        "    plt.legend(['train','val'])\n",
        "    plt.show()\n",
        "    plt.plot(hist.history['accuracy'],label='train'+str(idx))\n",
        "    plt.plot(hist.history['val_accuracy'],label='train'+str(idx))\n",
        "    plt.legend(['train','val'])\n",
        "    plt.title('acc',fontsize=15)\n",
        "    plt.show()\n",
        "    model=load_model(f'./mod{idx}.h5')\n",
        "    \n",
        "    train_score.append(model.evaluate(x_train,y_train))\n",
        "    test_score.append(model.evaluate(x_test,y_test))\n",
        "    val_score.append(model.evaluate(x_val,y_val))\n",
        "    idx+=1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 19, 19, 19, 1)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 19, 19, 19,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_27 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_29 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_31 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_33 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_35 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_37 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_39 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_41 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_43 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_45 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_47 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_49 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_51 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_53 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_55 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_57 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_59 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_61 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_63 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_65 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_67 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_69 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_71 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_73 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_75 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_77 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_79 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_81 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_83 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_85 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_87 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_89 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_91 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_93 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_95 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_97 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_99 (Lambda)              (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_101 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_103 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_105 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_107 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_109 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_111 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_113 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_115 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_117 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_119 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_121 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_123 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_125 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_127 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_129 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_131 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_133 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_135 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_137 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_139 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_141 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_143 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_145 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_147 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_149 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_151 (Lambda)             (None, 19, 5, 19, 1) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_24 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_26 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_28 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_30 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_32 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_34 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_36 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_38 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_40 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_42 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_44 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_46 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_48 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_50 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_52 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_54 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_56 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_58 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_60 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_62 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_64 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_66 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_68 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_70 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_72 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_74 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_76 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_78 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_80 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_82 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_84 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_86 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_88 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_90 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_92 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_94 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_96 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_98 (Lambda)              (None, 19, 5, 5, 1)  0           lambda_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_100 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_102 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_104 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_106 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_108 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_110 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_112 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_114 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_116 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_118 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_120 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_122 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_124 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_126 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_128 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_130 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_132 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_134 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_136 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_138 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_140 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_142 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_144 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_146 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_148 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_150 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_12 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_13 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_14 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_15 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_16 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_17 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_18 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_19 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_20 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_21 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_22 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_23 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_24 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_25 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_26 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_27 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_28 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_29 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_30 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_31 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_32 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_33 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_34 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_35 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_36 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_37 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_38 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_39 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_40 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_41 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_42 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_43 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_44 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_45 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_46 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_47 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_48 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_49 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_50 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_51 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_52 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_53 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_54 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_55 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_56 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_57 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_58 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_59 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_60 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_61 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_62 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_63 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_64 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_65 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_66 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_67 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_68 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_69 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_70 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_71 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_72 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_73 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_74 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_75 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_65 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_67 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_71 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_72 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_73 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_12 (Gl (None, 8)            0           dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_13 (Gl (None, 8)            0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_14 (Gl (None, 8)            0           dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_15 (Gl (None, 8)            0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_16 (Gl (None, 8)            0           dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_17 (Gl (None, 8)            0           dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_18 (Gl (None, 8)            0           dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_19 (Gl (None, 8)            0           dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_20 (Gl (None, 8)            0           dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_21 (Gl (None, 8)            0           dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_22 (Gl (None, 8)            0           dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_23 (Gl (None, 8)            0           dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_24 (Gl (None, 8)            0           dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_25 (Gl (None, 8)            0           dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_26 (Gl (None, 8)            0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_27 (Gl (None, 8)            0           dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_28 (Gl (None, 8)            0           dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_29 (Gl (None, 8)            0           dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_30 (Gl (None, 8)            0           dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_31 (Gl (None, 8)            0           dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_32 (Gl (None, 8)            0           dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_33 (Gl (None, 8)            0           dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_34 (Gl (None, 8)            0           dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_35 (Gl (None, 8)            0           dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_36 (Gl (None, 8)            0           dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_37 (Gl (None, 8)            0           dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_38 (Gl (None, 8)            0           dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_39 (Gl (None, 8)            0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_40 (Gl (None, 8)            0           dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_41 (Gl (None, 8)            0           dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_42 (Gl (None, 8)            0           dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_43 (Gl (None, 8)            0           dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_44 (Gl (None, 8)            0           dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_45 (Gl (None, 8)            0           dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_46 (Gl (None, 8)            0           dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_47 (Gl (None, 8)            0           dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_48 (Gl (None, 8)            0           dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_49 (Gl (None, 8)            0           dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_50 (Gl (None, 8)            0           dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_51 (Gl (None, 8)            0           dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_52 (Gl (None, 8)            0           dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_53 (Gl (None, 8)            0           dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_54 (Gl (None, 8)            0           dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_55 (Gl (None, 8)            0           dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_56 (Gl (None, 8)            0           dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_57 (Gl (None, 8)            0           dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_58 (Gl (None, 8)            0           dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_59 (Gl (None, 8)            0           dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_60 (Gl (None, 8)            0           dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_61 (Gl (None, 8)            0           dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_62 (Gl (None, 8)            0           dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_63 (Gl (None, 8)            0           dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_64 (Gl (None, 8)            0           dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_65 (Gl (None, 8)            0           dropout_65[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_66 (Gl (None, 8)            0           dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_67 (Gl (None, 8)            0           dropout_67[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_68 (Gl (None, 8)            0           dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_69 (Gl (None, 8)            0           dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_70 (Gl (None, 8)            0           dropout_70[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_71 (Gl (None, 8)            0           dropout_71[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_72 (Gl (None, 8)            0           dropout_72[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_73 (Gl (None, 8)            0           dropout_73[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_74 (Gl (None, 8)            0           dropout_74[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_75 (Gl (None, 8)            0           dropout_75[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 512)          0           global_average_pooling3d_12[0][0]\n",
            "                                                                 global_average_pooling3d_13[0][0]\n",
            "                                                                 global_average_pooling3d_14[0][0]\n",
            "                                                                 global_average_pooling3d_15[0][0]\n",
            "                                                                 global_average_pooling3d_16[0][0]\n",
            "                                                                 global_average_pooling3d_17[0][0]\n",
            "                                                                 global_average_pooling3d_18[0][0]\n",
            "                                                                 global_average_pooling3d_19[0][0]\n",
            "                                                                 global_average_pooling3d_20[0][0]\n",
            "                                                                 global_average_pooling3d_21[0][0]\n",
            "                                                                 global_average_pooling3d_22[0][0]\n",
            "                                                                 global_average_pooling3d_23[0][0]\n",
            "                                                                 global_average_pooling3d_24[0][0]\n",
            "                                                                 global_average_pooling3d_25[0][0]\n",
            "                                                                 global_average_pooling3d_26[0][0]\n",
            "                                                                 global_average_pooling3d_27[0][0]\n",
            "                                                                 global_average_pooling3d_28[0][0]\n",
            "                                                                 global_average_pooling3d_29[0][0]\n",
            "                                                                 global_average_pooling3d_30[0][0]\n",
            "                                                                 global_average_pooling3d_31[0][0]\n",
            "                                                                 global_average_pooling3d_32[0][0]\n",
            "                                                                 global_average_pooling3d_33[0][0]\n",
            "                                                                 global_average_pooling3d_34[0][0]\n",
            "                                                                 global_average_pooling3d_35[0][0]\n",
            "                                                                 global_average_pooling3d_36[0][0]\n",
            "                                                                 global_average_pooling3d_37[0][0]\n",
            "                                                                 global_average_pooling3d_38[0][0]\n",
            "                                                                 global_average_pooling3d_39[0][0]\n",
            "                                                                 global_average_pooling3d_40[0][0]\n",
            "                                                                 global_average_pooling3d_41[0][0]\n",
            "                                                                 global_average_pooling3d_42[0][0]\n",
            "                                                                 global_average_pooling3d_43[0][0]\n",
            "                                                                 global_average_pooling3d_44[0][0]\n",
            "                                                                 global_average_pooling3d_45[0][0]\n",
            "                                                                 global_average_pooling3d_46[0][0]\n",
            "                                                                 global_average_pooling3d_47[0][0]\n",
            "                                                                 global_average_pooling3d_48[0][0]\n",
            "                                                                 global_average_pooling3d_49[0][0]\n",
            "                                                                 global_average_pooling3d_50[0][0]\n",
            "                                                                 global_average_pooling3d_51[0][0]\n",
            "                                                                 global_average_pooling3d_52[0][0]\n",
            "                                                                 global_average_pooling3d_53[0][0]\n",
            "                                                                 global_average_pooling3d_54[0][0]\n",
            "                                                                 global_average_pooling3d_55[0][0]\n",
            "                                                                 global_average_pooling3d_56[0][0]\n",
            "                                                                 global_average_pooling3d_57[0][0]\n",
            "                                                                 global_average_pooling3d_58[0][0]\n",
            "                                                                 global_average_pooling3d_59[0][0]\n",
            "                                                                 global_average_pooling3d_60[0][0]\n",
            "                                                                 global_average_pooling3d_61[0][0]\n",
            "                                                                 global_average_pooling3d_62[0][0]\n",
            "                                                                 global_average_pooling3d_63[0][0]\n",
            "                                                                 global_average_pooling3d_64[0][0]\n",
            "                                                                 global_average_pooling3d_65[0][0]\n",
            "                                                                 global_average_pooling3d_66[0][0]\n",
            "                                                                 global_average_pooling3d_67[0][0]\n",
            "                                                                 global_average_pooling3d_68[0][0]\n",
            "                                                                 global_average_pooling3d_69[0][0]\n",
            "                                                                 global_average_pooling3d_70[0][0]\n",
            "                                                                 global_average_pooling3d_71[0][0]\n",
            "                                                                 global_average_pooling3d_72[0][0]\n",
            "                                                                 global_average_pooling3d_73[0][0]\n",
            "                                                                 global_average_pooling3d_74[0][0]\n",
            "                                                                 global_average_pooling3d_75[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          262656      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 512)          262656      dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 512)          262656      dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            513         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 802,817\n",
            "Trainable params: 802,817\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 12s 960ms/step - loss: 99.3337 - accuracy: 0.5366 - val_loss: 93.5451 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.54508, saving model to ./mod0.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 91.7585 - accuracy: 0.5366 - val_loss: 86.2665 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.54508 to 86.26653, saving model to ./mod0.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 84.5210 - accuracy: 0.5366 - val_loss: 79.1975 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.26653 to 79.19753, saving model to ./mod0.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 77.6026 - accuracy: 0.5488 - val_loss: 72.4800 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.19753 to 72.48002, saving model to ./mod0.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 71.0135 - accuracy: 0.7195 - val_loss: 66.1581 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.48002 to 66.15805, saving model to ./mod0.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 64.7413 - accuracy: 0.6463 - val_loss: 60.1383 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.15805 to 60.13827, saving model to ./mod0.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 58.7647 - accuracy: 0.5976 - val_loss: 54.3769 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.13827 to 54.37691, saving model to ./mod0.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 53.0832 - accuracy: 0.6951 - val_loss: 48.9591 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.37691 to 48.95908, saving model to ./mod0.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 47.7028 - accuracy: 0.5854 - val_loss: 43.9093 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.95908 to 43.90931, saving model to ./mod0.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 42.6437 - accuracy: 0.5488 - val_loss: 39.0196 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.90931 to 39.01961, saving model to ./mod0.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 37.8535 - accuracy: 0.6585 - val_loss: 34.3668 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00011: val_loss improved from 39.01961 to 34.36676, saving model to ./mod0.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 33.3994 - accuracy: 0.6098 - val_loss: 30.1621 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.36676 to 30.16211, saving model to ./mod0.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 29.2055 - accuracy: 0.7561 - val_loss: 26.2499 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.16211 to 26.24989, saving model to ./mod0.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 25.3193 - accuracy: 0.7805 - val_loss: 22.5524 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.24989 to 22.55243, saving model to ./mod0.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 21.7382 - accuracy: 0.7927 - val_loss: 19.2072 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.55243 to 19.20718, saving model to ./mod0.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 18.4690 - accuracy: 0.7317 - val_loss: 16.1227 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.20718 to 16.12273, saving model to ./mod0.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 15.4582 - accuracy: 0.7073 - val_loss: 13.5825 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.12273 to 13.58250, saving model to ./mod0.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 12.8340 - accuracy: 0.6098 - val_loss: 10.9488 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.58250 to 10.94884, saving model to ./mod0.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 365ms/step - loss: 10.4340 - accuracy: 0.7073 - val_loss: 8.8609 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00019: val_loss improved from 10.94884 to 8.86087, saving model to ./mod0.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 8.3559 - accuracy: 0.8049 - val_loss: 6.9982 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.86087 to 6.99817, saving model to ./mod0.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 6.5964 - accuracy: 0.7805 - val_loss: 5.4751 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00021: val_loss improved from 6.99817 to 5.47506, saving model to ./mod0.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 363ms/step - loss: 5.1334 - accuracy: 0.8049 - val_loss: 4.2927 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.47506 to 4.29270, saving model to ./mod0.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 3.9847 - accuracy: 0.8049 - val_loss: 3.3332 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.29270 to 3.33322, saving model to ./mod0.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 362ms/step - loss: 3.4015 - accuracy: 0.5976 - val_loss: 2.7491 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.33322 to 2.74914, saving model to ./mod0.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 2.6718 - accuracy: 0.6585 - val_loss: 2.6865 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.74914 to 2.68649, saving model to ./mod0.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 2.4058 - accuracy: 0.6951 - val_loss: 2.3390 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.68649 to 2.33905, saving model to ./mod0.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 2.3686 - accuracy: 0.6707 - val_loss: 2.1323 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.33905 to 2.13234, saving model to ./mod0.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 2.0234 - accuracy: 0.7927 - val_loss: 2.2428 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 2.13234\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 1.8499 - accuracy: 0.6220 - val_loss: 1.6097 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00029: val_loss improved from 2.13234 to 1.60972, saving model to ./mod0.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.6217 - accuracy: 0.6707 - val_loss: 1.4563 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.60972 to 1.45628, saving model to ./mod0.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.4271 - accuracy: 0.8049 - val_loss: 1.6938 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.45628\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.4329 - accuracy: 0.5976 - val_loss: 1.3529 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.45628 to 1.35291, saving model to ./mod0.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 1.2331 - accuracy: 0.8293 - val_loss: 1.2183 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.35291 to 1.21833, saving model to ./mod0.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.1400 - accuracy: 0.7561 - val_loss: 1.2031 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.21833 to 1.20312, saving model to ./mod0.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 1.0739 - accuracy: 0.8293 - val_loss: 1.0923 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.20312 to 1.09229, saving model to ./mod0.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 1.0208 - accuracy: 0.8659 - val_loss: 1.1044 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.09229\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.9756 - accuracy: 0.8415 - val_loss: 1.0266 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.09229 to 1.02658, saving model to ./mod0.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.9213 - accuracy: 0.8293 - val_loss: 0.9646 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.02658 to 0.96458, saving model to ./mod0.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.9489 - accuracy: 0.7683 - val_loss: 0.9339 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.96458 to 0.93388, saving model to ./mod0.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.8930 - accuracy: 0.8049 - val_loss: 1.1954 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.93388\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.9351 - accuracy: 0.7805 - val_loss: 0.9072 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.93388 to 0.90720, saving model to ./mod0.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.9771 - accuracy: 0.7683 - val_loss: 0.9101 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.90720\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.8284 - accuracy: 0.8293 - val_loss: 1.4344 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.90720\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 1.0246 - accuracy: 0.6829 - val_loss: 0.8705 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.90720 to 0.87048, saving model to ./mod0.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.8593 - accuracy: 0.8171 - val_loss: 0.8602 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.87048 to 0.86022, saving model to ./mod0.h5\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.8048 - accuracy: 0.8415 - val_loss: 1.1037 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.86022\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.9107 - accuracy: 0.7317 - val_loss: 0.8737 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.86022\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.7757 - accuracy: 0.8415 - val_loss: 0.8955 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.86022\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.9134 - accuracy: 0.7317 - val_loss: 0.8569 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.86022 to 0.85691, saving model to ./mod0.h5\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.7997 - accuracy: 0.8171 - val_loss: 1.0155 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.85691\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.7814 - accuracy: 0.8415 - val_loss: 0.8302 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.85691 to 0.83023, saving model to ./mod0.h5\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.8136 - accuracy: 0.8171 - val_loss: 0.7983 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.83023 to 0.79833, saving model to ./mod0.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.7647 - accuracy: 0.8293 - val_loss: 0.8852 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.79833\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.7204 - accuracy: 0.8902 - val_loss: 0.7707 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.79833 to 0.77073, saving model to ./mod0.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.7464 - accuracy: 0.8293 - val_loss: 0.7964 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.77073\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.7251 - accuracy: 0.8537 - val_loss: 0.8297 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.77073\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.7531 - accuracy: 0.8537 - val_loss: 0.7699 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.77073 to 0.76993, saving model to ./mod0.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.7230 - accuracy: 0.8780 - val_loss: 0.9232 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.76993\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.7196 - accuracy: 0.9146 - val_loss: 0.7368 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.76993 to 0.73682, saving model to ./mod0.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6989 - accuracy: 0.8537 - val_loss: 0.7265 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.73682 to 0.72650, saving model to ./mod0.h5\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.6619 - accuracy: 0.9268 - val_loss: 0.8186 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.72650\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6667 - accuracy: 0.9024 - val_loss: 0.7274 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.72650\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6452 - accuracy: 0.9146 - val_loss: 0.6982 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.72650 to 0.69821, saving model to ./mod0.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.6864 - accuracy: 0.8659 - val_loss: 0.6990 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.69821\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6321 - accuracy: 0.9390 - val_loss: 0.8662 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.69821\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6426 - accuracy: 0.9268 - val_loss: 0.7399 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.69821\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6782 - accuracy: 0.8659 - val_loss: 0.8721 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.69821\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.7644 - accuracy: 0.8293 - val_loss: 0.6676 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.69821 to 0.66763, saving model to ./mod0.h5\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6583 - accuracy: 0.8902 - val_loss: 0.6604 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.66763 to 0.66045, saving model to ./mod0.h5\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6884 - accuracy: 0.8780 - val_loss: 0.7612 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.66045\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.7049 - accuracy: 0.8902 - val_loss: 0.7358 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.66045\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.6807 - accuracy: 0.8659 - val_loss: 0.9015 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.66045\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6693 - accuracy: 0.9146 - val_loss: 0.7127 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.66045\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.5970 - accuracy: 0.9512 - val_loss: 0.6635 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.66045\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.6036 - accuracy: 0.9390 - val_loss: 0.6957 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.66045\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5844 - accuracy: 0.9512 - val_loss: 0.7298 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.66045\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5768 - accuracy: 0.9268 - val_loss: 0.6418 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.66045 to 0.64177, saving model to ./mod0.h5\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6443 - accuracy: 0.9024 - val_loss: 0.6302 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.64177 to 0.63016, saving model to ./mod0.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5566 - accuracy: 0.9634 - val_loss: 0.9228 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.63016\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 364ms/step - loss: 0.6187 - accuracy: 0.9146 - val_loss: 0.6399 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.63016\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6213 - accuracy: 0.8902 - val_loss: 0.6654 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.63016\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.5761 - accuracy: 0.9390 - val_loss: 0.8157 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.63016\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.5899 - accuracy: 0.9024 - val_loss: 0.7086 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.63016\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6838 - accuracy: 0.8659 - val_loss: 0.7149 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.63016\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.6500 - accuracy: 0.8902 - val_loss: 0.8572 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.63016\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.5898 - accuracy: 0.9268 - val_loss: 0.6775 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.63016\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6527 - accuracy: 0.8902 - val_loss: 0.6100 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.63016 to 0.61003, saving model to ./mod0.h5\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.5301 - accuracy: 0.9756 - val_loss: 0.7640 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.61003\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.5412 - accuracy: 0.9634 - val_loss: 0.5855 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.61003 to 0.58549, saving model to ./mod0.h5\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5141 - accuracy: 0.9878 - val_loss: 0.6104 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.58549\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.5107 - accuracy: 0.9634 - val_loss: 0.5815 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.58549 to 0.58152, saving model to ./mod0.h5\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5103 - accuracy: 0.9756 - val_loss: 0.5710 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.58152 to 0.57102, saving model to ./mod0.h5\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.5281 - accuracy: 0.9512 - val_loss: 0.6564 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.57102\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.5004 - accuracy: 1.0000 - val_loss: 0.5940 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.57102\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4856 - accuracy: 0.9878 - val_loss: 0.5515 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.57102 to 0.55149, saving model to ./mod0.h5\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4894 - accuracy: 0.9878 - val_loss: 0.6041 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.55149\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4886 - accuracy: 0.9756 - val_loss: 0.5749 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.55149\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4805 - accuracy: 0.9878 - val_loss: 0.5587 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.55149\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 0.4698 - accuracy: 0.9878 - val_loss: 0.5699 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.55149\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.4793 - accuracy: 0.9878 - val_loss: 0.5781 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.55149\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4611 - accuracy: 0.9878 - val_loss: 0.5371 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.55149 to 0.53715, saving model to ./mod0.h5\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 0.4640 - accuracy: 0.9878 - val_loss: 0.6085 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.53715\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.4816 - accuracy: 0.9634 - val_loss: 0.5688 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.53715\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.4587 - accuracy: 0.9756 - val_loss: 0.5663 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.53715\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4508 - accuracy: 1.0000 - val_loss: 0.5965 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.53715\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4516 - accuracy: 1.0000 - val_loss: 0.5341 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.53715 to 0.53411, saving model to ./mod0.h5\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4451 - accuracy: 1.0000 - val_loss: 0.5220 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.53411 to 0.52203, saving model to ./mod0.h5\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.4416 - accuracy: 1.0000 - val_loss: 0.5383 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.52203\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.4437 - accuracy: 1.0000 - val_loss: 0.5523 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.52203\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4357 - accuracy: 1.0000 - val_loss: 0.5451 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.52203\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.4412 - accuracy: 0.9878 - val_loss: 0.5790 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.52203\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4480 - accuracy: 1.0000 - val_loss: 0.5858 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.52203\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.4473 - accuracy: 0.9878 - val_loss: 0.5090 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.52203 to 0.50896, saving model to ./mod0.h5\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.4286 - accuracy: 1.0000 - val_loss: 0.5168 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.50896\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4397 - accuracy: 1.0000 - val_loss: 0.5614 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.50896\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.4599 - accuracy: 1.0000 - val_loss: 0.5255 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.50896\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.4984 - accuracy: 0.9634 - val_loss: 0.5268 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.50896\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4742 - accuracy: 0.9756 - val_loss: 0.5411 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.50896\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4620 - accuracy: 0.9756 - val_loss: 0.6441 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.50896\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5130 - accuracy: 0.9512 - val_loss: 0.7538 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.50896\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.5084 - accuracy: 0.9756 - val_loss: 0.5392 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.50896\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4823 - accuracy: 0.9634 - val_loss: 0.5179 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.50896\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4709 - accuracy: 0.9756 - val_loss: 0.9192 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.50896\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5282 - accuracy: 0.9390 - val_loss: 0.5162 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.50896\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4972 - accuracy: 0.9512 - val_loss: 0.5190 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.50896\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.4357 - accuracy: 1.0000 - val_loss: 0.7165 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.50896\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.4399 - accuracy: 1.0000 - val_loss: 0.5001 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.50896 to 0.50010, saving model to ./mod0.h5\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4342 - accuracy: 0.9878 - val_loss: 0.4919 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.50010 to 0.49190, saving model to ./mod0.h5\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4260 - accuracy: 0.9878 - val_loss: 0.5724 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.49190\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4187 - accuracy: 1.0000 - val_loss: 0.5739 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.49190\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.4176 - accuracy: 1.0000 - val_loss: 0.5184 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.49190\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.4115 - accuracy: 1.0000 - val_loss: 0.5157 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.49190\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4102 - accuracy: 1.0000 - val_loss: 0.5112 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.49190\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4076 - accuracy: 1.0000 - val_loss: 0.5039 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.49190\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4047 - accuracy: 1.0000 - val_loss: 0.5186 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.49190\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4029 - accuracy: 1.0000 - val_loss: 0.5050 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.49190\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.4006 - accuracy: 1.0000 - val_loss: 0.4915 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.49190 to 0.49149, saving model to ./mod0.h5\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 1s 445ms/step - loss: 0.4033 - accuracy: 1.0000 - val_loss: 0.5302 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.49149\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 1s 446ms/step - loss: 0.4079 - accuracy: 1.0000 - val_loss: 0.5173 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.49149\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 1s 446ms/step - loss: 0.4011 - accuracy: 1.0000 - val_loss: 0.4949 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.49149\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 1s 432ms/step - loss: 0.3986 - accuracy: 1.0000 - val_loss: 0.4996 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.49149\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 1s 451ms/step - loss: 0.3960 - accuracy: 1.0000 - val_loss: 0.5048 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.49149\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 2s 643ms/step - loss: 0.3984 - accuracy: 1.0000 - val_loss: 0.4896 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.49149 to 0.48963, saving model to ./mod0.h5\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4043 - accuracy: 1.0000 - val_loss: 0.5069 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.48963\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3974 - accuracy: 1.0000 - val_loss: 0.5245 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.48963\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 0.3981 - accuracy: 1.0000 - val_loss: 0.4854 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.48963 to 0.48538, saving model to ./mod0.h5\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3994 - accuracy: 1.0000 - val_loss: 0.4750 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.48538 to 0.47504, saving model to ./mod0.h5\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4000 - accuracy: 1.0000 - val_loss: 0.5178 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.47504\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3975 - accuracy: 1.0000 - val_loss: 0.4926 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.47504\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3995 - accuracy: 1.0000 - val_loss: 0.4845 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.47504\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3974 - accuracy: 1.0000 - val_loss: 0.5042 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.47504\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3935 - accuracy: 1.0000 - val_loss: 0.4789 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.47504\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3937 - accuracy: 1.0000 - val_loss: 0.4867 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.47504\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3866 - accuracy: 1.0000 - val_loss: 0.5120 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.47504\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 0.3915 - accuracy: 1.0000 - val_loss: 0.4742 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00155: val_loss improved from 0.47504 to 0.47418, saving model to ./mod0.h5\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3904 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00156: val_loss improved from 0.47418 to 0.46312, saving model to ./mod0.h5\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3882 - accuracy: 1.0000 - val_loss: 0.4885 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.46312\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 0.3939 - accuracy: 1.0000 - val_loss: 0.5211 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.46312\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3907 - accuracy: 1.0000 - val_loss: 0.4681 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.46312\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3897 - accuracy: 1.0000 - val_loss: 0.4612 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.46312 to 0.46116, saving model to ./mod0.h5\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3959 - accuracy: 1.0000 - val_loss: 0.4849 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.46116\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3856 - accuracy: 1.0000 - val_loss: 0.4769 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.46116\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3869 - accuracy: 1.0000 - val_loss: 0.4639 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.46116\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3828 - accuracy: 1.0000 - val_loss: 0.4948 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.46116\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3919 - accuracy: 1.0000 - val_loss: 0.4881 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.46116\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3818 - accuracy: 1.0000 - val_loss: 0.4576 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.46116 to 0.45759, saving model to ./mod0.h5\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3885 - accuracy: 1.0000 - val_loss: 0.4949 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.45759\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3859 - accuracy: 1.0000 - val_loss: 0.5137 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.45759\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3817 - accuracy: 1.0000 - val_loss: 0.4819 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.45759\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.3833 - accuracy: 1.0000 - val_loss: 0.4580 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.45759\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3861 - accuracy: 1.0000 - val_loss: 0.4669 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.45759\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3844 - accuracy: 1.0000 - val_loss: 0.4823 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.45759\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3791 - accuracy: 1.0000 - val_loss: 0.4717 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.45759\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3757 - accuracy: 1.0000 - val_loss: 0.4591 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.45759\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3788 - accuracy: 1.0000 - val_loss: 0.4761 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.45759\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.3811 - accuracy: 1.0000 - val_loss: 0.4843 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.45759\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 0.3796 - accuracy: 1.0000 - val_loss: 0.4920 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.45759\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3835 - accuracy: 1.0000 - val_loss: 0.4816 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.45759\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3854 - accuracy: 1.0000 - val_loss: 0.4402 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss improved from 0.45759 to 0.44017, saving model to ./mod0.h5\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3789 - accuracy: 1.0000 - val_loss: 0.4651 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.44017\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3777 - accuracy: 1.0000 - val_loss: 0.5110 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.44017\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3805 - accuracy: 1.0000 - val_loss: 0.4762 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.44017\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.3807 - accuracy: 1.0000 - val_loss: 0.4677 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.44017\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3779 - accuracy: 1.0000 - val_loss: 0.4966 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.44017\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3769 - accuracy: 1.0000 - val_loss: 0.4575 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.44017\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3777 - accuracy: 1.0000 - val_loss: 0.4663 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.44017\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3793 - accuracy: 1.0000 - val_loss: 0.5180 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.44017\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3771 - accuracy: 1.0000 - val_loss: 0.4685 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.44017\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.3777 - accuracy: 1.0000 - val_loss: 0.4513 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.44017\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3786 - accuracy: 1.0000 - val_loss: 0.4912 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.44017\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3740 - accuracy: 1.0000 - val_loss: 0.5074 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.44017\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3742 - accuracy: 1.0000 - val_loss: 0.4566 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.44017\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3741 - accuracy: 1.0000 - val_loss: 0.4466 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.44017\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.3711 - accuracy: 1.0000 - val_loss: 0.4457 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.44017\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3765 - accuracy: 1.0000 - val_loss: 0.4551 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.44017\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3685 - accuracy: 1.0000 - val_loss: 0.4641 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.44017\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3726 - accuracy: 1.0000 - val_loss: 0.4684 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.44017\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3721 - accuracy: 1.0000 - val_loss: 0.4568 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.44017\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3738 - accuracy: 1.0000 - val_loss: 0.4516 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.44017\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3719 - accuracy: 1.0000 - val_loss: 0.4333 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.44017 to 0.43329, saving model to ./mod0.h5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 2s 113ms/step - loss: 0.3686 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.4618 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.4333 - accuracy: 1.0000\n",
            "(None, 19, 19, 19, 1)\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 19, 19, 19,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_153 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_155 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_157 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_159 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_161 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_163 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_165 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_167 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_169 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_171 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_173 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_175 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_177 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_179 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_181 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_183 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_185 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_187 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_189 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_191 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_193 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_195 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_197 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_199 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_201 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_203 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_205 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_207 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_209 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_211 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_213 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_215 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_217 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_219 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_221 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_223 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_225 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_227 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_229 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_231 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_233 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_235 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_237 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_239 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_241 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_243 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_245 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_247 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_249 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_251 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_253 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_255 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_257 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_259 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_261 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_263 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_265 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_267 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_269 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_271 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_273 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_275 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_277 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_279 (Lambda)             (None, 19, 5, 19, 1) 0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_152 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_154 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_156 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_158 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_160 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_162 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_164 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_166 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_168 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_170 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_172 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_174 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_176 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_178 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_180 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_182 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_184 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_186 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_188 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_190 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_192 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_194 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_196 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_198 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_200 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_202 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_204 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_206 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_208 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_210 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_212 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_214 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_216 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_218 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_220 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_222 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_224 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_226 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_228 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_230 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_232 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_234 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_236 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_238 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_240 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_242 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_244 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_246 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_248 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_250 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_252 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_254 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_256 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_258 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_260 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_262 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_264 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_266 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_268 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_270 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_272 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_274 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_276 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_278 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_76 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_77 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_78 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_79 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_80 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_81 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_82 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_83 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_84 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_85 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_86 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_87 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_88 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_89 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_90 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_91 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_92 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_93 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_94 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_95 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_96 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_97 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_98 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_99 (Conv3D)              (None, 17, 3, 3, 8)  224         lambda_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_100 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_101 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_102 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_103 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_104 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_105 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_106 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_107 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_108 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_109 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_110 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_111 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_112 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_113 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_114 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_115 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_116 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_117 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_118 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_119 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_120 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_121 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_122 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_123 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_124 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_125 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_126 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_127 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_128 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_129 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_130 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_131 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_132 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_133 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_134 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_135 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_136 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_137 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_138 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_139 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_76 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_77 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_79 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_80 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_81 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_82 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_83 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_84 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_85 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_86 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_87 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_88 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_89 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_90 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_91 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_92 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_93 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_94 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_95 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_96 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_97 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_98 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_99 (Dropout)            (None, 17, 3, 3, 8)  0           conv3d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_100 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_101 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_102 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_103 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_104 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_105 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_106 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_107 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_108 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_109 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_110 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_111 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_114 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_115 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_116 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_117 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_118 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_119 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_120 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_121 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_122 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_123 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_124 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_125 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_126 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_127 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_128 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_129 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_130 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_131 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_132 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_133 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_134 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_135 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_136 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_137 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_138 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_139 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_76 (Gl (None, 8)            0           dropout_76[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_77 (Gl (None, 8)            0           dropout_77[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_78 (Gl (None, 8)            0           dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_79 (Gl (None, 8)            0           dropout_79[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_80 (Gl (None, 8)            0           dropout_80[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_81 (Gl (None, 8)            0           dropout_81[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_82 (Gl (None, 8)            0           dropout_82[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_83 (Gl (None, 8)            0           dropout_83[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_84 (Gl (None, 8)            0           dropout_84[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_85 (Gl (None, 8)            0           dropout_85[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_86 (Gl (None, 8)            0           dropout_86[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_87 (Gl (None, 8)            0           dropout_87[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_88 (Gl (None, 8)            0           dropout_88[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_89 (Gl (None, 8)            0           dropout_89[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_90 (Gl (None, 8)            0           dropout_90[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_91 (Gl (None, 8)            0           dropout_91[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_92 (Gl (None, 8)            0           dropout_92[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_93 (Gl (None, 8)            0           dropout_93[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_94 (Gl (None, 8)            0           dropout_94[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_95 (Gl (None, 8)            0           dropout_95[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_96 (Gl (None, 8)            0           dropout_96[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_97 (Gl (None, 8)            0           dropout_97[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_98 (Gl (None, 8)            0           dropout_98[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_99 (Gl (None, 8)            0           dropout_99[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_100 (G (None, 8)            0           dropout_100[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_101 (G (None, 8)            0           dropout_101[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_102 (G (None, 8)            0           dropout_102[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_103 (G (None, 8)            0           dropout_103[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_104 (G (None, 8)            0           dropout_104[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_105 (G (None, 8)            0           dropout_105[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_106 (G (None, 8)            0           dropout_106[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_107 (G (None, 8)            0           dropout_107[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_108 (G (None, 8)            0           dropout_108[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_109 (G (None, 8)            0           dropout_109[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_110 (G (None, 8)            0           dropout_110[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_111 (G (None, 8)            0           dropout_111[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_112 (G (None, 8)            0           dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_113 (G (None, 8)            0           dropout_113[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_114 (G (None, 8)            0           dropout_114[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_115 (G (None, 8)            0           dropout_115[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_116 (G (None, 8)            0           dropout_116[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_117 (G (None, 8)            0           dropout_117[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_118 (G (None, 8)            0           dropout_118[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_119 (G (None, 8)            0           dropout_119[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_120 (G (None, 8)            0           dropout_120[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_121 (G (None, 8)            0           dropout_121[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_122 (G (None, 8)            0           dropout_122[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_123 (G (None, 8)            0           dropout_123[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_124 (G (None, 8)            0           dropout_124[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_125 (G (None, 8)            0           dropout_125[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_126 (G (None, 8)            0           dropout_126[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_127 (G (None, 8)            0           dropout_127[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_128 (G (None, 8)            0           dropout_128[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_129 (G (None, 8)            0           dropout_129[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_130 (G (None, 8)            0           dropout_130[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_131 (G (None, 8)            0           dropout_131[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_132 (G (None, 8)            0           dropout_132[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_133 (G (None, 8)            0           dropout_133[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_134 (G (None, 8)            0           dropout_134[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_135 (G (None, 8)            0           dropout_135[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_136 (G (None, 8)            0           dropout_136[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_137 (G (None, 8)            0           dropout_137[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_138 (G (None, 8)            0           dropout_138[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_139 (G (None, 8)            0           dropout_139[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 512)          0           global_average_pooling3d_76[0][0]\n",
            "                                                                 global_average_pooling3d_77[0][0]\n",
            "                                                                 global_average_pooling3d_78[0][0]\n",
            "                                                                 global_average_pooling3d_79[0][0]\n",
            "                                                                 global_average_pooling3d_80[0][0]\n",
            "                                                                 global_average_pooling3d_81[0][0]\n",
            "                                                                 global_average_pooling3d_82[0][0]\n",
            "                                                                 global_average_pooling3d_83[0][0]\n",
            "                                                                 global_average_pooling3d_84[0][0]\n",
            "                                                                 global_average_pooling3d_85[0][0]\n",
            "                                                                 global_average_pooling3d_86[0][0]\n",
            "                                                                 global_average_pooling3d_87[0][0]\n",
            "                                                                 global_average_pooling3d_88[0][0]\n",
            "                                                                 global_average_pooling3d_89[0][0]\n",
            "                                                                 global_average_pooling3d_90[0][0]\n",
            "                                                                 global_average_pooling3d_91[0][0]\n",
            "                                                                 global_average_pooling3d_92[0][0]\n",
            "                                                                 global_average_pooling3d_93[0][0]\n",
            "                                                                 global_average_pooling3d_94[0][0]\n",
            "                                                                 global_average_pooling3d_95[0][0]\n",
            "                                                                 global_average_pooling3d_96[0][0]\n",
            "                                                                 global_average_pooling3d_97[0][0]\n",
            "                                                                 global_average_pooling3d_98[0][0]\n",
            "                                                                 global_average_pooling3d_99[0][0]\n",
            "                                                                 global_average_pooling3d_100[0][0\n",
            "                                                                 global_average_pooling3d_101[0][0\n",
            "                                                                 global_average_pooling3d_102[0][0\n",
            "                                                                 global_average_pooling3d_103[0][0\n",
            "                                                                 global_average_pooling3d_104[0][0\n",
            "                                                                 global_average_pooling3d_105[0][0\n",
            "                                                                 global_average_pooling3d_106[0][0\n",
            "                                                                 global_average_pooling3d_107[0][0\n",
            "                                                                 global_average_pooling3d_108[0][0\n",
            "                                                                 global_average_pooling3d_109[0][0\n",
            "                                                                 global_average_pooling3d_110[0][0\n",
            "                                                                 global_average_pooling3d_111[0][0\n",
            "                                                                 global_average_pooling3d_112[0][0\n",
            "                                                                 global_average_pooling3d_113[0][0\n",
            "                                                                 global_average_pooling3d_114[0][0\n",
            "                                                                 global_average_pooling3d_115[0][0\n",
            "                                                                 global_average_pooling3d_116[0][0\n",
            "                                                                 global_average_pooling3d_117[0][0\n",
            "                                                                 global_average_pooling3d_118[0][0\n",
            "                                                                 global_average_pooling3d_119[0][0\n",
            "                                                                 global_average_pooling3d_120[0][0\n",
            "                                                                 global_average_pooling3d_121[0][0\n",
            "                                                                 global_average_pooling3d_122[0][0\n",
            "                                                                 global_average_pooling3d_123[0][0\n",
            "                                                                 global_average_pooling3d_124[0][0\n",
            "                                                                 global_average_pooling3d_125[0][0\n",
            "                                                                 global_average_pooling3d_126[0][0\n",
            "                                                                 global_average_pooling3d_127[0][0\n",
            "                                                                 global_average_pooling3d_128[0][0\n",
            "                                                                 global_average_pooling3d_129[0][0\n",
            "                                                                 global_average_pooling3d_130[0][0\n",
            "                                                                 global_average_pooling3d_131[0][0\n",
            "                                                                 global_average_pooling3d_132[0][0\n",
            "                                                                 global_average_pooling3d_133[0][0\n",
            "                                                                 global_average_pooling3d_134[0][0\n",
            "                                                                 global_average_pooling3d_135[0][0\n",
            "                                                                 global_average_pooling3d_136[0][0\n",
            "                                                                 global_average_pooling3d_137[0][0\n",
            "                                                                 global_average_pooling3d_138[0][0\n",
            "                                                                 global_average_pooling3d_139[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          262656      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 512)          262656      dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          262656      dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            513         dense_10[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 802,817\n",
            "Trainable params: 802,817\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 12s 1s/step - loss: 99.3440 - accuracy: 0.4512 - val_loss: 93.4348 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.43479, saving model to ./mod1.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 91.7466 - accuracy: 0.6463 - val_loss: 86.1303 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.43479 to 86.13035, saving model to ./mod1.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 84.5258 - accuracy: 0.5000 - val_loss: 79.1525 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.13035 to 79.15255, saving model to ./mod1.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 77.6254 - accuracy: 0.4634 - val_loss: 72.4938 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.15255 to 72.49380, saving model to ./mod1.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 71.0352 - accuracy: 0.6098 - val_loss: 66.1581 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.49380 to 66.15809, saving model to ./mod1.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 64.7686 - accuracy: 0.7073 - val_loss: 60.1200 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.15809 to 60.12000, saving model to ./mod1.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 58.8008 - accuracy: 0.6220 - val_loss: 54.3868 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.12000 to 54.38676, saving model to ./mod1.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 53.1321 - accuracy: 0.6585 - val_loss: 48.9387 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.38676 to 48.93873, saving model to ./mod1.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 47.7548 - accuracy: 0.6463 - val_loss: 43.7907 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.93873 to 43.79070, saving model to ./mod1.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 42.6779 - accuracy: 0.6341 - val_loss: 38.9481 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.79070 to 38.94809, saving model to ./mod1.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 37.8968 - accuracy: 0.6829 - val_loss: 34.3971 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.94809 to 34.39707, saving model to ./mod1.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 33.4163 - accuracy: 0.6585 - val_loss: 30.1356 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.39707 to 30.13561, saving model to ./mod1.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 29.2390 - accuracy: 0.6220 - val_loss: 26.1813 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.13561 to 26.18129, saving model to ./mod1.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 25.3397 - accuracy: 0.7195 - val_loss: 22.5317 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.18129 to 22.53172, saving model to ./mod1.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 21.7746 - accuracy: 0.6098 - val_loss: 19.1725 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.53172 to 19.17248, saving model to ./mod1.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 18.4801 - accuracy: 0.6463 - val_loss: 16.1037 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.17248 to 16.10365, saving model to ./mod1.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 15.4763 - accuracy: 0.7439 - val_loss: 13.3781 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.10365 to 13.37815, saving model to ./mod1.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 12.8149 - accuracy: 0.7439 - val_loss: 10.9212 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.37815 to 10.92116, saving model to ./mod1.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 10.4326 - accuracy: 0.7683 - val_loss: 8.7582 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00019: val_loss improved from 10.92116 to 8.75820, saving model to ./mod1.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 8.3611 - accuracy: 0.6951 - val_loss: 6.9197 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.75820 to 6.91975, saving model to ./mod1.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 6.6068 - accuracy: 0.6585 - val_loss: 5.4837 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00021: val_loss improved from 6.91975 to 5.48366, saving model to ./mod1.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 5.2151 - accuracy: 0.5366 - val_loss: 4.2268 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.48366 to 4.22676, saving model to ./mod1.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 4.0230 - accuracy: 0.6951 - val_loss: 3.4642 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.22676 to 3.46417, saving model to ./mod1.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 3.2481 - accuracy: 0.5488 - val_loss: 2.6980 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.46417 to 2.69797, saving model to ./mod1.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 2.6293 - accuracy: 0.6829 - val_loss: 2.4750 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.69797 to 2.47495, saving model to ./mod1.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 2.4984 - accuracy: 0.5122 - val_loss: 2.2824 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.47495 to 2.28239, saving model to ./mod1.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 2.2901 - accuracy: 0.6951 - val_loss: 2.1196 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.28239 to 2.11958, saving model to ./mod1.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 2.0596 - accuracy: 0.7561 - val_loss: 1.8768 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.11958 to 1.87684, saving model to ./mod1.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.8481 - accuracy: 0.5488 - val_loss: 1.6376 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.87684 to 1.63762, saving model to ./mod1.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 1.5946 - accuracy: 0.6829 - val_loss: 1.4336 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.63762 to 1.43359, saving model to ./mod1.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 1.4386 - accuracy: 0.7439 - val_loss: 1.3748 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.43359 to 1.37482, saving model to ./mod1.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 1.3748 - accuracy: 0.7805 - val_loss: 1.2978 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.37482 to 1.29775, saving model to ./mod1.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 1.2666 - accuracy: 0.8171 - val_loss: 1.1654 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.29775 to 1.16536, saving model to ./mod1.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.1721 - accuracy: 0.7805 - val_loss: 1.0935 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.16536 to 1.09351, saving model to ./mod1.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.1320 - accuracy: 0.8171 - val_loss: 1.0491 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.09351 to 1.04909, saving model to ./mod1.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 1.0740 - accuracy: 0.7439 - val_loss: 0.9890 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.04909 to 0.98895, saving model to ./mod1.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.0206 - accuracy: 0.8049 - val_loss: 0.9961 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.98895\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.9893 - accuracy: 0.8171 - val_loss: 0.9079 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.98895 to 0.90791, saving model to ./mod1.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.9370 - accuracy: 0.8902 - val_loss: 0.9359 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.90791\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.9223 - accuracy: 0.8415 - val_loss: 0.8373 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.90791 to 0.83728, saving model to ./mod1.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.9042 - accuracy: 0.8049 - val_loss: 0.9964 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.83728\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.9641 - accuracy: 0.7317 - val_loss: 0.8371 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.83728 to 0.83706, saving model to ./mod1.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.9865 - accuracy: 0.7439 - val_loss: 0.8154 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.83706 to 0.81544, saving model to ./mod1.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.9047 - accuracy: 0.8049 - val_loss: 0.8709 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.81544\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.8422 - accuracy: 0.8659 - val_loss: 0.8387 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.81544\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.8742 - accuracy: 0.7683 - val_loss: 0.8034 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.81544 to 0.80343, saving model to ./mod1.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.9020 - accuracy: 0.7683 - val_loss: 0.7789 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.80343 to 0.77887, saving model to ./mod1.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.8011 - accuracy: 0.8902 - val_loss: 0.7495 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.77887 to 0.74949, saving model to ./mod1.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.8012 - accuracy: 0.8415 - val_loss: 0.7619 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.74949\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.7664 - accuracy: 0.8659 - val_loss: 0.6906 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.74949 to 0.69057, saving model to ./mod1.h5\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.7436 - accuracy: 0.9024 - val_loss: 0.6711 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.69057 to 0.67110, saving model to ./mod1.h5\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.7413 - accuracy: 0.8659 - val_loss: 0.6607 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.67110 to 0.66070, saving model to ./mod1.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6896 - accuracy: 0.9268 - val_loss: 0.6460 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.66070 to 0.64601, saving model to ./mod1.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.7041 - accuracy: 0.9268 - val_loss: 0.6577 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.64601\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6855 - accuracy: 0.9390 - val_loss: 0.6596 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.64601\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6710 - accuracy: 0.9268 - val_loss: 0.6276 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.64601 to 0.62760, saving model to ./mod1.h5\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.6678 - accuracy: 0.9146 - val_loss: 0.6660 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.62760\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.6440 - accuracy: 0.9390 - val_loss: 0.6150 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.62760 to 0.61499, saving model to ./mod1.h5\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.6310 - accuracy: 0.9268 - val_loss: 0.7260 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.61499\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.6244 - accuracy: 0.9512 - val_loss: 0.5991 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.61499 to 0.59908, saving model to ./mod1.h5\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6283 - accuracy: 0.9390 - val_loss: 0.5904 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.59908 to 0.59040, saving model to ./mod1.h5\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.6026 - accuracy: 0.9390 - val_loss: 0.5773 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.59040 to 0.57730, saving model to ./mod1.h5\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.5849 - accuracy: 0.9512 - val_loss: 0.5814 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.57730\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.5792 - accuracy: 0.9634 - val_loss: 0.6459 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.57730\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.5940 - accuracy: 0.9024 - val_loss: 0.5725 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.57730 to 0.57253, saving model to ./mod1.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5561 - accuracy: 0.9634 - val_loss: 0.5700 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.57253 to 0.56996, saving model to ./mod1.h5\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.5497 - accuracy: 0.9634 - val_loss: 0.6326 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.56996\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.5416 - accuracy: 0.9878 - val_loss: 0.5769 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.56996\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5448 - accuracy: 0.9756 - val_loss: 0.6463 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.56996\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5581 - accuracy: 0.9390 - val_loss: 0.6205 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.56996\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5145 - accuracy: 0.9756 - val_loss: 0.6880 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.56996\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5352 - accuracy: 0.9634 - val_loss: 0.5600 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.56996 to 0.56002, saving model to ./mod1.h5\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5257 - accuracy: 0.9512 - val_loss: 0.6239 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.56002\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.5054 - accuracy: 0.9756 - val_loss: 0.5712 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.56002\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5319 - accuracy: 0.9390 - val_loss: 0.5878 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.56002\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.4884 - accuracy: 0.9878 - val_loss: 0.6399 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.56002\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.5879 - accuracy: 0.9268 - val_loss: 0.6640 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.56002\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4854 - accuracy: 0.9878 - val_loss: 0.5573 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.56002 to 0.55735, saving model to ./mod1.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.4959 - accuracy: 0.9512 - val_loss: 0.7042 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.55735\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.5024 - accuracy: 0.9878 - val_loss: 0.5733 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.55735\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.5269 - accuracy: 0.9512 - val_loss: 0.5980 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.55735\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.5017 - accuracy: 0.9512 - val_loss: 0.5523 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.55735 to 0.55233, saving model to ./mod1.h5\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.4671 - accuracy: 0.9878 - val_loss: 0.6299 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.55233\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4578 - accuracy: 1.0000 - val_loss: 0.5688 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.55233\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4563 - accuracy: 1.0000 - val_loss: 0.5472 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.55233 to 0.54724, saving model to ./mod1.h5\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.4672 - accuracy: 0.9756 - val_loss: 0.6117 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.54724\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4546 - accuracy: 1.0000 - val_loss: 0.5524 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.54724\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4493 - accuracy: 1.0000 - val_loss: 0.5319 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.54724 to 0.53188, saving model to ./mod1.h5\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4448 - accuracy: 0.9878 - val_loss: 0.5702 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.53188\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4485 - accuracy: 1.0000 - val_loss: 0.5313 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.53188 to 0.53132, saving model to ./mod1.h5\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4510 - accuracy: 0.9878 - val_loss: 0.5341 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.53132\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4358 - accuracy: 1.0000 - val_loss: 0.5736 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.53132\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4292 - accuracy: 1.0000 - val_loss: 0.5279 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.53132 to 0.52787, saving model to ./mod1.h5\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4323 - accuracy: 1.0000 - val_loss: 0.5465 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.52787\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4285 - accuracy: 1.0000 - val_loss: 0.5747 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.52787\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4251 - accuracy: 1.0000 - val_loss: 0.5266 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.52787 to 0.52662, saving model to ./mod1.h5\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4442 - accuracy: 0.9878 - val_loss: 0.6343 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.52662\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4326 - accuracy: 1.0000 - val_loss: 0.5276 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.52662\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.4417 - accuracy: 0.9878 - val_loss: 0.5319 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.52662\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4176 - accuracy: 1.0000 - val_loss: 0.6368 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.52662\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.4477 - accuracy: 0.9878 - val_loss: 0.5397 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.52662\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.4536 - accuracy: 0.9756 - val_loss: 0.6541 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.52662\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4725 - accuracy: 0.9878 - val_loss: 0.5359 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.52662\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4635 - accuracy: 0.9756 - val_loss: 0.5976 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.52662\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4642 - accuracy: 0.9634 - val_loss: 0.5312 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.52662\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4307 - accuracy: 0.9878 - val_loss: 0.5269 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.52662\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4108 - accuracy: 1.0000 - val_loss: 0.7101 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.52662\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.4346 - accuracy: 1.0000 - val_loss: 0.5326 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.52662\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.4364 - accuracy: 0.9756 - val_loss: 0.5374 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.52662\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4174 - accuracy: 1.0000 - val_loss: 0.5735 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.52662\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.4105 - accuracy: 1.0000 - val_loss: 0.5237 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.52662 to 0.52373, saving model to ./mod1.h5\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4124 - accuracy: 1.0000 - val_loss: 0.5595 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.52373\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4111 - accuracy: 1.0000 - val_loss: 0.5283 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.52373\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4146 - accuracy: 1.0000 - val_loss: 0.5300 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.52373\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4110 - accuracy: 0.9878 - val_loss: 0.6153 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.52373\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4098 - accuracy: 1.0000 - val_loss: 0.5240 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.52373\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4029 - accuracy: 1.0000 - val_loss: 0.5340 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.52373\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3966 - accuracy: 1.0000 - val_loss: 0.5692 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.52373\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4016 - accuracy: 1.0000 - val_loss: 0.5202 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.52373 to 0.52021, saving model to ./mod1.h5\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.4090 - accuracy: 0.9878 - val_loss: 0.5820 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.52021\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4055 - accuracy: 1.0000 - val_loss: 0.5794 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.52021\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3999 - accuracy: 1.0000 - val_loss: 0.5221 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.52021\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4010 - accuracy: 1.0000 - val_loss: 0.7383 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.52021\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4366 - accuracy: 0.9878 - val_loss: 0.5326 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.52021\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4202 - accuracy: 1.0000 - val_loss: 0.5375 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.52021\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.4069 - accuracy: 1.0000 - val_loss: 0.7007 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.52021\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4021 - accuracy: 1.0000 - val_loss: 0.5804 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.52021\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4276 - accuracy: 1.0000 - val_loss: 0.7276 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.52021\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4271 - accuracy: 1.0000 - val_loss: 0.5951 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.52021\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3999 - accuracy: 1.0000 - val_loss: 0.5492 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.52021\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4150 - accuracy: 1.0000 - val_loss: 0.6982 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.52021\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4093 - accuracy: 1.0000 - val_loss: 0.5242 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.52021\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4045 - accuracy: 1.0000 - val_loss: 0.5262 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.52021\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4034 - accuracy: 1.0000 - val_loss: 0.6889 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.52021\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4007 - accuracy: 1.0000 - val_loss: 0.5283 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.52021\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4007 - accuracy: 1.0000 - val_loss: 0.5776 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.52021\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3889 - accuracy: 1.0000 - val_loss: 0.5575 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.52021\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3880 - accuracy: 1.0000 - val_loss: 0.5208 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.52021\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3905 - accuracy: 1.0000 - val_loss: 0.5737 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.52021\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.3882 - accuracy: 1.0000 - val_loss: 0.5250 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.52021\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3843 - accuracy: 1.0000 - val_loss: 0.5204 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.52021\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3827 - accuracy: 1.0000 - val_loss: 0.5321 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.52021\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3805 - accuracy: 1.0000 - val_loss: 0.5181 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.52021 to 0.51809, saving model to ./mod1.h5\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3827 - accuracy: 1.0000 - val_loss: 0.5120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.51809 to 0.51197, saving model to ./mod1.h5\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3811 - accuracy: 1.0000 - val_loss: 0.5114 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00145: val_loss improved from 0.51197 to 0.51139, saving model to ./mod1.h5\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3759 - accuracy: 1.0000 - val_loss: 0.5215 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.51139\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3778 - accuracy: 1.0000 - val_loss: 0.5047 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.51139 to 0.50473, saving model to ./mod1.h5\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.3738 - accuracy: 1.0000 - val_loss: 0.5237 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.50473\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3757 - accuracy: 1.0000 - val_loss: 0.5176 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.50473\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3778 - accuracy: 1.0000 - val_loss: 0.5126 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.50473\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.3767 - accuracy: 1.0000 - val_loss: 0.5045 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00151: val_loss improved from 0.50473 to 0.50451, saving model to ./mod1.h5\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3782 - accuracy: 1.0000 - val_loss: 0.5418 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.50451\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3760 - accuracy: 1.0000 - val_loss: 0.5431 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.50451\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3726 - accuracy: 1.0000 - val_loss: 0.5286 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.50451\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3769 - accuracy: 1.0000 - val_loss: 0.5099 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.50451\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3729 - accuracy: 1.0000 - val_loss: 0.5031 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00156: val_loss improved from 0.50451 to 0.50312, saving model to ./mod1.h5\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3752 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.50312\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3758 - accuracy: 1.0000 - val_loss: 0.5017 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.50312 to 0.50172, saving model to ./mod1.h5\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3821 - accuracy: 1.0000 - val_loss: 0.5056 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.50172\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3754 - accuracy: 1.0000 - val_loss: 0.5509 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.50172\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3708 - accuracy: 1.0000 - val_loss: 0.5100 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.50172\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3707 - accuracy: 1.0000 - val_loss: 0.5187 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.50172\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3677 - accuracy: 1.0000 - val_loss: 0.4965 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss improved from 0.50172 to 0.49654, saving model to ./mod1.h5\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3709 - accuracy: 1.0000 - val_loss: 0.5168 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.49654\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3686 - accuracy: 1.0000 - val_loss: 0.5018 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.49654\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3726 - accuracy: 1.0000 - val_loss: 0.5145 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.49654\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3680 - accuracy: 1.0000 - val_loss: 0.5125 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.49654\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.3683 - accuracy: 1.0000 - val_loss: 0.4986 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.49654\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3695 - accuracy: 1.0000 - val_loss: 0.4997 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.49654\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3686 - accuracy: 1.0000 - val_loss: 0.5146 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.49654\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.3658 - accuracy: 1.0000 - val_loss: 0.4947 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.49654 to 0.49466, saving model to ./mod1.h5\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3665 - accuracy: 1.0000 - val_loss: 0.4988 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.49466\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3660 - accuracy: 1.0000 - val_loss: 0.4967 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.49466\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 0.3640 - accuracy: 1.0000 - val_loss: 0.4930 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00174: val_loss improved from 0.49466 to 0.49300, saving model to ./mod1.h5\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3656 - accuracy: 1.0000 - val_loss: 0.5111 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.49300\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3649 - accuracy: 1.0000 - val_loss: 0.5480 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.49300\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3676 - accuracy: 1.0000 - val_loss: 0.5014 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.49300\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3644 - accuracy: 1.0000 - val_loss: 0.5199 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.49300\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3658 - accuracy: 1.0000 - val_loss: 0.5096 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.49300\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3623 - accuracy: 1.0000 - val_loss: 0.5164 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.49300\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3657 - accuracy: 1.0000 - val_loss: 0.4910 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss improved from 0.49300 to 0.49096, saving model to ./mod1.h5\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3636 - accuracy: 1.0000 - val_loss: 0.4937 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.49096\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.3633 - accuracy: 1.0000 - val_loss: 0.4939 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.49096\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3600 - accuracy: 1.0000 - val_loss: 0.4892 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.49096 to 0.48919, saving model to ./mod1.h5\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3615 - accuracy: 1.0000 - val_loss: 0.4913 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.48919\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3633 - accuracy: 1.0000 - val_loss: 0.4907 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.48919\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3605 - accuracy: 1.0000 - val_loss: 0.4828 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.48919 to 0.48285, saving model to ./mod1.h5\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3600 - accuracy: 1.0000 - val_loss: 0.4908 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.48285\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.3611 - accuracy: 1.0000 - val_loss: 0.5166 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.48285\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3646 - accuracy: 1.0000 - val_loss: 0.4889 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.48285\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3693 - accuracy: 1.0000 - val_loss: 0.4773 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.48285 to 0.47726, saving model to ./mod1.h5\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3655 - accuracy: 1.0000 - val_loss: 0.5641 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.47726\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3626 - accuracy: 1.0000 - val_loss: 0.5174 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.47726\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3603 - accuracy: 1.0000 - val_loss: 0.4875 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.47726\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3617 - accuracy: 1.0000 - val_loss: 0.4820 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.47726\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3577 - accuracy: 1.0000 - val_loss: 0.4904 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.47726\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3603 - accuracy: 1.0000 - val_loss: 0.5113 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.47726\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3617 - accuracy: 1.0000 - val_loss: 0.5307 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.47726\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3628 - accuracy: 1.0000 - val_loss: 0.4815 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.47726\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3603 - accuracy: 1.0000 - val_loss: 0.5583 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.47726\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3Scd33n8fd3LprR3bIkS75LcRw7TgJ27KTOBigllIYASWgggaVt2nLItqXLbdkSlj2FnsMfYdvSwp4CDSVt2g0BNiFA9xBuaUJom6Q4iUOcxIkv8UW2JUuyZEm2NNfv/jGPHMVItqSRZqRnPq9zdGbmuczznWfGH//mN7/neczdERGRcImUuwAREZl7CncRkRBSuIuIhJDCXUQkhBTuIiIhpHAXEQkhhbtUDDP7BzPbUe46REpB4S4iEkIKdxGREFK4S8Uys81m9pCZnTazATO7x8zazlrmk2a218zGzKzHzH5gZu3BvLiZ/YWZHTKzlJkdNbMHzKyqPK9I5BWxchcgUg5m1go8ArwA/GegDrgD+LGZbXP3tJn9DvA/gE8AzwHNwJuA2uBpPgm8D7gdeBloB64DoqV7JSKTU7hLpfpvwe1vuPsQgJntAR4HbgLuBa4EfuTuX5qw3rcn3L8S+Lq73z1h2rfmr2SR6VO3jFSq8eAeGp/g7k8AB4DXBZN2AteZ2Z+Z2ZVmdnaLfCfwu2b2J2b2GjOzUhQuMh0Kd6lUy4GeSab3AEuD+3dR6Ja5GXgC6DGzz04I+c8CfwP8EfAMcNjMPjyvVYtMk8JdKtUxYNkk09uAEwDunnf3v3L3i4E1wF9Q6Gf/QDB/zN3/1N07gIuAbwJ/bWbXlqB+kXNSuEulegL4DTOrH59gZlcAHcC/nr2wux929zuAvcCmSebvAT4OpCabL1Jq+kFVKtXngT8Efmhmn+OV0TLPAvcDmNnfUmjFPw6cBH4NWE9h9Axm9gDwJPA0MAq8i8K/qUdL+UJEJqNwl4rk7r1m9mvAX1IYGZMGvg981N3TwWKPUeiC+S9AkkKr/QPu/p1g/r8DtwD/ncK34OeBm9xdpziQsjNdZk9EJHzU5y4iEkIKdxGREFK4i4iEkMJdRCSEFsRomZaWFu/o6Ch3GSIii8qTTz7Z5+6tk81bEOHe0dHBjh0aPSYiMhNmdnCqeeqWEREJIYW7iEgIKdxFREJoQfS5i4jMRiaToauri7GxsXKXMq+SySSrVq0iHo9Pe53zhruZ3QW8HTju7pcG05ZSOL1pB4WLG9zs7gPBxQq+QOFSY6eB33X3p2b4OkREpqWrq4v6+no6OjoI67VS3J3+/n66urro7Oyc9nrT6Zb5B+Ds81PfDjzk7uuBh4LHAG+lcNa89cBtwJenXYmIyAyNjY3R3Nwc2mAHMDOam5tn/O3kvOHu7o8SXLxgghuA8etG3g3cOGH6P3rB48ASM1s+o4pERGYgzME+bjavcbY/qLa5+7HgfjeFq9cArAQOT1iuK5j2S8zsNjPbYWY7ent7Z1XEzw+c4HM/2I3ObCki8mpFj5bxQrLOOF3d/U533+bu21pbJz3A6rx+0XWSLz+yj5OjmVmtLyJSjMHBQb70pS/NeL3rrruOwcHBeajoFbMN957x7pbg9ngw/QiwesJyq4Jp86KtIQFA91C4fykXkYVpqnDPZrPnXO/73/8+S5Ysma+ygNmH+/eAW4P7twLfnTD9d6xgO3ByQvfNnGtvSALQM5Sar02IiEzp9ttvZ9++fWzevJkrrriC17/+9Vx//fVs2lS4jO6NN97I1q1bueSSS7jzzjvPrNfR0UFfXx8HDhzg4osv5gMf+ACXXHIJb3nLWxgdHZ2T2qYzFPJe4I1Ai5l1AZ+mcK3Jb5nZ+4GDwM3B4t+nMAxyL4WhkL83J1VOoW083E+q5S5S6f7sn5/j+aNDc/qcm1Y08Ol3XDLl/DvuuINdu3axc+dOHnnkEd72trexa9euM0MW77rrLpYuXcro6ChXXHEFN910E83Nza96jj179nDvvffy1a9+lZtvvpn777+f3/qt3yq69vOGu7u/d4pZ10yyrAMfLLao6VoWdMv0qFtGRBaAK6+88lVj0b/4xS/ywAMPAHD48GH27NnzS+He2dnJ5s2bAdi6dSsHDhyYk1oW9RGqiViUppq4+txF5Jwt7FKpra09c/+RRx7hJz/5CY899hg1NTW88Y1vnHSseiKROHM/Go3OWbfMoj+3TFtDUn3uIlIW9fX1DA8PTzrv5MmTNDU1UVNTw+7du3n88cdLWtuibrnDeLir5S4ipdfc3MzVV1/NpZdeSnV1NW1tbWfmXXvttXzlK1/h4osvZsOGDWzfvr2ktYUg3BO8cGxuf0QREZmur3/965NOTyQSPPjgg5POG+9Xb2lpYdeuXWemf/zjH5+zuhZ9t0x7Q5K+kRTZXL7cpYiILBiLPtyXNSTJO/SNpMtdiojIgrG4w/257/DWp/8QI69+dxGRCRZ3uJ/qpbnn32hhSMMhRUQmWNzh3rACgDY7wXGFu4jIGaEI95WRAbXcRUQmWNzhXl8I9wurh3Qgk4gseHV1dSXb1uIO99pWiMToiJ/UD6oiIhMs7oOYIhGoX86q3IDCXURK7vbbb2f16tV88IOF8yV+5jOfIRaL8fDDDzMwMEAmk+Gzn/0sN9xwQ8lrW9zhDtCwgraBE3TrtL8ile3B26H72bl9zvbL4K13TDn7lltu4SMf+ciZcP/Wt77FD3/4Qz70oQ/R0NBAX18f27dv5/rrry/5tV4Xf7jXL6ep72mGxrKMpnNUV0XLXZGIVIgtW7Zw/Phxjh49Sm9vL01NTbS3t/PRj36URx99lEgkwpEjR+jp6aG9vb2ktS3+cG9YQX36h4DTMzRGR0vteVcRkRA6Rwt7Pr373e/mvvvuo7u7m1tuuYV77rmH3t5ennzySeLxOB0dHZOe6ne+Le4fVAEaVhDLjVLPqPrdRaTkbrnlFr7xjW9w33338e53v5uTJ0+ybNky4vE4Dz/8MAcPHixLXYu/5V6/HIB2O6Gx7iJScpdccgnDw8OsXLmS5cuX8773vY93vOMdXHbZZWzbto2NGzeWpa7FH+4NK4FCuB/XWHcRKYNnn33lh9yWlhYee+yxSZcbGRkpVUlh6JYptNzXxAbVchcRCSz+cA+6ZS5I6EAmEZFxiz/cYwmoaWFtfFDhLlKB3L3cJcy72bzGxR/uAA3LabcBnV9GpMIkk0n6+/tDHfDuTn9/P8lkckbrLf4fVAEaVtI6tI/uoTHcveRHgolIeaxatYquri56e3vLXcq8SiaTrFq1akbrhCPc65fTkHmcdDbP4OkMTbVV5a5IREogHo/T2dlZ7jIWpJB0y6ykOjNIgjTHdI4ZEZGwhHthxMwyG+DYydEyFyMiUn4hCffCRTvaGeCoWu4iIiEJ9+CKTCujJzg2qJa7iEg4wj1ouV+YHFafu4gIYQn3ZANU1dFRNchRtdxFRIoLdzP7qJk9Z2a7zOxeM0uaWaeZPWFme83sm2ZWmnGJ9ctZGRlUy11EhCLC3cxWAh8Ctrn7pUAUeA/wOeCv3P1CYAB4/1wUel4NK1hGP90nx8jnw3u0mojIdBTbLRMDqs0sBtQAx4A3AfcF8+8GbixyG9PTsILGbB/pXJ7+U+mSbFJEZKGadbi7+xHgL4BDFEL9JPAkMOju2WCxLmDlZOub2W1mtsPMdszJocMNK6hJ9RIhr7HuIlLxiumWaQJuADqBFUAtcO1013f3O919m7tva21tnW0Zr6hfTsRztHCSo4PqdxeRylZMt8ybgZfdvdfdM8C3gauBJUE3DcAq4EiRNU5P42oAVli/Wu4iUvGKCfdDwHYzq7HCaRivAZ4HHgbeFSxzK/Dd4kqcpsbCGdM6Yv0aDikiFa+YPvcnKPxw+hTwbPBcdwKfAD5mZnuBZuBrc1Dn+S0ptNw3VJ/UKQhEpOIVdcpfd/808OmzJu8HrizmeWcl2QiJBjpjA/xYLXcRqXDhOEJ1XOMqVkb6dSCTiFS80IX7snwvPUNjZHP5clcjIlI2oQv3xnQPeYfjw7qeqohUrpCF+2qSmUGqGdNwSBGpaKELdyiMddeBTCJSyUIW7oWx7iutTy13EalooQz3zviAWu4iUtHCFe71y8GirE8MquUuIhUtXOEejUHDCjpiJzTWXUQqWrjCHaBxFSusT90yIlLRQhnuzdle+kZSpLK5clcjIlIWoQz3+nQPEfJ0q2tGRCpUKMM94llaOMmRAf2oKiKVKYThvgYojHXvUriLSIUKYbgXxrqvivTRNXC6zMWIiJRHaMN9Y/VJtdxFpGKFL9yTDZBo5IKqQYW7iFSs8IU7QOMqVkf61S0jIhUrnOG+ZDXLvJfuoTHSWV20Q0QqTzjDvXEVjelu8o7GuotIRQptuCcyQ9Qyqq4ZEalIIQ33Vy7aoR9VRaQShTPcl6wFYG3kuFruIlKRwhnuTYVwv6RawyFFpDKFM9xrWyFew/qEumVEpDKFM9zNYMla1kZ61S0jIhUpnOEO0LSW9lyPxrqLSEUKb7gvWcuS9FHy7rqeqohUnPCGe1MH8ewpmhhWv7uIVJwQh3thxMxqU7+7iFSe8Ib7+Fj3aK9a7iJScYoKdzNbYmb3mdluM3vBzK4ys6Vm9mMz2xPcNs1VsTMStNw3JU8o3EWk4hTbcv8C8AN33wi8FngBuB14yN3XAw8Fj0svUQ81zayv0ql/RaTyzDrczawReAPwNQB3T7v7IHADcHew2N3AjcUWOWtL1gZ97mq5i0hlKabl3gn0An9vZk+b2d+ZWS3Q5u7HgmW6gbbJVjaz28xsh5nt6O3tLaKMc2haS1uuW2PdRaTiFBPuMeBy4MvuvgU4xVldMO7ugE+2srvf6e7b3H1ba2trEWWcQ1MHDaluzPMa6y4iFaWYcO8Cutz9ieDxfRTCvsfMlgMEt8eLK7EIS9YS8SztnODQCfW7i0jlmHW4u3s3cNjMNgSTrgGeB74H3BpMuxX4blEVFiMYMbMmcpyD/Qp3EakcsSLX/6/APWZWBewHfo/CfxjfMrP3AweBm4vcxuw1dQDQEe3jYP+pspUhIlJqRYW7u+8Etk0y65pinnfONK4Gi3Bp9QCPquUuIhUkvEeoAkTj0LCSC+N96pYRkYoS7nAHWLKWlRzn0InTFAbviIiEX/jDvamD5mw3o5kcvcOpclcjIlISFRDua6lJ9ZIgzQF1zYhIhQh/uAdnh1xlvRoxIyIVI/zhvrQTgAsiPfpRVUQqRgWE+zoAXlNzgoM6SlVEKkT4w71mKSQbubjquLplRKRihD/czWDpOjqsW90yIlIxwh/uAM3raMse4eRohsHT6XJXIyIy7yoj3Jeuo26smwRptd5FpCJURrg3r8NwVttxDqjfXUQqQGWEezBiptO6OaSWu4hUgMoI9+YLALisuk9HqYpIRaiMcK9uguqlXJzo5dAJdcuISPhVRrgDNK+j07rVcheRilA54b50HW3Zo/QOpzidzpa7GhGReVU54d68jvpUD0lSuli2iIRe5YT70sKPqmuth5d71e8uIuFWOeHeXBgO2WHd7O9TuItIuFVOuI+fHbK6j33HR8pcjIjI/KqccE82QG0rlyT62NercBeRcKuccIfC2SEj3ezvPaWLZYtIqFVWuDevY1nmCMOprC6WLSKhVlnhvvQCalK91DDGXnXNiEiIVVa4N18IFEbM7NNwSBEJsYoM943xHo2YEZFQq7xwtwjbans1YkZEQq2ywj2ehKYOLo4dZb+6ZUQkxCor3AFaN7Imd4gjg6OMpnPlrkZEZF5UYLhvoGn0MDGy7O9T14yIhFPR4W5mUTN72sz+X/C408yeMLO9ZvZNM6sqvsw51LKBiGdYaz0aMSMioTUXLfcPAy9MePw54K/c/UJgAHj/HGxj7rRuAODCyFGNmBGR0Coq3M1sFfA24O+Cxwa8CbgvWORu4MZitjHnWi4CYGtNj84OKSKhVWzL/a+BPwHyweNmYNDdxy911AWsnGxFM7vNzHaY2Y7e3t4iy5iBRB00ruayeLda7iISWrMOdzN7O3Dc3Z+czfrufqe7b3P3ba2trbMtY3ZaN9Dph9nfN0I+rxOIiUj4FNNyvxq43swOAN+g0B3zBWCJmcWCZVYBR4qqcD60bqQldYh0JsvRk6PlrkZEZM7NOtzd/ZPuvsrdO4D3AP/i7u8DHgbeFSx2K/Ddoquca60biOVTrLRejZgRkVCaj3HunwA+ZmZ7KfTBf20etlGclsKImfV2hD09w2UuRkRk7sXOv8j5ufsjwCPB/f3AlXPxvPOmtTBiZnOyhxeOKdxFJHwq7whVgOomqGtnc7KHF3uGyl2NiMicq8xwB2i9iHXWxZ6eEXIaMSMiIVPB4b6RZWMHSGVzHOjXj6oiEi4VHO4biOdOs5wT7Fa/u4iETAWH+0YALop08WK3+t1FJFwqN9yXbQLgqvoedner5S4i4VK54V6zFBpWcnlVFy9qrLuIhEzlhjtA26Wsy+3nYP9pTqWy519eRGSRqOxwb7+MptGDJEjzklrvIhIiFR/uEc+x3rp4Uf3uIhIiFR/uAJvjh/WjqoiESmWHe1MnVNVxVc1Rdms4pIiESGWHeyQCbZewKXqIF7uHcddpCEQkHCo73AHaL2Pl2F4GT6foHU6VuxoRkTmhcG+/jKrcKVZZr/rdRSQ0FO5thR9VN9lBnj+mfncRCQeF+7KLwSL8Ss1Rdh05We5qRETmhMK9qgaa17O1qovnjqrlLiLhoHAHaL+Mztx+Xu47xfBYptzViIgUTeEO0H4pDaluGhhR611EQkHhDmeOVN0UOaR+dxEJBYU7QPtrALiqWv3uIhIOCneAumXQuJrtyYM8q5a7iISAwn3cii1syL3Evt4R/agqIouewn3cyq0sGTvCEh/i2S613kVkcVO4j1u5FYDXRvbz9OHBMhcjIlIchfu4FZsB41frDrNT4S4ii5zCfVyiHlo3cmXVAXYeHtTpf0VkUVO4T7TyctaldtM7PMbRk2PlrkZEZNYU7hOtuoJkZoC11sPOQ+qaEZHFS+E+0ZrtAGyP7eGpQwNlLkZEZPZmHe5mttrMHjaz583sOTP7cDB9qZn92Mz2BLdNc1fuPGvZAIlG3lx/gB0HTpS7GhGRWSum5Z4F/pu7bwK2Ax80s03A7cBD7r4eeCh4vDhEIrD6Cjb7S+w6OsTpdLbcFYmIzMqsw93dj7n7U8H9YeAFYCVwA3B3sNjdwI3FFllSq7fTMrqf2vyIhkSKyKI1J33uZtYBbAGeANrc/Vgwqxtom2Kd28xsh5nt6O3tnYsy5sbqKzGcLZG97DigfncRWZyKDnczqwPuBz7i7q86paIXBotPOmDc3e90923uvq21tbXYMubOyq1gUa6t38+Ogwp3EVmcigp3M4tTCPZ73P3bweQeM1sezF8OHC+uxBJL1MGKLVwV3c1TBwfI5vLlrkhEZMaKGS1jwNeAF9z98xNmfQ+4Nbh/K/Dd2ZdXJh2vY83obnIpXZlJRBanYlruVwO/DbzJzHYGf9cBdwC/bmZ7gDcHjxeXjtcT8QyXR/bw2P7+clcjIjJjsdmu6O7/CtgUs6+Z7fMuCGt+BSzKdXX7+NG+fv7gV9eVuyIRkRnREaqTSdTDii28Lr6bnx84QUb97iKyyCjcp9LxOlaffgHSp/hFl8a7i8jionCfyoXXEPEMV0We49/3qt9dRBYXhftUVm+HeC2/Wb+bn760gA6yEhGZBoX7VGJV0PkGruZpnjp0gsHT6XJXJCIybQr3c7nwGpakjrKGbh7d01fuakREpk3hfi4XvhmA65K7eOTFxXWgrYhUNoX7uSzthJaLuKH6F/z0xV7yeV1XVUQWB4X7+Wx8G+tHnyFzakBXZxKRRUPhfj4b3kbEs7w59gw/2NVd7mpERKZF4X4+K7dCXRvvbfgFP3ium8JZjEVEFjaF+/lEIrDhOjandtA7cFJniRSRRUHhPh2bbiCeO82bo0/z4K5j519eRKTMFO7T0fkGqGvn9xp+zneePqpRMyKy4CncpyMShUtvYkvq5wwP9vL4yzrXjIgsbAr36brsXUTzGd6ZeJL7nzxS7mpERM5J4T5dK7ZAywbeX/MoD+46xkgqW+6KRESmpHCfLjO44v2sGX2BdZk9fPuprnJXJCIyJYX7TLz2PXi8lg83/pS//7cD+mFVRBYshftMJBux19zMr6UfZajvqM7zLiILlsJ9pq76YyKe5RM1/8z//pc9OmJVRBYkhftMtVyIXf7b3OQ/4vjhl3S+GRFZkBTus/GrnyASjfHZuvv53A92k87my12RiMirKNxno2EF9rqP8sbMz1gx8B985af7yl2RiMirKNxn6+qPQFMnn6/9J772L7vYe3y43BWJiJyhcJ+teBLe/nnaskf4avzP+dj/+Xf6R1LlrkpEBFC4F2fdm7B3/i1X2At8avAz/P5Xf8rxobFyVyUionAv2mtuxt55J1dGd/M/B/+Uz3zhyzz49EG6Bk5rmKSIlI0thADatm2b79ixo9xlFOfZ+8h/54+I5FKMehX/kd/I3uQmWPOf6NhyDdvXt1ObiJW7ShEJETN70t23TTpP4T6HUsOk9/2MgWd/SNWhn9F4aj8RnCGv4Wf+Wg61vJ7WLe/g8o0X0NFcSyRi5a5YRBaxkoe7mV0LfAGIAn/n7neca/nQhPvZxobI7HuEE0/9M7UHf0Jd9gR5N05QzwkaGY41MZBczUhdJ1bbQqyuhaqGVhINLdTVVFNXnaR2aTuNNQlqq6KYGbjD6ACeXMJY1hnN5GhIxohF1cMmUmlKGu5mFgVeAn4d6AJ+DrzX3Z+fap3QhvtE+Tx+9Gn6nvkBQz0vkx3qITF2nNbUYWr91JSrpT1KlhhR8hyjmSWMsMRG6PcGDvkyskSoZ5TqSI5sJEGKOGNUMepxPJbE4jXkIgny0SqykST5aJJcNIFHq7BIlEg0hkWiZ/6iESPpo2BRPJakfuwYFolwqr6z8HyRKNFIBCxK3FMkMkPkq+rIJ5uIGFRlh4nlU3iikWh2lGh6CI8liKRHiGRHSTVegEfjxDLD5OJ1mMWI5k6TJEU2dZqRU6dJNq+mrsqInthLrradXF0bns2QjyUxi5AY6yUaryKarIdEPZYegfQpqKrFq+oglsQypwGHaByLJiCXgewYlqjFLYblxrBsCrcIxGuwkWOFx0vWYrEEpIZh8BCWbICqWhg4gFUvgfrlMDoIsQRUNxXOFtr7IvTvg/ZLoXE1eL6wbfdXbl91/+z5+cL9SAySDZAZLWw/Xg3xGkiPwMuPFupYcxVEq2DgZejbC42rIJeG/r2FbTd1FNbrewmyY4ULvCca4NRx6H62UP/STkiNQPDaiSchM1bYTiwBsWThzwyws27P8kvTbGbzF8pzTPbaZvocoyfgVF9hf1bVQ1VNYXo+B54rvM+5LOSzhfcSYKSn8L41LJ9k++d3rnCfj07gK4G97r4/2Pg3gBuAKcO9IkQi2KqttK7aSuvE6eMt8VN9jA71cepED6nhXlKpFGOpFDZ8lEwmQzqbp3asm4FINX1Vq2nPHKQ520+MHKdtBYO5KJ4ZpYo0jaRZ5qNYdoDI2BhVnqbKU1SRIUF6RmXn3DAgYqXpvmsryVYWn5wb0XO8B1mPEDMdKb0YPbP507z2xo/N+fPOR7ivBA5PeNwF/Mo8bCcczKBmKVazlJrWi6hZN8/by+chl4JsqtCSCFoVns+SzeXI5ZxMrJp8NkM+dZpMTRvZfA4bOEg+myafz5PLZQu3xEhVNWKpIWzsJDmHdKyObCRReBytIVNVj+XS5GK15CJxaof3A5CN1RLNnAJ3stEEQ7k40UQtzfU1nOo9wKlUjuH6TmrHeklmBgqt/VwKz+cYiTeTyWSIpEeI506RidaQiVYTy41SlTtFJJcmG63GMaKeIZpPk7M42UgV8dwo5oVvOTmLE/EcsdwoI4lWcpEE9aNHiHqWTLSak4nlJHIjxHOnOZlYQSI7Qk26n7F4A9F8mmR2CPM8Q4l2TlR30HJqDzWZAdwMd8ODFq9jOBS+JQB5L7SCnVf+MMPyORK5EbKRBGPRGmL5NPH8GE6ErrrXEM+P0XZqNxHyDMWX0ZvsoCFzHMc4UbWS+kwfDenjxPKjDCRWkY1UsXzkBWKeJhWt41j1hdRn+mhMd5OK1AJOPJ8inh8jE0mQjtQQzaeI5dPEvHDMhgXfLgyHCf+3+JlXFXwBCaa+ip/9YJL/nCb0HPjElabxnB48OLu2iQ/8zGt4Zapx1uOzVp/42jhrncnqGJ9/OlrHSHQJMU+TzI9SlR/FieBm5ImSt0hwGyWRHwWcoVgzWztfz3wo2/ANM7sNuA1gzZo15Sqj8kQiEKkufHWfwIB48JecbL2WpXNUwIXTWOaiCfc3ztF2S2FLCbbxhhkuPz/BIQvffPwKdwRYPeHxqmDaq7j7ne6+zd23tba2nj1bRESKMB/h/nNgvZl1mlkV8B7ge/OwHRERmcKcd8u4e9bM/hj4IYWhkHe5+3NzvR0REZnavPS5u/v3ge/Px3OLiMj56cgXEZEQUriLiISQwl1EJIQU7iIiIbQgzgppZr3AwVmu3gL0zWE5c2mh1qa6ZkZ1zdxCrS1sda1190kPFFoQ4V4MM9sx1Ylzym2h1qa6ZkZ1zdxCra2S6lK3jIhICCncRURCKAzhfme5CziHhVqb6poZ1TVzC7W2iqlr0fe5i4jILwtDy11ERM6icBcRCaFFHe5mdq2ZvWhme83s9jLWsdrMHjaz583sOTP7cDD9M2Z2xMx2Bn/XlaG2A2b2bLD9HcG0pWb2YzPbE9w2lbimDRP2yU4zGzKzj5Rrf5nZXWZ23Mx2TZg26T6ygi8Gn7lfmNnlJa7rz81sd7DtB8xsSTC9w8xGJ+y7r5S4rinfOzP7ZLC/XjSz35ivus5R2zcn1HXAzHYG00uyz86RD/P7GXP3RflH4XTC+4ALgCrgGWBTmWpZDlwe3K+ncIHwTcBngI+XeT8dAFrOmva/gNuD+7cDnyvz+9gNrC3X/qJweaPLgV3n20fAdcCDFC5etR14osR1vQWIBfc/N6GujonLlWF/TfreBa3hfrwAAANHSURBVP8OngESQGfwbzZaytrOmv+XwJ+Wcp+dIx/m9TO2mFvuZy7E7e5pYPxC3CXn7sfc/ang/jDwAoVryS5UNwB3B/fvBm4sYy3XAPvcfbZHKBfN3R8FTpw1eap9dAPwj17wOLDEzGZ36fpZ1OXuP3L3bPDwcQpXOiupKfbXVG4AvuHuKXd/GdhL4d9uyWszMwNuBu6dr+1PUdNU+TCvn7HFHO6TXYi77IFqZh0ULqb5RDDpj4OvVneVuvsj4MCPzOxJK1y3FqDN3Y8F97uBtjLUNe49vPofW7n317ip9tFC+tz9PoUW3rhOM3vazH5qZuW4eOpk791C2l+vB3rcfc+EaSXdZ2flw7x+xhZzuC84ZlYH3A98xN2HgC8D64DNwDEKXwlL7XXufjnwVuCDZvaqKyx74XtgWcbDWuEyjNcD/zeYtBD21y8p5z6aipl9CsgC9wSTjgFr3H0L8DHg62bWUMKSFuR7d5b38uqGREn32ST5cMZ8fMYWc7hP60LcpWJmcQpv3D3u/m0Ad+9x95y754GvMo9fR6fi7keC2+PAA0ENPeNf84Lb46WuK/BW4Cl37wlqLPv+mmCqfVT2z52Z/S7wduB9QSgQdHv0B/efpNC3fVGpajrHe1f2/QVgZjHgN4Fvjk8r5T6bLB+Y58/YYg73BXMh7qAv72vAC+7++QnTJ/aTvRPYdfa681xXrZnVj9+n8GPcLgr76dZgsVuB75ayrgle1ZIq9/46y1T76HvA7wQjGrYDJyd8tZ53ZnYt8CfA9e5+esL0VjOLBvcvANYD+0tY11Tv3feA95hZwsw6g7r+o1R1TfBmYLe7d41PKNU+myofmO/P2Hz/UjyffxR+VX6Jwv+4nypjHa+j8JXqF8DO4O864J+AZ4Pp3wOWl7iuCyiMVHgGeG58HwHNwEPAHuAnwNIy7LNaoB9onDCtLPuLwn8wx4AMhf7N90+1jyiMYPib4DP3LLCtxHXtpdAfO/45+0qw7E3Be7wTeAp4R4nrmvK9Az4V7K8XgbeW+r0Mpv8D8AdnLVuSfXaOfJjXz5hOPyAiEkKLuVtGRESmoHAXEQkhhbuISAgp3EVEQkjhLiISQgp3EZEQUriLiITQ/we7QDxReduQ2QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 2s 108ms/step - loss: 0.3619 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.5014 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.4773 - accuracy: 1.0000\n",
            "(None, 19, 19, 19, 1)\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 19, 19, 19,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_281 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_283 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_285 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_287 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_289 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_291 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_293 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_295 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_297 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_299 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_301 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_303 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_305 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_307 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_309 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_311 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_313 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_315 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_317 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_319 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_321 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_323 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_325 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_327 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_329 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_331 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_333 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_335 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_337 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_339 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_341 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_343 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_345 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_347 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_349 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_351 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_353 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_355 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_357 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_359 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_361 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_363 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_365 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_367 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_369 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_371 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_373 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_375 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_377 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_379 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_381 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_383 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_385 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_387 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_389 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_391 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_393 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_395 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_397 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_399 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_401 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_403 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_405 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_407 (Lambda)             (None, 19, 5, 19, 1) 0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_280 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_282 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_283[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_284 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_285[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_286 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_287[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_288 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_289[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_290 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_291[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_292 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_293[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_294 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_295[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_296 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_297[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_298 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_299[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_300 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_301[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_302 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_303[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_304 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_305[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_306 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_307[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_308 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_309[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_310 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_311[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_312 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_313[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_314 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_315[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_316 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_317[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_318 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_319[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_320 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_321[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_322 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_323[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_324 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_325[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_326 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_327[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_328 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_329[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_330 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_331[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_332 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_333[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_334 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_335[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_336 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_337[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_338 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_339[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_340 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_341[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_342 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_343[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_344 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_345[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_346 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_347[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_348 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_349[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_350 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_351[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_352 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_353[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_354 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_355[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_356 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_357[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_358 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_359[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_360 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_361[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_362 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_363[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_364 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_365[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_366 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_368 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_369[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_370 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_372 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_374 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_376 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_377[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_378 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_379[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_380 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_381[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_382 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_383[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_384 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_385[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_386 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_387[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_388 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_389[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_390 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_391[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_392 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_393[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_394 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_395[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_396 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_397[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_398 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_399[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_400 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_401[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_402 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_403[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_404 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_405[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_406 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_407[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_140 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_141 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_282[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_142 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_284[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_143 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_286[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_144 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_288[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_145 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_290[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_146 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_292[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_147 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_294[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_148 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_296[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_149 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_298[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_150 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_300[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_151 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_302[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_152 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_304[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_153 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_306[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_154 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_308[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_155 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_310[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_156 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_312[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_157 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_314[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_158 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_316[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_159 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_318[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_160 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_320[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_161 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_322[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_162 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_324[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_163 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_326[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_164 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_328[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_165 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_330[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_166 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_332[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_167 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_334[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_168 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_336[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_169 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_338[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_170 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_340[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_171 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_342[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_172 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_344[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_173 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_346[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_174 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_348[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_175 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_350[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_176 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_352[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_177 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_354[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_178 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_356[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_179 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_358[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_180 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_360[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_181 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_362[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_182 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_364[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_183 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_366[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_184 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_185 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_186 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_187 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_188 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_189 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_378[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_190 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_380[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_191 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_382[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_192 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_384[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_193 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_386[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_194 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_388[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_195 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_390[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_196 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_392[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_197 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_394[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_198 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_396[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_199 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_398[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_200 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_400[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_201 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_402[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_202 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_404[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_203 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_406[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_140 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_141 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_142 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_143 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_144 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_145 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_146 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_147 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_148 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_149 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_150 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_151 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_152 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_153 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_154 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_155 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_156 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_157 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_158 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_159 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_160 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_161 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_162 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_163 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_164 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_165 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_166 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_167 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_168 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_169 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_170 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_171 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_172 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_173 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_174 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_175 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_176 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_177 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_178 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_179 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_180 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_181 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_182 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_183 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_184 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_185 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_186 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_187 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_188 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_189 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_190 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_191 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_192 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_193 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_194 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_195 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_196 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_197 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_198 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_199 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_200 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_201 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_202 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_203 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_140 (G (None, 8)            0           dropout_140[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_141 (G (None, 8)            0           dropout_141[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_142 (G (None, 8)            0           dropout_142[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_143 (G (None, 8)            0           dropout_143[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_144 (G (None, 8)            0           dropout_144[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_145 (G (None, 8)            0           dropout_145[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_146 (G (None, 8)            0           dropout_146[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_147 (G (None, 8)            0           dropout_147[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_148 (G (None, 8)            0           dropout_148[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_149 (G (None, 8)            0           dropout_149[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_150 (G (None, 8)            0           dropout_150[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_151 (G (None, 8)            0           dropout_151[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_152 (G (None, 8)            0           dropout_152[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_153 (G (None, 8)            0           dropout_153[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_154 (G (None, 8)            0           dropout_154[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_155 (G (None, 8)            0           dropout_155[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_156 (G (None, 8)            0           dropout_156[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_157 (G (None, 8)            0           dropout_157[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_158 (G (None, 8)            0           dropout_158[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_159 (G (None, 8)            0           dropout_159[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_160 (G (None, 8)            0           dropout_160[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_161 (G (None, 8)            0           dropout_161[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_162 (G (None, 8)            0           dropout_162[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_163 (G (None, 8)            0           dropout_163[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_164 (G (None, 8)            0           dropout_164[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_165 (G (None, 8)            0           dropout_165[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_166 (G (None, 8)            0           dropout_166[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_167 (G (None, 8)            0           dropout_167[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_168 (G (None, 8)            0           dropout_168[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_169 (G (None, 8)            0           dropout_169[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_170 (G (None, 8)            0           dropout_170[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_171 (G (None, 8)            0           dropout_171[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_172 (G (None, 8)            0           dropout_172[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_173 (G (None, 8)            0           dropout_173[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_174 (G (None, 8)            0           dropout_174[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_175 (G (None, 8)            0           dropout_175[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_176 (G (None, 8)            0           dropout_176[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_177 (G (None, 8)            0           dropout_177[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_178 (G (None, 8)            0           dropout_178[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_179 (G (None, 8)            0           dropout_179[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_180 (G (None, 8)            0           dropout_180[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_181 (G (None, 8)            0           dropout_181[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_182 (G (None, 8)            0           dropout_182[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_183 (G (None, 8)            0           dropout_183[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_184 (G (None, 8)            0           dropout_184[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_185 (G (None, 8)            0           dropout_185[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_186 (G (None, 8)            0           dropout_186[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_187 (G (None, 8)            0           dropout_187[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_188 (G (None, 8)            0           dropout_188[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_189 (G (None, 8)            0           dropout_189[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_190 (G (None, 8)            0           dropout_190[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_191 (G (None, 8)            0           dropout_191[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_192 (G (None, 8)            0           dropout_192[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_193 (G (None, 8)            0           dropout_193[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_194 (G (None, 8)            0           dropout_194[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_195 (G (None, 8)            0           dropout_195[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_196 (G (None, 8)            0           dropout_196[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_197 (G (None, 8)            0           dropout_197[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_198 (G (None, 8)            0           dropout_198[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_199 (G (None, 8)            0           dropout_199[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_200 (G (None, 8)            0           dropout_200[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_201 (G (None, 8)            0           dropout_201[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_202 (G (None, 8)            0           dropout_202[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_203 (G (None, 8)            0           dropout_203[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 512)          0           global_average_pooling3d_140[0][0\n",
            "                                                                 global_average_pooling3d_141[0][0\n",
            "                                                                 global_average_pooling3d_142[0][0\n",
            "                                                                 global_average_pooling3d_143[0][0\n",
            "                                                                 global_average_pooling3d_144[0][0\n",
            "                                                                 global_average_pooling3d_145[0][0\n",
            "                                                                 global_average_pooling3d_146[0][0\n",
            "                                                                 global_average_pooling3d_147[0][0\n",
            "                                                                 global_average_pooling3d_148[0][0\n",
            "                                                                 global_average_pooling3d_149[0][0\n",
            "                                                                 global_average_pooling3d_150[0][0\n",
            "                                                                 global_average_pooling3d_151[0][0\n",
            "                                                                 global_average_pooling3d_152[0][0\n",
            "                                                                 global_average_pooling3d_153[0][0\n",
            "                                                                 global_average_pooling3d_154[0][0\n",
            "                                                                 global_average_pooling3d_155[0][0\n",
            "                                                                 global_average_pooling3d_156[0][0\n",
            "                                                                 global_average_pooling3d_157[0][0\n",
            "                                                                 global_average_pooling3d_158[0][0\n",
            "                                                                 global_average_pooling3d_159[0][0\n",
            "                                                                 global_average_pooling3d_160[0][0\n",
            "                                                                 global_average_pooling3d_161[0][0\n",
            "                                                                 global_average_pooling3d_162[0][0\n",
            "                                                                 global_average_pooling3d_163[0][0\n",
            "                                                                 global_average_pooling3d_164[0][0\n",
            "                                                                 global_average_pooling3d_165[0][0\n",
            "                                                                 global_average_pooling3d_166[0][0\n",
            "                                                                 global_average_pooling3d_167[0][0\n",
            "                                                                 global_average_pooling3d_168[0][0\n",
            "                                                                 global_average_pooling3d_169[0][0\n",
            "                                                                 global_average_pooling3d_170[0][0\n",
            "                                                                 global_average_pooling3d_171[0][0\n",
            "                                                                 global_average_pooling3d_172[0][0\n",
            "                                                                 global_average_pooling3d_173[0][0\n",
            "                                                                 global_average_pooling3d_174[0][0\n",
            "                                                                 global_average_pooling3d_175[0][0\n",
            "                                                                 global_average_pooling3d_176[0][0\n",
            "                                                                 global_average_pooling3d_177[0][0\n",
            "                                                                 global_average_pooling3d_178[0][0\n",
            "                                                                 global_average_pooling3d_179[0][0\n",
            "                                                                 global_average_pooling3d_180[0][0\n",
            "                                                                 global_average_pooling3d_181[0][0\n",
            "                                                                 global_average_pooling3d_182[0][0\n",
            "                                                                 global_average_pooling3d_183[0][0\n",
            "                                                                 global_average_pooling3d_184[0][0\n",
            "                                                                 global_average_pooling3d_185[0][0\n",
            "                                                                 global_average_pooling3d_186[0][0\n",
            "                                                                 global_average_pooling3d_187[0][0\n",
            "                                                                 global_average_pooling3d_188[0][0\n",
            "                                                                 global_average_pooling3d_189[0][0\n",
            "                                                                 global_average_pooling3d_190[0][0\n",
            "                                                                 global_average_pooling3d_191[0][0\n",
            "                                                                 global_average_pooling3d_192[0][0\n",
            "                                                                 global_average_pooling3d_193[0][0\n",
            "                                                                 global_average_pooling3d_194[0][0\n",
            "                                                                 global_average_pooling3d_195[0][0\n",
            "                                                                 global_average_pooling3d_196[0][0\n",
            "                                                                 global_average_pooling3d_197[0][0\n",
            "                                                                 global_average_pooling3d_198[0][0\n",
            "                                                                 global_average_pooling3d_199[0][0\n",
            "                                                                 global_average_pooling3d_200[0][0\n",
            "                                                                 global_average_pooling3d_201[0][0\n",
            "                                                                 global_average_pooling3d_202[0][0\n",
            "                                                                 global_average_pooling3d_203[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 512)          262656      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 512)          262656      dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 512)          262656      dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1)            513         dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 802,817\n",
            "Trainable params: 802,817\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 12s 960ms/step - loss: 99.3805 - accuracy: 0.4390 - val_loss: 93.4672 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.46722, saving model to ./mod2.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 91.7855 - accuracy: 0.5610 - val_loss: 86.1450 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.46722 to 86.14498, saving model to ./mod2.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 84.5469 - accuracy: 0.5122 - val_loss: 79.2479 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.14498 to 79.24789, saving model to ./mod2.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 77.6482 - accuracy: 0.5366 - val_loss: 72.6135 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.24789 to 72.61346, saving model to ./mod2.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 71.0627 - accuracy: 0.5366 - val_loss: 66.2062 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.61346 to 66.20615, saving model to ./mod2.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 64.7874 - accuracy: 0.6341 - val_loss: 60.1190 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.20615 to 60.11897, saving model to ./mod2.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 58.8139 - accuracy: 0.5976 - val_loss: 54.4033 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.11897 to 54.40333, saving model to ./mod2.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 53.1303 - accuracy: 0.6585 - val_loss: 48.9428 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.40333 to 48.94276, saving model to ./mod2.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 47.7496 - accuracy: 0.7683 - val_loss: 43.8839 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.94276 to 43.88391, saving model to ./mod2.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 42.6848 - accuracy: 0.5976 - val_loss: 38.9835 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.88391 to 38.98355, saving model to ./mod2.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 37.9037 - accuracy: 0.6707 - val_loss: 34.3731 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.98355 to 34.37307, saving model to ./mod2.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 33.4228 - accuracy: 0.6707 - val_loss: 30.1943 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.37307 to 30.19432, saving model to ./mod2.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 29.2183 - accuracy: 0.7561 - val_loss: 26.1551 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.19432 to 26.15510, saving model to ./mod2.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 25.3260 - accuracy: 0.6829 - val_loss: 22.5665 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.15510 to 22.56648, saving model to ./mod2.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 21.7519 - accuracy: 0.6707 - val_loss: 19.2518 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.56648 to 19.25180, saving model to ./mod2.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 18.4586 - accuracy: 0.7561 - val_loss: 16.0534 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.25180 to 16.05339, saving model to ./mod2.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 15.4840 - accuracy: 0.6585 - val_loss: 13.5990 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.05339 to 13.59899, saving model to ./mod2.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 12.8484 - accuracy: 0.6463 - val_loss: 10.9290 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.59899 to 10.92901, saving model to ./mod2.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 10.5116 - accuracy: 0.6707 - val_loss: 8.8587 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00019: val_loss improved from 10.92901 to 8.85865, saving model to ./mod2.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 8.4096 - accuracy: 0.7439 - val_loss: 7.2549 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.85865 to 7.25486, saving model to ./mod2.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 6.6846 - accuracy: 0.7073 - val_loss: 5.4664 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.25486 to 5.46640, saving model to ./mod2.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 5.2190 - accuracy: 0.7195 - val_loss: 4.4113 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.46640 to 4.41126, saving model to ./mod2.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 4.0897 - accuracy: 0.6341 - val_loss: 3.3876 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.41126 to 3.38761, saving model to ./mod2.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 3.1979 - accuracy: 0.7927 - val_loss: 2.7064 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.38761 to 2.70637, saving model to ./mod2.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 2.6512 - accuracy: 0.7073 - val_loss: 2.6414 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.70637 to 2.64136, saving model to ./mod2.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 2.4206 - accuracy: 0.6341 - val_loss: 2.3057 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.64136 to 2.30570, saving model to ./mod2.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 2.3122 - accuracy: 0.7073 - val_loss: 2.0987 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.30570 to 2.09873, saving model to ./mod2.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 2.0447 - accuracy: 0.7561 - val_loss: 2.0370 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.09873 to 2.03698, saving model to ./mod2.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.7703 - accuracy: 0.7927 - val_loss: 1.5635 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00029: val_loss improved from 2.03698 to 1.56347, saving model to ./mod2.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 1.5543 - accuracy: 0.7439 - val_loss: 1.5293 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.56347 to 1.52929, saving model to ./mod2.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.3968 - accuracy: 0.7683 - val_loss: 1.4861 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.52929 to 1.48608, saving model to ./mod2.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 1.3258 - accuracy: 0.7805 - val_loss: 1.2823 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.48608 to 1.28233, saving model to ./mod2.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 1.2024 - accuracy: 0.8659 - val_loss: 1.2821 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.28233 to 1.28206, saving model to ./mod2.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.1059 - accuracy: 0.7927 - val_loss: 1.1478 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.28206 to 1.14775, saving model to ./mod2.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 1.0477 - accuracy: 0.8049 - val_loss: 1.0987 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.14775 to 1.09872, saving model to ./mod2.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.0098 - accuracy: 0.8415 - val_loss: 1.1098 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.09872\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.9462 - accuracy: 0.8415 - val_loss: 1.0058 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.09872 to 1.00579, saving model to ./mod2.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.9039 - accuracy: 0.8659 - val_loss: 0.9766 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.00579 to 0.97659, saving model to ./mod2.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.8877 - accuracy: 0.8537 - val_loss: 1.2712 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.97659\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.8831 - accuracy: 0.8293 - val_loss: 0.9253 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.97659 to 0.92533, saving model to ./mod2.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.8382 - accuracy: 0.8537 - val_loss: 1.2667 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.92533\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.8502 - accuracy: 0.8293 - val_loss: 0.9046 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.92533 to 0.90465, saving model to ./mod2.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.8578 - accuracy: 0.7683 - val_loss: 1.4816 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.90465\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.9172 - accuracy: 0.7439 - val_loss: 0.8952 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.90465 to 0.89523, saving model to ./mod2.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.9331 - accuracy: 0.7317 - val_loss: 0.9203 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.89523\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.8057 - accuracy: 0.9146 - val_loss: 1.1290 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.89523\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.7790 - accuracy: 0.8780 - val_loss: 0.8767 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.89523 to 0.87671, saving model to ./mod2.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.7548 - accuracy: 0.9146 - val_loss: 0.9758 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.87671\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.7239 - accuracy: 0.8902 - val_loss: 0.8806 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.87671\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.7255 - accuracy: 0.9024 - val_loss: 1.0060 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.87671\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.7537 - accuracy: 0.8780 - val_loss: 0.8907 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.87671\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6889 - accuracy: 0.9146 - val_loss: 0.9136 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.87671\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.6986 - accuracy: 0.9146 - val_loss: 0.9094 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.87671\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.6467 - accuracy: 0.9512 - val_loss: 0.8978 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.87671\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.6485 - accuracy: 0.9390 - val_loss: 1.0999 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.87671\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6762 - accuracy: 0.9024 - val_loss: 0.8853 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.87671\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.7500 - accuracy: 0.8537 - val_loss: 1.0145 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.87671\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6418 - accuracy: 0.9390 - val_loss: 0.8937 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.87671\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.6178 - accuracy: 0.9390 - val_loss: 0.9463 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.87671\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6045 - accuracy: 0.9634 - val_loss: 0.9404 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.87671\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5937 - accuracy: 0.9390 - val_loss: 0.8894 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.87671\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6419 - accuracy: 0.9146 - val_loss: 1.1264 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.87671\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6601 - accuracy: 0.9390 - val_loss: 0.8920 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.87671\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6485 - accuracy: 0.9146 - val_loss: 1.0364 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.87671\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6463 - accuracy: 0.9024 - val_loss: 0.8883 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.87671\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.6009 - accuracy: 0.9390 - val_loss: 0.8760 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.87671 to 0.87599, saving model to ./mod2.h5\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5750 - accuracy: 0.9634 - val_loss: 0.9076 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.87599\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5454 - accuracy: 0.9756 - val_loss: 0.8787 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.87599\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.5811 - accuracy: 0.9512 - val_loss: 1.1156 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.87599\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5826 - accuracy: 0.9268 - val_loss: 0.8666 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.87599 to 0.86663, saving model to ./mod2.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.5507 - accuracy: 0.9390 - val_loss: 1.0024 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.86663\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.5410 - accuracy: 0.9756 - val_loss: 0.8788 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.86663\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5004 - accuracy: 0.9878 - val_loss: 0.9013 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.86663\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.5161 - accuracy: 0.9878 - val_loss: 0.9583 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.86663\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.5383 - accuracy: 0.9512 - val_loss: 0.8611 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.86663 to 0.86108, saving model to ./mod2.h5\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.5267 - accuracy: 0.9634 - val_loss: 0.9221 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.86108\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.5232 - accuracy: 0.9512 - val_loss: 0.9714 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.86108\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.5074 - accuracy: 0.9878 - val_loss: 0.8649 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.86108\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.4848 - accuracy: 0.9878 - val_loss: 0.9332 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.86108\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4880 - accuracy: 1.0000 - val_loss: 0.8613 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.86108\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4841 - accuracy: 0.9756 - val_loss: 0.8505 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.86108 to 0.85053, saving model to ./mod2.h5\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4774 - accuracy: 0.9878 - val_loss: 0.9792 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.85053\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4888 - accuracy: 0.9756 - val_loss: 0.8793 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.85053\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.5343 - accuracy: 0.9512 - val_loss: 0.9240 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.85053\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5206 - accuracy: 0.9756 - val_loss: 0.8547 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.85053\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.4940 - accuracy: 0.9756 - val_loss: 0.9206 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.85053\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.5055 - accuracy: 0.9878 - val_loss: 0.8382 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.85053 to 0.83823, saving model to ./mod2.h5\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.4966 - accuracy: 0.9756 - val_loss: 0.8805 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.83823\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4661 - accuracy: 0.9878 - val_loss: 1.3004 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.83823\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5158 - accuracy: 0.9756 - val_loss: 0.8886 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.83823\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.5727 - accuracy: 0.9268 - val_loss: 1.0126 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.83823\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.6856 - accuracy: 0.8659 - val_loss: 0.8253 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.83823 to 0.82535, saving model to ./mod2.h5\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6000 - accuracy: 0.8780 - val_loss: 0.8349 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.82535\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4856 - accuracy: 0.9756 - val_loss: 1.5277 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.82535\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.6186 - accuracy: 0.9146 - val_loss: 0.8493 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.82535\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.5927 - accuracy: 0.8902 - val_loss: 0.8182 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.82535 to 0.81817, saving model to ./mod2.h5\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5080 - accuracy: 0.9756 - val_loss: 1.0250 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.81817\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.4780 - accuracy: 0.9878 - val_loss: 0.8228 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.81817\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5041 - accuracy: 0.9634 - val_loss: 0.7968 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.81817 to 0.79679, saving model to ./mod2.h5\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4519 - accuracy: 1.0000 - val_loss: 0.8874 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.79679\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4508 - accuracy: 1.0000 - val_loss: 0.7985 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.79679\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4442 - accuracy: 1.0000 - val_loss: 0.8346 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.79679\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4551 - accuracy: 0.9878 - val_loss: 0.8030 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.79679\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4404 - accuracy: 1.0000 - val_loss: 0.8156 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.79679\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.4325 - accuracy: 1.0000 - val_loss: 0.7787 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.79679 to 0.77872, saving model to ./mod2.h5\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4260 - accuracy: 1.0000 - val_loss: 0.7799 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.77872\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4256 - accuracy: 1.0000 - val_loss: 0.7774 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.77872 to 0.77735, saving model to ./mod2.h5\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.4239 - accuracy: 1.0000 - val_loss: 0.7824 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.77735\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4221 - accuracy: 1.0000 - val_loss: 0.7685 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.77735 to 0.76850, saving model to ./mod2.h5\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4196 - accuracy: 1.0000 - val_loss: 0.7774 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.76850\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4164 - accuracy: 1.0000 - val_loss: 0.7642 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.76850 to 0.76416, saving model to ./mod2.h5\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4108 - accuracy: 1.0000 - val_loss: 0.7689 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.76416\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.4171 - accuracy: 1.0000 - val_loss: 0.7622 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.76416 to 0.76217, saving model to ./mod2.h5\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4185 - accuracy: 1.0000 - val_loss: 0.7616 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.76217 to 0.76161, saving model to ./mod2.h5\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4134 - accuracy: 1.0000 - val_loss: 0.7917 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.76161\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4040 - accuracy: 1.0000 - val_loss: 0.7584 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.76161 to 0.75844, saving model to ./mod2.h5\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.4105 - accuracy: 1.0000 - val_loss: 0.7522 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.75844 to 0.75225, saving model to ./mod2.h5\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4080 - accuracy: 1.0000 - val_loss: 0.7707 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.75225\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4112 - accuracy: 1.0000 - val_loss: 0.7472 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.75225 to 0.74721, saving model to ./mod2.h5\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4039 - accuracy: 1.0000 - val_loss: 0.7481 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.74721\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.4100 - accuracy: 1.0000 - val_loss: 0.7505 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.74721\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4059 - accuracy: 1.0000 - val_loss: 0.7492 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.74721\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4044 - accuracy: 1.0000 - val_loss: 0.7489 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.74721\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3993 - accuracy: 1.0000 - val_loss: 0.7603 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.74721\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3993 - accuracy: 1.0000 - val_loss: 0.7362 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.74721 to 0.73617, saving model to ./mod2.h5\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3970 - accuracy: 1.0000 - val_loss: 0.7383 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.73617\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3945 - accuracy: 1.0000 - val_loss: 0.7349 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.73617 to 0.73488, saving model to ./mod2.h5\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3971 - accuracy: 1.0000 - val_loss: 0.7317 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.73488 to 0.73172, saving model to ./mod2.h5\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3965 - accuracy: 1.0000 - val_loss: 0.7468 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.73172\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.3966 - accuracy: 1.0000 - val_loss: 0.7617 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.73172\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3984 - accuracy: 1.0000 - val_loss: 0.7320 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.73172\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4017 - accuracy: 1.0000 - val_loss: 0.7356 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.73172\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3935 - accuracy: 1.0000 - val_loss: 0.7403 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.73172\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.3925 - accuracy: 1.0000 - val_loss: 0.7281 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.73172 to 0.72813, saving model to ./mod2.h5\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3898 - accuracy: 1.0000 - val_loss: 0.7287 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.72813\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3927 - accuracy: 1.0000 - val_loss: 0.7298 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.72813\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.3921 - accuracy: 1.0000 - val_loss: 0.7359 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.72813\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3866 - accuracy: 1.0000 - val_loss: 0.7345 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.72813\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3991 - accuracy: 1.0000 - val_loss: 0.7480 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.72813\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3869 - accuracy: 1.0000 - val_loss: 0.7457 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.72813\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3887 - accuracy: 1.0000 - val_loss: 0.7270 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.72813 to 0.72703, saving model to ./mod2.h5\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3899 - accuracy: 1.0000 - val_loss: 0.7342 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.72703\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.3869 - accuracy: 1.0000 - val_loss: 0.7262 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.72703 to 0.72620, saving model to ./mod2.h5\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3873 - accuracy: 1.0000 - val_loss: 0.7228 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.72620 to 0.72281, saving model to ./mod2.h5\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3887 - accuracy: 1.0000 - val_loss: 0.7239 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.72281\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3833 - accuracy: 1.0000 - val_loss: 0.7256 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.72281\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3846 - accuracy: 1.0000 - val_loss: 0.7177 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.72281 to 0.71769, saving model to ./mod2.h5\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3815 - accuracy: 1.0000 - val_loss: 0.7201 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.71769\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3810 - accuracy: 1.0000 - val_loss: 0.7183 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.71769\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3803 - accuracy: 1.0000 - val_loss: 0.7178 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.71769\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3814 - accuracy: 1.0000 - val_loss: 0.7153 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00151: val_loss improved from 0.71769 to 0.71531, saving model to ./mod2.h5\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3821 - accuracy: 1.0000 - val_loss: 0.7149 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.71531 to 0.71487, saving model to ./mod2.h5\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3813 - accuracy: 1.0000 - val_loss: 0.7196 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.71487\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3794 - accuracy: 1.0000 - val_loss: 0.7100 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.71487 to 0.71002, saving model to ./mod2.h5\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3847 - accuracy: 1.0000 - val_loss: 0.7155 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.71002\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3870 - accuracy: 1.0000 - val_loss: 0.7411 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.71002\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3852 - accuracy: 1.0000 - val_loss: 0.7349 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.71002\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3887 - accuracy: 1.0000 - val_loss: 0.7309 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.71002\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3845 - accuracy: 1.0000 - val_loss: 0.7110 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.71002\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3808 - accuracy: 1.0000 - val_loss: 0.7183 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.71002\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3844 - accuracy: 1.0000 - val_loss: 0.8110 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.71002\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3928 - accuracy: 1.0000 - val_loss: 0.7105 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.71002\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3793 - accuracy: 1.0000 - val_loss: 0.7028 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00163: val_loss improved from 0.71002 to 0.70278, saving model to ./mod2.h5\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.3760 - accuracy: 1.0000 - val_loss: 0.7101 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.70278\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3818 - accuracy: 1.0000 - val_loss: 0.7094 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.70278\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3775 - accuracy: 1.0000 - val_loss: 0.7017 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.70278 to 0.70167, saving model to ./mod2.h5\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3760 - accuracy: 1.0000 - val_loss: 0.7006 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.70167 to 0.70062, saving model to ./mod2.h5\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3740 - accuracy: 1.0000 - val_loss: 0.7259 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.70062\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3778 - accuracy: 1.0000 - val_loss: 0.6978 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.70062 to 0.69783, saving model to ./mod2.h5\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3801 - accuracy: 1.0000 - val_loss: 0.7181 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.69783\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3770 - accuracy: 1.0000 - val_loss: 0.7170 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.69783\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3735 - accuracy: 1.0000 - val_loss: 0.7193 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.69783\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.3696 - accuracy: 1.0000 - val_loss: 0.6930 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.69783 to 0.69303, saving model to ./mod2.h5\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3729 - accuracy: 1.0000 - val_loss: 0.7021 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.69303\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3741 - accuracy: 1.0000 - val_loss: 0.6921 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.69303 to 0.69205, saving model to ./mod2.h5\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3731 - accuracy: 1.0000 - val_loss: 0.6891 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.69205 to 0.68912, saving model to ./mod2.h5\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.3710 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00177: val_loss improved from 0.68912 to 0.68760, saving model to ./mod2.h5\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.3734 - accuracy: 1.0000 - val_loss: 0.6843 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.68760 to 0.68426, saving model to ./mod2.h5\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3737 - accuracy: 1.0000 - val_loss: 0.6807 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00179: val_loss improved from 0.68426 to 0.68072, saving model to ./mod2.h5\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3655 - accuracy: 1.0000 - val_loss: 0.6952 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.68072\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3728 - accuracy: 1.0000 - val_loss: 0.6990 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.68072\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.3709 - accuracy: 1.0000 - val_loss: 0.6839 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.68072\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3726 - accuracy: 1.0000 - val_loss: 0.6845 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.68072\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3695 - accuracy: 1.0000 - val_loss: 0.7039 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.68072\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3690 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.68072\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3680 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.68072\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3702 - accuracy: 1.0000 - val_loss: 0.6803 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.68072 to 0.68033, saving model to ./mod2.h5\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3659 - accuracy: 1.0000 - val_loss: 0.6788 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00188: val_loss improved from 0.68033 to 0.67884, saving model to ./mod2.h5\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3673 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.67884 to 0.67573, saving model to ./mod2.h5\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.3655 - accuracy: 1.0000 - val_loss: 0.6821 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.67573\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3661 - accuracy: 1.0000 - val_loss: 0.6759 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.67573\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3652 - accuracy: 1.0000 - val_loss: 0.6828 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.67573\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3644 - accuracy: 1.0000 - val_loss: 0.6768 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.67573\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3657 - accuracy: 1.0000 - val_loss: 0.6761 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.67573\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3646 - accuracy: 1.0000 - val_loss: 0.6729 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00195: val_loss improved from 0.67573 to 0.67291, saving model to ./mod2.h5\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3617 - accuracy: 1.0000 - val_loss: 0.6767 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.67291\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3636 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.67291\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3632 - accuracy: 1.0000 - val_loss: 0.6759 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.67291\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3618 - accuracy: 1.0000 - val_loss: 0.6761 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.67291\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3612 - accuracy: 1.0000 - val_loss: 0.6764 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.67291\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 116ms/step - loss: 0.3594 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.4437 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.6729 - accuracy: 0.8571\n",
            "(None, 19, 19, 19, 1)\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 19, 19, 19,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_409 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_411 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_413 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_415 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_417 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_419 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_421 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_423 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_425 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_427 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_429 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_431 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_433 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_435 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_437 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_439 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_441 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_443 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_445 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_447 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_449 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_451 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_453 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_455 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_457 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_459 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_461 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_463 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_465 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_467 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_469 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_471 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_473 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_475 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_477 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_479 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_481 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_483 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_485 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_487 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_489 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_491 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_493 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_495 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_497 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_499 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_501 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_503 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_505 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_507 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_509 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_511 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_513 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_515 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_517 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_519 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_521 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_523 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_525 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_527 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_529 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_531 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_533 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_535 (Lambda)             (None, 19, 5, 19, 1) 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_408 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_409[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_410 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_411[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_412 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_413[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_414 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_415[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_416 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_417[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_418 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_419[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_420 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_421[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_422 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_423[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_424 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_425[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_426 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_427[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_428 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_429[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_430 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_431[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_432 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_433[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_434 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_435[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_436 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_437[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_438 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_439[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_440 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_441[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_442 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_443[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_444 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_445[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_446 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_447[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_448 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_449[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_450 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_451[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_452 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_453[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_454 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_455[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_456 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_457[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_458 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_459[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_460 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_461[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_462 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_463[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_464 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_465[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_466 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_467[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_468 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_469[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_470 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_471[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_472 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_473[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_474 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_475[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_476 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_477[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_478 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_479[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_480 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_481[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_482 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_483[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_484 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_485[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_486 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_487[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_488 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_489[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_490 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_491[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_492 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_493[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_494 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_495[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_496 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_497[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_498 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_499[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_500 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_501[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_502 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_503[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_504 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_505[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_506 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_507[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_508 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_509[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_510 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_511[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_512 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_513[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_514 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_515[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_516 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_517[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_518 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_519[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_520 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_521[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_522 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_523[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_524 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_525[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_526 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_527[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_528 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_529[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_530 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_531[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_532 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_533[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_534 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_535[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_204 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_408[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_205 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_410[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_206 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_412[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_207 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_414[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_208 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_416[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_209 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_418[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_210 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_420[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_211 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_422[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_212 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_424[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_213 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_426[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_214 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_428[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_215 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_430[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_216 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_432[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_217 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_434[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_218 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_436[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_219 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_438[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_220 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_440[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_221 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_442[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_222 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_444[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_223 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_446[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_224 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_448[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_225 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_450[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_226 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_452[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_227 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_454[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_228 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_456[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_229 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_458[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_230 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_460[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_231 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_462[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_232 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_464[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_233 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_466[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_234 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_468[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_235 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_470[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_236 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_472[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_237 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_474[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_238 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_476[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_239 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_478[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_240 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_480[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_241 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_482[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_242 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_484[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_243 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_486[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_244 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_488[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_245 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_490[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_246 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_492[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_247 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_494[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_248 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_496[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_249 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_498[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_250 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_500[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_251 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_502[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_252 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_504[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_253 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_506[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_254 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_508[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_255 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_510[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_256 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_512[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_257 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_514[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_258 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_516[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_259 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_518[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_260 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_520[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_261 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_522[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_262 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_524[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_263 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_526[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_264 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_528[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_265 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_530[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_266 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_532[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_267 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_534[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_204 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_205 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_206 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_207 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_208 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_209 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_210 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_211 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_212 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_213 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_214 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_215 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_216 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_217 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_218 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_219 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_220 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_221 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_222 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_223 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_224 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_225 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_226 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_227 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_228 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_229 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_230 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_231 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_232 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_233 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_234 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_235 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_236 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_237 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_238 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_239 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_240 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_241 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_242 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_243 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_244 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_245 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_246 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_247 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_248 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_249 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_250 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_251 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_252 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_253 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_254 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_255 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_256 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_257 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_258 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_259 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_260 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_261 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_262 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_263 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_264 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_265 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_266 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_267 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_204 (G (None, 8)            0           dropout_204[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_205 (G (None, 8)            0           dropout_205[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_206 (G (None, 8)            0           dropout_206[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_207 (G (None, 8)            0           dropout_207[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_208 (G (None, 8)            0           dropout_208[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_209 (G (None, 8)            0           dropout_209[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_210 (G (None, 8)            0           dropout_210[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_211 (G (None, 8)            0           dropout_211[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_212 (G (None, 8)            0           dropout_212[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_213 (G (None, 8)            0           dropout_213[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_214 (G (None, 8)            0           dropout_214[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_215 (G (None, 8)            0           dropout_215[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_216 (G (None, 8)            0           dropout_216[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_217 (G (None, 8)            0           dropout_217[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_218 (G (None, 8)            0           dropout_218[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_219 (G (None, 8)            0           dropout_219[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_220 (G (None, 8)            0           dropout_220[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_221 (G (None, 8)            0           dropout_221[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_222 (G (None, 8)            0           dropout_222[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_223 (G (None, 8)            0           dropout_223[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_224 (G (None, 8)            0           dropout_224[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_225 (G (None, 8)            0           dropout_225[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_226 (G (None, 8)            0           dropout_226[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_227 (G (None, 8)            0           dropout_227[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_228 (G (None, 8)            0           dropout_228[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_229 (G (None, 8)            0           dropout_229[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_230 (G (None, 8)            0           dropout_230[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_231 (G (None, 8)            0           dropout_231[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_232 (G (None, 8)            0           dropout_232[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_233 (G (None, 8)            0           dropout_233[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_234 (G (None, 8)            0           dropout_234[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_235 (G (None, 8)            0           dropout_235[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_236 (G (None, 8)            0           dropout_236[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_237 (G (None, 8)            0           dropout_237[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_238 (G (None, 8)            0           dropout_238[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_239 (G (None, 8)            0           dropout_239[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_240 (G (None, 8)            0           dropout_240[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_241 (G (None, 8)            0           dropout_241[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_242 (G (None, 8)            0           dropout_242[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_243 (G (None, 8)            0           dropout_243[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_244 (G (None, 8)            0           dropout_244[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_245 (G (None, 8)            0           dropout_245[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_246 (G (None, 8)            0           dropout_246[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_247 (G (None, 8)            0           dropout_247[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_248 (G (None, 8)            0           dropout_248[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_249 (G (None, 8)            0           dropout_249[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_250 (G (None, 8)            0           dropout_250[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_251 (G (None, 8)            0           dropout_251[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_252 (G (None, 8)            0           dropout_252[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_253 (G (None, 8)            0           dropout_253[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_254 (G (None, 8)            0           dropout_254[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_255 (G (None, 8)            0           dropout_255[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_256 (G (None, 8)            0           dropout_256[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_257 (G (None, 8)            0           dropout_257[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_258 (G (None, 8)            0           dropout_258[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_259 (G (None, 8)            0           dropout_259[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_260 (G (None, 8)            0           dropout_260[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_261 (G (None, 8)            0           dropout_261[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_262 (G (None, 8)            0           dropout_262[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_263 (G (None, 8)            0           dropout_263[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_264 (G (None, 8)            0           dropout_264[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_265 (G (None, 8)            0           dropout_265[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_266 (G (None, 8)            0           dropout_266[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_267 (G (None, 8)            0           dropout_267[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 512)          0           global_average_pooling3d_204[0][0\n",
            "                                                                 global_average_pooling3d_205[0][0\n",
            "                                                                 global_average_pooling3d_206[0][0\n",
            "                                                                 global_average_pooling3d_207[0][0\n",
            "                                                                 global_average_pooling3d_208[0][0\n",
            "                                                                 global_average_pooling3d_209[0][0\n",
            "                                                                 global_average_pooling3d_210[0][0\n",
            "                                                                 global_average_pooling3d_211[0][0\n",
            "                                                                 global_average_pooling3d_212[0][0\n",
            "                                                                 global_average_pooling3d_213[0][0\n",
            "                                                                 global_average_pooling3d_214[0][0\n",
            "                                                                 global_average_pooling3d_215[0][0\n",
            "                                                                 global_average_pooling3d_216[0][0\n",
            "                                                                 global_average_pooling3d_217[0][0\n",
            "                                                                 global_average_pooling3d_218[0][0\n",
            "                                                                 global_average_pooling3d_219[0][0\n",
            "                                                                 global_average_pooling3d_220[0][0\n",
            "                                                                 global_average_pooling3d_221[0][0\n",
            "                                                                 global_average_pooling3d_222[0][0\n",
            "                                                                 global_average_pooling3d_223[0][0\n",
            "                                                                 global_average_pooling3d_224[0][0\n",
            "                                                                 global_average_pooling3d_225[0][0\n",
            "                                                                 global_average_pooling3d_226[0][0\n",
            "                                                                 global_average_pooling3d_227[0][0\n",
            "                                                                 global_average_pooling3d_228[0][0\n",
            "                                                                 global_average_pooling3d_229[0][0\n",
            "                                                                 global_average_pooling3d_230[0][0\n",
            "                                                                 global_average_pooling3d_231[0][0\n",
            "                                                                 global_average_pooling3d_232[0][0\n",
            "                                                                 global_average_pooling3d_233[0][0\n",
            "                                                                 global_average_pooling3d_234[0][0\n",
            "                                                                 global_average_pooling3d_235[0][0\n",
            "                                                                 global_average_pooling3d_236[0][0\n",
            "                                                                 global_average_pooling3d_237[0][0\n",
            "                                                                 global_average_pooling3d_238[0][0\n",
            "                                                                 global_average_pooling3d_239[0][0\n",
            "                                                                 global_average_pooling3d_240[0][0\n",
            "                                                                 global_average_pooling3d_241[0][0\n",
            "                                                                 global_average_pooling3d_242[0][0\n",
            "                                                                 global_average_pooling3d_243[0][0\n",
            "                                                                 global_average_pooling3d_244[0][0\n",
            "                                                                 global_average_pooling3d_245[0][0\n",
            "                                                                 global_average_pooling3d_246[0][0\n",
            "                                                                 global_average_pooling3d_247[0][0\n",
            "                                                                 global_average_pooling3d_248[0][0\n",
            "                                                                 global_average_pooling3d_249[0][0\n",
            "                                                                 global_average_pooling3d_250[0][0\n",
            "                                                                 global_average_pooling3d_251[0][0\n",
            "                                                                 global_average_pooling3d_252[0][0\n",
            "                                                                 global_average_pooling3d_253[0][0\n",
            "                                                                 global_average_pooling3d_254[0][0\n",
            "                                                                 global_average_pooling3d_255[0][0\n",
            "                                                                 global_average_pooling3d_256[0][0\n",
            "                                                                 global_average_pooling3d_257[0][0\n",
            "                                                                 global_average_pooling3d_258[0][0\n",
            "                                                                 global_average_pooling3d_259[0][0\n",
            "                                                                 global_average_pooling3d_260[0][0\n",
            "                                                                 global_average_pooling3d_261[0][0\n",
            "                                                                 global_average_pooling3d_262[0][0\n",
            "                                                                 global_average_pooling3d_263[0][0\n",
            "                                                                 global_average_pooling3d_264[0][0\n",
            "                                                                 global_average_pooling3d_265[0][0\n",
            "                                                                 global_average_pooling3d_266[0][0\n",
            "                                                                 global_average_pooling3d_267[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 512)          262656      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 512)          262656      dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 512)          262656      dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 1)            513         dense_18[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 802,817\n",
            "Trainable params: 802,817\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 12s 985ms/step - loss: 99.2746 - accuracy: 0.5244 - val_loss: 93.3894 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.38942, saving model to ./mod3.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 91.6931 - accuracy: 0.4512 - val_loss: 86.0315 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.38942 to 86.03146, saving model to ./mod3.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 84.4681 - accuracy: 0.4756 - val_loss: 79.0823 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.03146 to 79.08228, saving model to ./mod3.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 77.5647 - accuracy: 0.6098 - val_loss: 72.4631 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.08228 to 72.46305, saving model to ./mod3.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 70.9778 - accuracy: 0.6220 - val_loss: 66.0915 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.46305 to 66.09149, saving model to ./mod3.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 64.7127 - accuracy: 0.6829 - val_loss: 60.0572 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.09149 to 60.05723, saving model to ./mod3.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 58.7355 - accuracy: 0.6829 - val_loss: 54.3527 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.05723 to 54.35268, saving model to ./mod3.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 53.0749 - accuracy: 0.6463 - val_loss: 48.8809 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.35268 to 48.88090, saving model to ./mod3.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 47.7021 - accuracy: 0.6707 - val_loss: 43.7799 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.88090 to 43.77992, saving model to ./mod3.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 42.6257 - accuracy: 0.7073 - val_loss: 38.8579 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.77992 to 38.85788, saving model to ./mod3.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 37.8760 - accuracy: 0.6707 - val_loss: 34.3158 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.85788 to 34.31577, saving model to ./mod3.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 33.3658 - accuracy: 0.6829 - val_loss: 30.1113 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.31577 to 30.11127, saving model to ./mod3.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 29.1897 - accuracy: 0.6707 - val_loss: 26.1508 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.11127 to 26.15080, saving model to ./mod3.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 25.3077 - accuracy: 0.6951 - val_loss: 22.5832 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.15080 to 22.58320, saving model to ./mod3.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 21.7717 - accuracy: 0.7073 - val_loss: 19.2078 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.58320 to 19.20781, saving model to ./mod3.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 18.4605 - accuracy: 0.7195 - val_loss: 16.0447 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.20781 to 16.04471, saving model to ./mod3.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 15.5064 - accuracy: 0.6585 - val_loss: 13.4064 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.04471 to 13.40636, saving model to ./mod3.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 12.8260 - accuracy: 0.7073 - val_loss: 10.9217 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.40636 to 10.92172, saving model to ./mod3.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 10.4587 - accuracy: 0.7317 - val_loss: 8.8304 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00019: val_loss improved from 10.92172 to 8.83044, saving model to ./mod3.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 8.3902 - accuracy: 0.7317 - val_loss: 7.0346 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.83044 to 7.03456, saving model to ./mod3.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 6.6375 - accuracy: 0.7439 - val_loss: 5.3810 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.03456 to 5.38100, saving model to ./mod3.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 5.1839 - accuracy: 0.6829 - val_loss: 4.3725 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.38100 to 4.37245, saving model to ./mod3.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 4.0338 - accuracy: 0.7561 - val_loss: 3.2333 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.37245 to 3.23330, saving model to ./mod3.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 3.1548 - accuracy: 0.7439 - val_loss: 2.5859 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.23330 to 2.58593, saving model to ./mod3.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 2.6073 - accuracy: 0.7195 - val_loss: 2.9387 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 2.58593\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 2.6527 - accuracy: 0.5732 - val_loss: 2.2900 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.58593 to 2.28995, saving model to ./mod3.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 2.3661 - accuracy: 0.6707 - val_loss: 2.0728 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.28995 to 2.07276, saving model to ./mod3.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 2.0801 - accuracy: 0.7683 - val_loss: 1.9433 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.07276 to 1.94331, saving model to ./mod3.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 1.8007 - accuracy: 0.7439 - val_loss: 1.5228 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.94331 to 1.52282, saving model to ./mod3.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.6739 - accuracy: 0.5610 - val_loss: 1.3633 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.52282 to 1.36326, saving model to ./mod3.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.4316 - accuracy: 0.8415 - val_loss: 1.3533 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.36326 to 1.35325, saving model to ./mod3.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.3385 - accuracy: 0.8415 - val_loss: 1.1812 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.35325 to 1.18122, saving model to ./mod3.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 1.2182 - accuracy: 0.8902 - val_loss: 1.2437 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.18122\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.1453 - accuracy: 0.7805 - val_loss: 1.0123 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.18122 to 1.01228, saving model to ./mod3.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 1.0686 - accuracy: 0.8780 - val_loss: 1.0632 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.01228\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.0362 - accuracy: 0.7805 - val_loss: 0.8871 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.01228 to 0.88708, saving model to ./mod3.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.9838 - accuracy: 0.8780 - val_loss: 1.0611 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.88708\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.0113 - accuracy: 0.7683 - val_loss: 0.9181 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.88708\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.9241 - accuracy: 0.9024 - val_loss: 0.7910 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.88708 to 0.79105, saving model to ./mod3.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.8859 - accuracy: 0.8659 - val_loss: 0.8305 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.79105\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.8546 - accuracy: 0.8659 - val_loss: 0.8747 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.79105\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.8960 - accuracy: 0.7805 - val_loss: 0.9393 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.79105\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.8490 - accuracy: 0.8415 - val_loss: 0.7532 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.79105 to 0.75319, saving model to ./mod3.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.8071 - accuracy: 0.8902 - val_loss: 0.7495 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.75319 to 0.74948, saving model to ./mod3.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.8319 - accuracy: 0.8659 - val_loss: 0.7799 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.74948\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.8289 - accuracy: 0.7927 - val_loss: 0.9499 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.74948\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.8067 - accuracy: 0.8049 - val_loss: 0.6925 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.74948 to 0.69251, saving model to ./mod3.h5\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.8753 - accuracy: 0.7439 - val_loss: 0.8979 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69251\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.8858 - accuracy: 0.7927 - val_loss: 0.9123 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69251\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.7765 - accuracy: 0.8902 - val_loss: 0.6979 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69251\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.8631 - accuracy: 0.8171 - val_loss: 0.7665 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69251\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.7657 - accuracy: 0.8537 - val_loss: 0.8395 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69251\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.7459 - accuracy: 0.8780 - val_loss: 0.6610 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.69251 to 0.66104, saving model to ./mod3.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.7179 - accuracy: 0.9024 - val_loss: 0.7564 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.66104\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.7055 - accuracy: 0.9024 - val_loss: 0.6733 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.66104\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.6845 - accuracy: 0.9024 - val_loss: 0.7561 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.66104\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.7129 - accuracy: 0.8780 - val_loss: 0.5969 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.66104 to 0.59692, saving model to ./mod3.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.7050 - accuracy: 0.9146 - val_loss: 0.8674 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.59692\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.8360 - accuracy: 0.8415 - val_loss: 0.7285 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.59692\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.7187 - accuracy: 0.8780 - val_loss: 0.5912 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.59692 to 0.59124, saving model to ./mod3.h5\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6916 - accuracy: 0.9024 - val_loss: 1.0334 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.59124\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.8503 - accuracy: 0.7683 - val_loss: 0.7472 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.59124\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6966 - accuracy: 0.9146 - val_loss: 0.5872 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.59124 to 0.58722, saving model to ./mod3.h5\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.6742 - accuracy: 0.9268 - val_loss: 0.8283 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.58722\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.7234 - accuracy: 0.8537 - val_loss: 0.7431 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.58722\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.6234 - accuracy: 0.9512 - val_loss: 0.5641 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.58722 to 0.56410, saving model to ./mod3.h5\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6447 - accuracy: 0.9268 - val_loss: 0.7554 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.56410\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6814 - accuracy: 0.8902 - val_loss: 0.6201 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.56410\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6484 - accuracy: 0.8780 - val_loss: 0.5678 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.56410\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6177 - accuracy: 0.9512 - val_loss: 0.6847 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.56410\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5900 - accuracy: 0.9390 - val_loss: 0.5513 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.56410 to 0.55133, saving model to ./mod3.h5\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.5902 - accuracy: 0.9634 - val_loss: 0.6767 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.55133\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.5659 - accuracy: 0.9390 - val_loss: 0.5364 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.55133 to 0.53642, saving model to ./mod3.h5\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.5829 - accuracy: 0.9634 - val_loss: 0.5483 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.53642\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5440 - accuracy: 0.9878 - val_loss: 0.6370 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.53642\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5560 - accuracy: 0.9268 - val_loss: 0.5651 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.53642\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.5537 - accuracy: 0.9634 - val_loss: 0.5514 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.53642\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.5445 - accuracy: 0.9756 - val_loss: 0.6134 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.53642\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.5449 - accuracy: 0.9512 - val_loss: 0.5548 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.53642\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5112 - accuracy: 0.9756 - val_loss: 0.5947 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.53642\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5260 - accuracy: 0.9512 - val_loss: 0.4991 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.53642 to 0.49913, saving model to ./mod3.h5\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5920 - accuracy: 0.9268 - val_loss: 0.5648 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.49913\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5099 - accuracy: 0.9756 - val_loss: 0.5916 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.49913\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4995 - accuracy: 0.9878 - val_loss: 0.4962 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.49913 to 0.49622, saving model to ./mod3.h5\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.5301 - accuracy: 0.9756 - val_loss: 0.5850 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.49622\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.5642 - accuracy: 0.9146 - val_loss: 0.5839 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.49622\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.4989 - accuracy: 0.9756 - val_loss: 0.4852 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.49622 to 0.48525, saving model to ./mod3.h5\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4967 - accuracy: 0.9878 - val_loss: 0.6074 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.48525\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4963 - accuracy: 0.9756 - val_loss: 0.4825 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.48525 to 0.48248, saving model to ./mod3.h5\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.4929 - accuracy: 1.0000 - val_loss: 0.5568 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.48248\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5026 - accuracy: 0.9634 - val_loss: 0.5209 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.48248\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.5027 - accuracy: 0.9878 - val_loss: 0.5256 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.48248\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.5031 - accuracy: 0.9634 - val_loss: 0.5981 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.48248\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4893 - accuracy: 0.9878 - val_loss: 0.4974 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.48248\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4605 - accuracy: 0.9878 - val_loss: 0.5861 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.48248\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4664 - accuracy: 0.9878 - val_loss: 0.4712 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.48248 to 0.47117, saving model to ./mod3.h5\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4585 - accuracy: 1.0000 - val_loss: 0.4826 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.47117\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4461 - accuracy: 1.0000 - val_loss: 0.5179 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.47117\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4550 - accuracy: 0.9878 - val_loss: 0.4856 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.47117\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4573 - accuracy: 0.9878 - val_loss: 0.4686 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.47117 to 0.46863, saving model to ./mod3.h5\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4431 - accuracy: 1.0000 - val_loss: 0.4787 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.46863\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4467 - accuracy: 0.9878 - val_loss: 0.4381 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.46863 to 0.43806, saving model to ./mod3.h5\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4853 - accuracy: 0.9756 - val_loss: 0.5445 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.43806\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4704 - accuracy: 0.9634 - val_loss: 0.4664 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.43806\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4554 - accuracy: 1.0000 - val_loss: 0.4483 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.43806\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.4367 - accuracy: 1.0000 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.43806\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.4586 - accuracy: 0.9756 - val_loss: 0.4355 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.43806 to 0.43551, saving model to ./mod3.h5\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4436 - accuracy: 1.0000 - val_loss: 0.5324 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.43551\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4776 - accuracy: 0.9878 - val_loss: 0.4796 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.43551\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4369 - accuracy: 1.0000 - val_loss: 0.4587 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.43551\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4259 - accuracy: 1.0000 - val_loss: 0.5669 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.43551\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.4605 - accuracy: 0.9756 - val_loss: 0.4246 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.43551 to 0.42457, saving model to ./mod3.h5\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4557 - accuracy: 0.9878 - val_loss: 0.5238 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.42457\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4432 - accuracy: 0.9878 - val_loss: 0.5115 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.42457\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4300 - accuracy: 1.0000 - val_loss: 0.4165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.42457 to 0.41655, saving model to ./mod3.h5\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4576 - accuracy: 1.0000 - val_loss: 0.5683 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.41655\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4574 - accuracy: 0.9634 - val_loss: 0.4638 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.41655\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4214 - accuracy: 1.0000 - val_loss: 0.4210 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.41655\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4205 - accuracy: 1.0000 - val_loss: 0.4615 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.41655\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4212 - accuracy: 1.0000 - val_loss: 0.4653 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.41655\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4101 - accuracy: 1.0000 - val_loss: 0.4591 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.41655\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4169 - accuracy: 0.9878 - val_loss: 0.4368 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.41655\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4162 - accuracy: 1.0000 - val_loss: 0.4299 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.41655\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4088 - accuracy: 1.0000 - val_loss: 0.4571 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.41655\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4131 - accuracy: 0.9878 - val_loss: 0.5655 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.41655\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4426 - accuracy: 0.9756 - val_loss: 0.4204 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.41655\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4265 - accuracy: 1.0000 - val_loss: 0.4953 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.41655\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4429 - accuracy: 0.9756 - val_loss: 0.4357 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.41655\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4238 - accuracy: 1.0000 - val_loss: 0.4222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.41655\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.4070 - accuracy: 1.0000 - val_loss: 0.5203 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.41655\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.4162 - accuracy: 0.9878 - val_loss: 0.4448 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.41655\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4064 - accuracy: 1.0000 - val_loss: 0.4231 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.41655\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4044 - accuracy: 1.0000 - val_loss: 0.4856 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.41655\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4020 - accuracy: 1.0000 - val_loss: 0.4250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.41655\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4136 - accuracy: 1.0000 - val_loss: 0.4003 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.41655 to 0.40028, saving model to ./mod3.h5\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4080 - accuracy: 1.0000 - val_loss: 0.4537 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.40028\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4048 - accuracy: 1.0000 - val_loss: 0.4088 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.40028\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3932 - accuracy: 1.0000 - val_loss: 0.4458 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.40028\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4101 - accuracy: 1.0000 - val_loss: 0.4213 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.40028\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3902 - accuracy: 1.0000 - val_loss: 0.4229 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.40028\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3924 - accuracy: 1.0000 - val_loss: 0.4079 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.40028\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3943 - accuracy: 1.0000 - val_loss: 0.4344 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.40028\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3920 - accuracy: 1.0000 - val_loss: 0.4646 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.40028\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3978 - accuracy: 1.0000 - val_loss: 0.4247 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.40028\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.4017 - accuracy: 1.0000 - val_loss: 0.4523 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.40028\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3960 - accuracy: 1.0000 - val_loss: 0.4427 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.40028\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3886 - accuracy: 1.0000 - val_loss: 0.4098 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.40028\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3845 - accuracy: 1.0000 - val_loss: 0.4209 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.40028\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.3833 - accuracy: 1.0000 - val_loss: 0.4252 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.40028\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3868 - accuracy: 1.0000 - val_loss: 0.4517 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.40028\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3924 - accuracy: 1.0000 - val_loss: 0.4086 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.40028\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3853 - accuracy: 1.0000 - val_loss: 0.4008 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.40028\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3863 - accuracy: 1.0000 - val_loss: 0.4059 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.40028\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3852 - accuracy: 1.0000 - val_loss: 0.4308 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.40028\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3837 - accuracy: 1.0000 - val_loss: 0.4638 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.40028\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3835 - accuracy: 1.0000 - val_loss: 0.4001 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss improved from 0.40028 to 0.40007, saving model to ./mod3.h5\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3840 - accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.40007\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3836 - accuracy: 1.0000 - val_loss: 0.4333 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.40007\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3851 - accuracy: 1.0000 - val_loss: 0.4083 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.40007\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3795 - accuracy: 1.0000 - val_loss: 0.4275 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.40007\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3869 - accuracy: 1.0000 - val_loss: 0.3934 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss improved from 0.40007 to 0.39336, saving model to ./mod3.h5\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3826 - accuracy: 1.0000 - val_loss: 0.3936 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.39336\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3761 - accuracy: 1.0000 - val_loss: 0.4290 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.39336\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3831 - accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.39336\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3811 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.39336\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3825 - accuracy: 1.0000 - val_loss: 0.3950 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.39336\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3865 - accuracy: 1.0000 - val_loss: 0.3923 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.39336 to 0.39231, saving model to ./mod3.h5\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3781 - accuracy: 1.0000 - val_loss: 0.4356 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.39231\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3799 - accuracy: 1.0000 - val_loss: 0.4002 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.39231\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3809 - accuracy: 1.0000 - val_loss: 0.3917 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.39231 to 0.39172, saving model to ./mod3.h5\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3779 - accuracy: 1.0000 - val_loss: 0.3870 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.39172 to 0.38696, saving model to ./mod3.h5\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3757 - accuracy: 1.0000 - val_loss: 0.4154 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.38696\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3747 - accuracy: 1.0000 - val_loss: 0.4215 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.38696\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3777 - accuracy: 1.0000 - val_loss: 0.4081 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.38696\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.3802 - accuracy: 1.0000 - val_loss: 0.3912 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.38696\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3788 - accuracy: 1.0000 - val_loss: 0.3856 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.38696 to 0.38555, saving model to ./mod3.h5\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3776 - accuracy: 1.0000 - val_loss: 0.4044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.38555\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3718 - accuracy: 1.0000 - val_loss: 0.3874 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.38555\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3728 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.38555\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3722 - accuracy: 1.0000 - val_loss: 0.3794 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss improved from 0.38555 to 0.37943, saving model to ./mod3.h5\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3828 - accuracy: 1.0000 - val_loss: 0.4082 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.37943\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3861 - accuracy: 1.0000 - val_loss: 0.4544 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.37943\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3778 - accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.37943\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3998 - accuracy: 1.0000 - val_loss: 0.4415 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.37943\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3757 - accuracy: 1.0000 - val_loss: 0.4287 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.37943\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3793 - accuracy: 1.0000 - val_loss: 0.4427 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.37943\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3745 - accuracy: 1.0000 - val_loss: 0.4496 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.37943\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3726 - accuracy: 1.0000 - val_loss: 0.3926 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.37943\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3737 - accuracy: 1.0000 - val_loss: 0.3888 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.37943\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3712 - accuracy: 1.0000 - val_loss: 0.4143 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.37943\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3694 - accuracy: 1.0000 - val_loss: 0.4071 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.37943\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3679 - accuracy: 1.0000 - val_loss: 0.3808 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.37943\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3667 - accuracy: 1.0000 - val_loss: 0.4012 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.37943\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3709 - accuracy: 1.0000 - val_loss: 0.4065 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.37943\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3692 - accuracy: 1.0000 - val_loss: 0.3812 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.37943\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3625 - accuracy: 1.0000 - val_loss: 0.3883 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.37943\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3691 - accuracy: 1.0000 - val_loss: 0.4059 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.37943\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.3680 - accuracy: 1.0000 - val_loss: 0.3989 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.37943\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3661 - accuracy: 1.0000 - val_loss: 0.3879 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.37943\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.3647 - accuracy: 1.0000 - val_loss: 0.4054 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.37943\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 110ms/step - loss: 0.3750 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.4940 - accuracy: 0.9200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.3794 - accuracy: 1.0000\n",
            "(None, 19, 19, 19, 1)\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 19, 19, 19,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_537 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_539 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_541 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_543 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_545 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_547 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_549 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_551 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_553 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_555 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_557 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_559 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_561 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_563 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_565 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_567 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_569 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_571 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_573 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_575 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_577 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_579 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_581 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_583 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_585 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_587 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_589 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_591 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_593 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_595 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_597 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_599 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_601 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_603 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_605 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_607 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_609 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_611 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_613 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_615 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_617 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_619 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_621 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_623 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_625 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_627 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_629 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_631 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_633 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_635 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_637 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_639 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_641 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_643 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_645 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_647 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_649 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_651 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_653 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_655 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_657 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_659 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_661 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_663 (Lambda)             (None, 19, 5, 19, 1) 0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_536 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_537[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_538 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_539[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_540 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_541[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_542 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_543[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_544 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_545[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_546 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_547[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_548 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_549[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_550 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_551[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_552 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_553[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_554 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_555[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_556 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_557[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_558 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_559[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_560 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_561[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_562 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_563[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_564 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_565[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_566 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_567[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_568 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_569[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_570 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_571[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_572 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_573[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_574 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_575[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_576 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_577[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_578 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_579[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_580 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_581[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_582 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_583[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_584 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_585[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_586 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_587[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_588 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_589[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_590 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_591[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_592 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_593[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_594 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_595[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_596 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_597[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_598 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_599[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_600 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_601[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_602 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_603[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_604 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_605[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_606 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_607[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_608 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_609[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_610 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_611[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_612 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_613[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_614 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_615[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_616 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_617[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_618 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_619[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_620 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_621[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_622 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_623[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_624 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_625[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_626 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_627[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_628 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_629[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_630 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_631[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_632 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_633[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_634 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_635[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_636 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_637[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_638 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_639[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_640 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_641[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_642 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_643[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_644 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_645[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_646 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_647[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_648 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_649[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_650 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_651[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_652 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_653[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_654 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_655[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_656 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_657[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_658 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_659[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_660 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_661[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_662 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_663[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_268 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_536[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_269 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_538[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_270 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_540[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_271 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_542[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_272 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_544[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_273 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_546[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_274 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_548[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_275 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_550[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_276 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_552[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_277 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_554[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_278 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_556[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_279 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_558[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_280 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_560[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_281 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_562[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_282 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_564[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_283 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_566[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_284 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_568[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_285 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_570[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_286 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_572[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_287 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_574[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_288 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_576[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_289 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_578[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_290 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_580[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_291 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_582[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_292 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_584[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_293 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_586[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_294 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_588[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_295 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_590[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_296 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_592[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_297 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_594[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_298 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_596[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_299 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_598[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_300 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_600[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_301 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_602[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_302 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_604[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_303 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_606[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_304 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_608[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_305 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_610[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_306 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_612[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_307 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_614[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_308 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_616[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_309 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_618[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_310 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_620[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_311 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_622[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_312 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_624[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_313 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_626[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_314 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_628[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_315 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_630[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_316 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_632[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_317 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_634[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_318 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_636[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_319 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_638[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_320 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_640[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_321 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_642[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_322 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_644[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_323 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_646[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_324 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_648[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_325 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_650[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_326 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_652[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_327 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_654[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_328 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_656[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_329 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_658[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_330 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_660[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_331 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_662[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_268 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_269 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_270 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_271 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_272 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_273 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_274 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_275 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_276 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_277 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_278 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_279 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_280 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_281 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_282 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_282[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_283 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_283[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_284 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_284[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_285 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_285[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_286 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_286[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_287 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_287[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_288 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_288[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_289 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_289[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_290 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_290[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_291 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_291[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_292 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_292[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_293 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_293[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_294 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_294[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_295 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_295[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_296 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_296[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_297 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_297[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_298 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_298[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_299 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_299[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_300 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_300[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_301 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_301[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_302 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_302[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_303 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_303[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_304 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_304[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_305 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_305[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_306 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_306[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_307 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_307[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_308 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_308[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_309 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_309[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_310 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_310[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_311 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_311[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_312 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_312[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_313 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_313[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_314 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_314[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_315 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_315[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_316 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_316[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_317 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_317[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_318 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_318[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_319 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_319[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_320 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_320[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_321 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_321[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_322 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_322[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_323 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_323[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_324 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_324[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_325 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_325[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_326 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_326[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_327 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_327[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_328 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_328[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_329 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_329[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_330 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_330[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_331 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_331[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_268 (G (None, 8)            0           dropout_268[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_269 (G (None, 8)            0           dropout_269[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_270 (G (None, 8)            0           dropout_270[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_271 (G (None, 8)            0           dropout_271[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_272 (G (None, 8)            0           dropout_272[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_273 (G (None, 8)            0           dropout_273[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_274 (G (None, 8)            0           dropout_274[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_275 (G (None, 8)            0           dropout_275[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_276 (G (None, 8)            0           dropout_276[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_277 (G (None, 8)            0           dropout_277[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_278 (G (None, 8)            0           dropout_278[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_279 (G (None, 8)            0           dropout_279[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_280 (G (None, 8)            0           dropout_280[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_281 (G (None, 8)            0           dropout_281[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_282 (G (None, 8)            0           dropout_282[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_283 (G (None, 8)            0           dropout_283[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_284 (G (None, 8)            0           dropout_284[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_285 (G (None, 8)            0           dropout_285[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_286 (G (None, 8)            0           dropout_286[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_287 (G (None, 8)            0           dropout_287[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_288 (G (None, 8)            0           dropout_288[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_289 (G (None, 8)            0           dropout_289[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_290 (G (None, 8)            0           dropout_290[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_291 (G (None, 8)            0           dropout_291[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_292 (G (None, 8)            0           dropout_292[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_293 (G (None, 8)            0           dropout_293[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_294 (G (None, 8)            0           dropout_294[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_295 (G (None, 8)            0           dropout_295[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_296 (G (None, 8)            0           dropout_296[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_297 (G (None, 8)            0           dropout_297[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_298 (G (None, 8)            0           dropout_298[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_299 (G (None, 8)            0           dropout_299[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_300 (G (None, 8)            0           dropout_300[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_301 (G (None, 8)            0           dropout_301[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_302 (G (None, 8)            0           dropout_302[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_303 (G (None, 8)            0           dropout_303[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_304 (G (None, 8)            0           dropout_304[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_305 (G (None, 8)            0           dropout_305[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_306 (G (None, 8)            0           dropout_306[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_307 (G (None, 8)            0           dropout_307[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_308 (G (None, 8)            0           dropout_308[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_309 (G (None, 8)            0           dropout_309[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_310 (G (None, 8)            0           dropout_310[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_311 (G (None, 8)            0           dropout_311[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_312 (G (None, 8)            0           dropout_312[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_313 (G (None, 8)            0           dropout_313[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_314 (G (None, 8)            0           dropout_314[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_315 (G (None, 8)            0           dropout_315[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_316 (G (None, 8)            0           dropout_316[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_317 (G (None, 8)            0           dropout_317[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_318 (G (None, 8)            0           dropout_318[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_319 (G (None, 8)            0           dropout_319[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_320 (G (None, 8)            0           dropout_320[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_321 (G (None, 8)            0           dropout_321[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_322 (G (None, 8)            0           dropout_322[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_323 (G (None, 8)            0           dropout_323[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_324 (G (None, 8)            0           dropout_324[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_325 (G (None, 8)            0           dropout_325[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_326 (G (None, 8)            0           dropout_326[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_327 (G (None, 8)            0           dropout_327[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_328 (G (None, 8)            0           dropout_328[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_329 (G (None, 8)            0           dropout_329[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_330 (G (None, 8)            0           dropout_330[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_331 (G (None, 8)            0           dropout_331[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 512)          0           global_average_pooling3d_268[0][0\n",
            "                                                                 global_average_pooling3d_269[0][0\n",
            "                                                                 global_average_pooling3d_270[0][0\n",
            "                                                                 global_average_pooling3d_271[0][0\n",
            "                                                                 global_average_pooling3d_272[0][0\n",
            "                                                                 global_average_pooling3d_273[0][0\n",
            "                                                                 global_average_pooling3d_274[0][0\n",
            "                                                                 global_average_pooling3d_275[0][0\n",
            "                                                                 global_average_pooling3d_276[0][0\n",
            "                                                                 global_average_pooling3d_277[0][0\n",
            "                                                                 global_average_pooling3d_278[0][0\n",
            "                                                                 global_average_pooling3d_279[0][0\n",
            "                                                                 global_average_pooling3d_280[0][0\n",
            "                                                                 global_average_pooling3d_281[0][0\n",
            "                                                                 global_average_pooling3d_282[0][0\n",
            "                                                                 global_average_pooling3d_283[0][0\n",
            "                                                                 global_average_pooling3d_284[0][0\n",
            "                                                                 global_average_pooling3d_285[0][0\n",
            "                                                                 global_average_pooling3d_286[0][0\n",
            "                                                                 global_average_pooling3d_287[0][0\n",
            "                                                                 global_average_pooling3d_288[0][0\n",
            "                                                                 global_average_pooling3d_289[0][0\n",
            "                                                                 global_average_pooling3d_290[0][0\n",
            "                                                                 global_average_pooling3d_291[0][0\n",
            "                                                                 global_average_pooling3d_292[0][0\n",
            "                                                                 global_average_pooling3d_293[0][0\n",
            "                                                                 global_average_pooling3d_294[0][0\n",
            "                                                                 global_average_pooling3d_295[0][0\n",
            "                                                                 global_average_pooling3d_296[0][0\n",
            "                                                                 global_average_pooling3d_297[0][0\n",
            "                                                                 global_average_pooling3d_298[0][0\n",
            "                                                                 global_average_pooling3d_299[0][0\n",
            "                                                                 global_average_pooling3d_300[0][0\n",
            "                                                                 global_average_pooling3d_301[0][0\n",
            "                                                                 global_average_pooling3d_302[0][0\n",
            "                                                                 global_average_pooling3d_303[0][0\n",
            "                                                                 global_average_pooling3d_304[0][0\n",
            "                                                                 global_average_pooling3d_305[0][0\n",
            "                                                                 global_average_pooling3d_306[0][0\n",
            "                                                                 global_average_pooling3d_307[0][0\n",
            "                                                                 global_average_pooling3d_308[0][0\n",
            "                                                                 global_average_pooling3d_309[0][0\n",
            "                                                                 global_average_pooling3d_310[0][0\n",
            "                                                                 global_average_pooling3d_311[0][0\n",
            "                                                                 global_average_pooling3d_312[0][0\n",
            "                                                                 global_average_pooling3d_313[0][0\n",
            "                                                                 global_average_pooling3d_314[0][0\n",
            "                                                                 global_average_pooling3d_315[0][0\n",
            "                                                                 global_average_pooling3d_316[0][0\n",
            "                                                                 global_average_pooling3d_317[0][0\n",
            "                                                                 global_average_pooling3d_318[0][0\n",
            "                                                                 global_average_pooling3d_319[0][0\n",
            "                                                                 global_average_pooling3d_320[0][0\n",
            "                                                                 global_average_pooling3d_321[0][0\n",
            "                                                                 global_average_pooling3d_322[0][0\n",
            "                                                                 global_average_pooling3d_323[0][0\n",
            "                                                                 global_average_pooling3d_324[0][0\n",
            "                                                                 global_average_pooling3d_325[0][0\n",
            "                                                                 global_average_pooling3d_326[0][0\n",
            "                                                                 global_average_pooling3d_327[0][0\n",
            "                                                                 global_average_pooling3d_328[0][0\n",
            "                                                                 global_average_pooling3d_329[0][0\n",
            "                                                                 global_average_pooling3d_330[0][0\n",
            "                                                                 global_average_pooling3d_331[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 512)          262656      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 512)          262656      dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 512)          262656      dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 1)            513         dense_22[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 802,817\n",
            "Trainable params: 802,817\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 12s 993ms/step - loss: 99.2224 - accuracy: 0.4390 - val_loss: 93.3084 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.30836, saving model to ./mod4.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 91.6265 - accuracy: 0.5122 - val_loss: 86.0130 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.30836 to 86.01298, saving model to ./mod4.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 84.3898 - accuracy: 0.5366 - val_loss: 79.0302 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.01298 to 79.03024, saving model to ./mod4.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 77.4957 - accuracy: 0.5610 - val_loss: 72.3830 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.03024 to 72.38303, saving model to ./mod4.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 70.9173 - accuracy: 0.5976 - val_loss: 66.0670 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.38303 to 66.06705, saving model to ./mod4.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 64.6530 - accuracy: 0.6341 - val_loss: 60.0358 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.06705 to 60.03576, saving model to ./mod4.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 58.6900 - accuracy: 0.6707 - val_loss: 54.3125 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.03576 to 54.31248, saving model to ./mod4.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 53.0405 - accuracy: 0.6341 - val_loss: 48.8959 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.31248 to 48.89592, saving model to ./mod4.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 47.6658 - accuracy: 0.6220 - val_loss: 43.7890 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.89592 to 43.78900, saving model to ./mod4.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 42.5983 - accuracy: 0.6829 - val_loss: 38.9552 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.78900 to 38.95520, saving model to ./mod4.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 37.8112 - accuracy: 0.6707 - val_loss: 34.3916 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.95520 to 34.39156, saving model to ./mod4.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 33.3319 - accuracy: 0.7073 - val_loss: 30.1772 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.39156 to 30.17723, saving model to ./mod4.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 29.1444 - accuracy: 0.6463 - val_loss: 26.2489 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.17723 to 26.24890, saving model to ./mod4.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 25.2418 - accuracy: 0.6951 - val_loss: 22.6120 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.24890 to 22.61201, saving model to ./mod4.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 21.6610 - accuracy: 0.7317 - val_loss: 19.2608 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.61201 to 19.26082, saving model to ./mod4.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 18.3895 - accuracy: 0.7683 - val_loss: 16.5713 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.26082 to 16.57133, saving model to ./mod4.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 15.6180 - accuracy: 0.6220 - val_loss: 13.5529 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.57133 to 13.55289, saving model to ./mod4.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 12.8232 - accuracy: 0.6098 - val_loss: 11.1003 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.55289 to 11.10028, saving model to ./mod4.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 10.4220 - accuracy: 0.7927 - val_loss: 9.0425 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.10028 to 9.04253, saving model to ./mod4.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 8.4092 - accuracy: 0.7073 - val_loss: 7.1667 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00020: val_loss improved from 9.04253 to 7.16669, saving model to ./mod4.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 6.6519 - accuracy: 0.7683 - val_loss: 5.6462 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.16669 to 5.64621, saving model to ./mod4.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 5.1749 - accuracy: 0.7805 - val_loss: 4.4553 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.64621 to 4.45530, saving model to ./mod4.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 4.0354 - accuracy: 0.7195 - val_loss: 3.5150 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.45530 to 3.51503, saving model to ./mod4.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 3.1897 - accuracy: 0.7805 - val_loss: 2.9115 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.51503 to 2.91146, saving model to ./mod4.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 2.6257 - accuracy: 0.7439 - val_loss: 2.6455 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.91146 to 2.64547, saving model to ./mod4.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 2.3633 - accuracy: 0.7805 - val_loss: 2.5031 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.64547 to 2.50311, saving model to ./mod4.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 2.2640 - accuracy: 0.8049 - val_loss: 2.3161 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.50311 to 2.31609, saving model to ./mod4.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 2.0062 - accuracy: 0.7561 - val_loss: 2.0412 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.31609 to 2.04122, saving model to ./mod4.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 1.7103 - accuracy: 0.8537 - val_loss: 1.7891 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00029: val_loss improved from 2.04122 to 1.78912, saving model to ./mod4.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 1.4728 - accuracy: 0.8537 - val_loss: 1.7034 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.78912 to 1.70341, saving model to ./mod4.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.3375 - accuracy: 0.8049 - val_loss: 1.5803 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.70341 to 1.58034, saving model to ./mod4.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.3087 - accuracy: 0.7439 - val_loss: 1.4826 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.58034 to 1.48256, saving model to ./mod4.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.1490 - accuracy: 0.8780 - val_loss: 1.4176 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.48256 to 1.41755, saving model to ./mod4.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 1.0451 - accuracy: 0.8659 - val_loss: 1.4533 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.41755\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.0058 - accuracy: 0.8171 - val_loss: 1.3178 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.41755 to 1.31781, saving model to ./mod4.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.9723 - accuracy: 0.8780 - val_loss: 1.4105 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.31781\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.9131 - accuracy: 0.8537 - val_loss: 1.3138 - val_accuracy: 0.2857\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.31781 to 1.31385, saving model to ./mod4.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.8779 - accuracy: 0.8780 - val_loss: 1.4734 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.31385\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.9208 - accuracy: 0.7927 - val_loss: 1.3163 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.31385\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.0362 - accuracy: 0.6707 - val_loss: 1.5210 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.31385\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 1.0424 - accuracy: 0.7317 - val_loss: 1.4977 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.31385\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.9202 - accuracy: 0.7805 - val_loss: 1.2556 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.31385 to 1.25564, saving model to ./mod4.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.9106 - accuracy: 0.7561 - val_loss: 1.1864 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.25564 to 1.18642, saving model to ./mod4.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.8226 - accuracy: 0.9024 - val_loss: 1.3174 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.18642\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.8064 - accuracy: 0.8537 - val_loss: 1.2609 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.18642\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.8245 - accuracy: 0.8171 - val_loss: 1.2425 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.18642\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.7306 - accuracy: 0.9390 - val_loss: 1.3758 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.18642\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.8244 - accuracy: 0.7805 - val_loss: 1.2589 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.18642\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.7667 - accuracy: 0.9146 - val_loss: 1.2653 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.18642\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.7305 - accuracy: 0.9146 - val_loss: 1.3887 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.18642\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.7506 - accuracy: 0.8415 - val_loss: 1.2672 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.18642\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.7125 - accuracy: 0.8902 - val_loss: 1.2570 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.18642\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.7066 - accuracy: 0.8902 - val_loss: 1.3067 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.18642\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.6828 - accuracy: 0.9146 - val_loss: 1.2599 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.18642\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.6675 - accuracy: 0.9268 - val_loss: 1.2557 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.18642\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.6793 - accuracy: 0.9268 - val_loss: 1.3323 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.18642\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6884 - accuracy: 0.9024 - val_loss: 1.2893 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.18642\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.6580 - accuracy: 0.9268 - val_loss: 1.2671 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.18642\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.6726 - accuracy: 0.8902 - val_loss: 1.2619 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.18642\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.6411 - accuracy: 0.9390 - val_loss: 1.2507 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.18642\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6415 - accuracy: 0.9268 - val_loss: 1.2633 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.18642\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6313 - accuracy: 0.9512 - val_loss: 1.2473 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.18642\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6445 - accuracy: 0.9146 - val_loss: 1.2052 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.18642\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6468 - accuracy: 0.9268 - val_loss: 1.3382 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.18642\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6189 - accuracy: 0.9390 - val_loss: 1.2026 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.18642\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6318 - accuracy: 0.9146 - val_loss: 1.4396 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.18642\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.7749 - accuracy: 0.8049 - val_loss: 1.2554 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.18642\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.6708 - accuracy: 0.9024 - val_loss: 1.2840 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.18642\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.6922 - accuracy: 0.9024 - val_loss: 1.3229 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.18642\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6753 - accuracy: 0.8780 - val_loss: 1.1251 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00070: val_loss improved from 1.18642 to 1.12506, saving model to ./mod4.h5\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6368 - accuracy: 0.9390 - val_loss: 1.1365 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.12506\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.6541 - accuracy: 0.9146 - val_loss: 1.1923 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.12506\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.6665 - accuracy: 0.8902 - val_loss: 1.3257 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.12506\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6143 - accuracy: 0.9390 - val_loss: 1.1722 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.12506\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.6182 - accuracy: 0.9268 - val_loss: 1.2189 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.12506\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5847 - accuracy: 0.9512 - val_loss: 1.4991 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.12506\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6459 - accuracy: 0.8902 - val_loss: 1.1849 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.12506\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.5997 - accuracy: 0.9390 - val_loss: 1.1881 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.12506\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.5742 - accuracy: 0.9634 - val_loss: 1.4761 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.12506\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6122 - accuracy: 0.9390 - val_loss: 1.2403 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.12506\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.5632 - accuracy: 0.9512 - val_loss: 1.1539 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.12506\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.5536 - accuracy: 0.9512 - val_loss: 1.2920 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.12506\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.5590 - accuracy: 0.9634 - val_loss: 1.1894 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.12506\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.5366 - accuracy: 0.9634 - val_loss: 1.1359 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.12506\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.5282 - accuracy: 0.9634 - val_loss: 1.2270 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.12506\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.5515 - accuracy: 0.9512 - val_loss: 1.2038 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.12506\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.5284 - accuracy: 0.9756 - val_loss: 1.0719 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00087: val_loss improved from 1.12506 to 1.07187, saving model to ./mod4.h5\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.5634 - accuracy: 0.9268 - val_loss: 1.2150 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.07187\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.5414 - accuracy: 0.9512 - val_loss: 1.1847 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.07187\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.5337 - accuracy: 0.9634 - val_loss: 1.1711 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.07187\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.5099 - accuracy: 0.9756 - val_loss: 1.3552 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.07187\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.5474 - accuracy: 0.9634 - val_loss: 1.0036 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00092: val_loss improved from 1.07187 to 1.00364, saving model to ./mod4.h5\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.5766 - accuracy: 0.9146 - val_loss: 1.0889 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.00364\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.5738 - accuracy: 0.9512 - val_loss: 1.4032 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.00364\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.5371 - accuracy: 0.9512 - val_loss: 1.0032 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00095: val_loss improved from 1.00364 to 1.00324, saving model to ./mod4.h5\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.5493 - accuracy: 0.9390 - val_loss: 1.0744 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.00324\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.5093 - accuracy: 0.9756 - val_loss: 1.0322 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.00324\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.5070 - accuracy: 0.9756 - val_loss: 1.0930 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.00324\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.5057 - accuracy: 0.9756 - val_loss: 0.9877 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00099: val_loss improved from 1.00324 to 0.98771, saving model to ./mod4.h5\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4831 - accuracy: 0.9756 - val_loss: 0.9282 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.98771 to 0.92821, saving model to ./mod4.h5\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 1s 370ms/step - loss: 0.4761 - accuracy: 0.9756 - val_loss: 1.0029 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.92821\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4729 - accuracy: 0.9756 - val_loss: 1.0185 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.92821\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4728 - accuracy: 0.9756 - val_loss: 1.0054 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.92821\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4873 - accuracy: 0.9756 - val_loss: 0.9471 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.92821\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4825 - accuracy: 0.9756 - val_loss: 0.8892 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.92821 to 0.88918, saving model to ./mod4.h5\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.4749 - accuracy: 0.9634 - val_loss: 1.2588 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.88918\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4825 - accuracy: 0.9756 - val_loss: 0.9368 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.88918\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4693 - accuracy: 0.9756 - val_loss: 0.8899 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.88918\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4557 - accuracy: 0.9756 - val_loss: 1.0599 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.88918\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.4499 - accuracy: 0.9756 - val_loss: 0.9922 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.88918\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4487 - accuracy: 0.9756 - val_loss: 0.8984 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.88918\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.4506 - accuracy: 0.9756 - val_loss: 0.8664 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.88918 to 0.86643, saving model to ./mod4.h5\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4396 - accuracy: 0.9756 - val_loss: 0.8100 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.86643 to 0.81000, saving model to ./mod4.h5\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4415 - accuracy: 0.9756 - val_loss: 0.9022 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.81000\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4529 - accuracy: 0.9756 - val_loss: 0.9606 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.81000\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4365 - accuracy: 0.9878 - val_loss: 0.7786 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.81000 to 0.77856, saving model to ./mod4.h5\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4522 - accuracy: 0.9878 - val_loss: 1.1850 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.77856\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4566 - accuracy: 0.9756 - val_loss: 0.8429 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.77856\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4397 - accuracy: 0.9878 - val_loss: 0.7380 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.77856 to 0.73802, saving model to ./mod4.h5\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4441 - accuracy: 0.9756 - val_loss: 0.9807 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.73802\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4362 - accuracy: 0.9756 - val_loss: 0.7254 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.73802 to 0.72539, saving model to ./mod4.h5\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4302 - accuracy: 1.0000 - val_loss: 1.0201 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.72539\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4480 - accuracy: 0.9756 - val_loss: 0.8049 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.72539\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4123 - accuracy: 1.0000 - val_loss: 0.7498 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.72539\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.4233 - accuracy: 0.9878 - val_loss: 0.8813 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.72539\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4212 - accuracy: 0.9878 - val_loss: 0.9334 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.72539\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4170 - accuracy: 0.9878 - val_loss: 0.7652 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.72539\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.4103 - accuracy: 1.0000 - val_loss: 0.8212 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.72539\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4088 - accuracy: 1.0000 - val_loss: 0.7779 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.72539\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4082 - accuracy: 1.0000 - val_loss: 0.7344 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.72539\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4031 - accuracy: 1.0000 - val_loss: 0.7368 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.72539\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.4037 - accuracy: 1.0000 - val_loss: 0.7208 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.72539 to 0.72085, saving model to ./mod4.h5\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.4013 - accuracy: 1.0000 - val_loss: 0.7178 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.72085 to 0.71781, saving model to ./mod4.h5\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4040 - accuracy: 1.0000 - val_loss: 0.9916 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.71781\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.4357 - accuracy: 0.9756 - val_loss: 0.7352 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.71781\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4016 - accuracy: 1.0000 - val_loss: 0.7306 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.71781\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3961 - accuracy: 1.0000 - val_loss: 0.8801 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.71781\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3996 - accuracy: 1.0000 - val_loss: 0.7076 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.71781 to 0.70756, saving model to ./mod4.h5\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4122 - accuracy: 1.0000 - val_loss: 0.7568 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.70756\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3892 - accuracy: 1.0000 - val_loss: 0.8583 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.70756\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.4064 - accuracy: 0.9878 - val_loss: 0.7634 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.70756\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3967 - accuracy: 1.0000 - val_loss: 0.7482 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.70756\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3961 - accuracy: 1.0000 - val_loss: 0.7658 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.70756\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3945 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.70756 to 0.68507, saving model to ./mod4.h5\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4017 - accuracy: 1.0000 - val_loss: 1.0204 - val_accuracy: 0.6429\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.68507\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.4006 - accuracy: 1.0000 - val_loss: 0.7506 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.68507\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3963 - accuracy: 1.0000 - val_loss: 0.6709 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.68507 to 0.67093, saving model to ./mod4.h5\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3959 - accuracy: 1.0000 - val_loss: 0.7999 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.67093\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4006 - accuracy: 0.9878 - val_loss: 0.8151 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.67093\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 0.3854 - accuracy: 1.0000 - val_loss: 0.6809 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.67093\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.4016 - accuracy: 1.0000 - val_loss: 0.9012 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.67093\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.4138 - accuracy: 0.9878 - val_loss: 0.7821 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.67093\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3862 - accuracy: 1.0000 - val_loss: 0.6699 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.67093 to 0.66991, saving model to ./mod4.h5\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3904 - accuracy: 1.0000 - val_loss: 0.7306 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.66991\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3884 - accuracy: 1.0000 - val_loss: 0.7878 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.66991\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3848 - accuracy: 1.0000 - val_loss: 0.6482 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00156: val_loss improved from 0.66991 to 0.64824, saving model to ./mod4.h5\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3930 - accuracy: 1.0000 - val_loss: 0.7714 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.64824\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3876 - accuracy: 1.0000 - val_loss: 0.8994 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.64824\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3837 - accuracy: 1.0000 - val_loss: 0.6460 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.64824 to 0.64603, saving model to ./mod4.h5\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3958 - accuracy: 1.0000 - val_loss: 0.7515 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.64603\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3824 - accuracy: 1.0000 - val_loss: 0.7465 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.64603\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3799 - accuracy: 1.0000 - val_loss: 0.7057 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.64603\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3768 - accuracy: 1.0000 - val_loss: 0.7094 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.64603\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3752 - accuracy: 1.0000 - val_loss: 0.6904 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.64603\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3789 - accuracy: 1.0000 - val_loss: 0.6574 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.64603\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3762 - accuracy: 1.0000 - val_loss: 0.7064 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.64603\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3763 - accuracy: 1.0000 - val_loss: 0.7368 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.64603\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3753 - accuracy: 1.0000 - val_loss: 0.6762 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.64603\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3774 - accuracy: 1.0000 - val_loss: 0.6420 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.64603 to 0.64202, saving model to ./mod4.h5\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.3760 - accuracy: 1.0000 - val_loss: 0.8274 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.64202\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3827 - accuracy: 1.0000 - val_loss: 0.8108 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.64202\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3751 - accuracy: 1.0000 - val_loss: 0.6392 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00172: val_loss improved from 0.64202 to 0.63917, saving model to ./mod4.h5\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 0.3736 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.63917 to 0.63739, saving model to ./mod4.h5\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3769 - accuracy: 1.0000 - val_loss: 0.7332 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.63739\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.3737 - accuracy: 1.0000 - val_loss: 0.7737 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.63739\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3746 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.63739\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3738 - accuracy: 1.0000 - val_loss: 0.6599 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.63739\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 0.3724 - accuracy: 1.0000 - val_loss: 0.7135 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.63739\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3715 - accuracy: 1.0000 - val_loss: 0.6773 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.63739\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.3671 - accuracy: 1.0000 - val_loss: 0.6723 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.63739\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3686 - accuracy: 1.0000 - val_loss: 0.7045 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.63739\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3674 - accuracy: 1.0000 - val_loss: 0.6761 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.63739\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3688 - accuracy: 1.0000 - val_loss: 0.6516 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.63739\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3692 - accuracy: 1.0000 - val_loss: 0.6820 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.63739\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3679 - accuracy: 1.0000 - val_loss: 0.6453 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.63739\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3686 - accuracy: 1.0000 - val_loss: 0.6647 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.63739\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3676 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.63739 to 0.60814, saving model to ./mod4.h5\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3655 - accuracy: 1.0000 - val_loss: 0.6323 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.60814\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3692 - accuracy: 1.0000 - val_loss: 0.6288 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.60814\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3652 - accuracy: 1.0000 - val_loss: 0.6445 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.60814\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.3649 - accuracy: 1.0000 - val_loss: 0.6187 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.60814\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3656 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.60814\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3618 - accuracy: 1.0000 - val_loss: 0.6690 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.60814\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3638 - accuracy: 1.0000 - val_loss: 0.6535 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.60814\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.3648 - accuracy: 1.0000 - val_loss: 0.6302 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.60814\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3613 - accuracy: 1.0000 - val_loss: 0.6611 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.60814\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3670 - accuracy: 1.0000 - val_loss: 0.6494 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.60814\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3677 - accuracy: 1.0000 - val_loss: 0.6888 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.60814\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3654 - accuracy: 1.0000 - val_loss: 0.6630 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.60814\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.3651 - accuracy: 1.0000 - val_loss: 0.6271 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.60814\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 108ms/step - loss: 0.3648 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.4696 - accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.6081 - accuracy: 0.9286\n",
            "(None, 19, 19, 19, 1)\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 19, 19, 19,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_665 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_667 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_669 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_671 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_673 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_675 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_677 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_679 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_681 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_683 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_685 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_687 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_689 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_691 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_693 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_695 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_697 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_699 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_701 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_703 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_705 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_707 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_709 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_711 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_713 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_715 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_717 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_719 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_721 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_723 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_725 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_727 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_729 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_731 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_733 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_735 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_737 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_739 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_741 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_743 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_745 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_747 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_749 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_751 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_753 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_755 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_757 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_759 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_761 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_763 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_765 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_767 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_769 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_771 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_773 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_775 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_777 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_779 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_781 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_783 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_785 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_787 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_789 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_791 (Lambda)             (None, 19, 5, 19, 1) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_664 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_665[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_666 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_667[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_668 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_669[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_670 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_671[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_672 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_673[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_674 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_675[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_676 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_677[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_678 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_679[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_680 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_681[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_682 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_683[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_684 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_685[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_686 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_687[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_688 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_689[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_690 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_691[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_692 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_693[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_694 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_695[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_696 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_697[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_698 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_699[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_700 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_701[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_702 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_703[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_704 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_705[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_706 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_707[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_708 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_709[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_710 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_711[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_712 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_713[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_714 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_715[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_716 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_717[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_718 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_719[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_720 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_721[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_722 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_723[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_724 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_725[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_726 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_727[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_728 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_729[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_730 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_731[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_732 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_733[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_734 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_735[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_736 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_737[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_738 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_739[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_740 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_741[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_742 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_743[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_744 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_745[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_746 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_747[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_748 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_749[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_750 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_751[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_752 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_753[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_754 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_755[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_756 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_757[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_758 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_759[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_760 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_761[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_762 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_763[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_764 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_765[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_766 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_767[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_768 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_769[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_770 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_771[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_772 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_773[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_774 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_775[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_776 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_777[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_778 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_779[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_780 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_781[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_782 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_783[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_784 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_785[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_786 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_787[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_788 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_789[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_790 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_791[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_332 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_664[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_333 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_666[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_334 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_668[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_335 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_670[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_336 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_672[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_337 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_674[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_338 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_676[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_339 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_678[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_340 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_680[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_341 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_682[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_342 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_684[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_343 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_686[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_344 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_688[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_345 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_690[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_346 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_692[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_347 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_694[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_348 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_696[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_349 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_698[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_350 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_700[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_351 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_702[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_352 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_704[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_353 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_706[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_354 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_708[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_355 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_710[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_356 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_712[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_357 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_714[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_358 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_716[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_359 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_718[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_360 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_720[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_361 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_722[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_362 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_724[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_363 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_726[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_364 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_728[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_365 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_730[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_366 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_732[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_367 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_734[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_368 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_736[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_369 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_738[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_370 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_740[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_371 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_742[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_372 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_744[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_373 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_746[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_374 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_748[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_375 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_750[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_376 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_752[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_377 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_754[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_378 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_756[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_379 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_758[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_380 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_760[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_381 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_762[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_382 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_764[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_383 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_766[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_384 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_768[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_385 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_770[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_386 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_772[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_387 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_774[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_388 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_776[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_389 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_778[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_390 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_780[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_391 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_782[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_392 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_784[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_393 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_786[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_394 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_788[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_395 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_790[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_332 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_332[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_333 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_333[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_334 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_334[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_335 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_335[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_336 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_336[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_337 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_337[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_338 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_338[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_339 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_339[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_340 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_340[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_341 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_341[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_342 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_342[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_343 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_343[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_344 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_344[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_345 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_345[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_346 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_346[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_347 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_347[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_348 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_348[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_349 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_349[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_350 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_350[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_351 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_351[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_352 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_352[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_353 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_353[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_354 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_354[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_355 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_355[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_356 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_356[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_357 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_357[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_358 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_358[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_359 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_359[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_360 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_360[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_361 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_361[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_362 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_362[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_363 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_363[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_364 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_364[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_365 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_365[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_366 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_366[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_367 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_368 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_369 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_369[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_370 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_371 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_372 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_373 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_374 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_375 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_376 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_377 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_377[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_378 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_378[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_379 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_379[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_380 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_380[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_381 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_381[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_382 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_382[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_383 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_383[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_384 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_384[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_385 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_385[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_386 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_386[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_387 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_387[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_388 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_388[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_389 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_389[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_390 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_390[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_391 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_391[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_392 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_392[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_393 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_393[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_394 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_394[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_395 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_395[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_332 (G (None, 8)            0           dropout_332[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_333 (G (None, 8)            0           dropout_333[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_334 (G (None, 8)            0           dropout_334[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_335 (G (None, 8)            0           dropout_335[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_336 (G (None, 8)            0           dropout_336[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_337 (G (None, 8)            0           dropout_337[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_338 (G (None, 8)            0           dropout_338[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_339 (G (None, 8)            0           dropout_339[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_340 (G (None, 8)            0           dropout_340[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_341 (G (None, 8)            0           dropout_341[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_342 (G (None, 8)            0           dropout_342[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_343 (G (None, 8)            0           dropout_343[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_344 (G (None, 8)            0           dropout_344[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_345 (G (None, 8)            0           dropout_345[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_346 (G (None, 8)            0           dropout_346[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_347 (G (None, 8)            0           dropout_347[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_348 (G (None, 8)            0           dropout_348[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_349 (G (None, 8)            0           dropout_349[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_350 (G (None, 8)            0           dropout_350[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_351 (G (None, 8)            0           dropout_351[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_352 (G (None, 8)            0           dropout_352[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_353 (G (None, 8)            0           dropout_353[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_354 (G (None, 8)            0           dropout_354[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_355 (G (None, 8)            0           dropout_355[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_356 (G (None, 8)            0           dropout_356[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_357 (G (None, 8)            0           dropout_357[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_358 (G (None, 8)            0           dropout_358[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_359 (G (None, 8)            0           dropout_359[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_360 (G (None, 8)            0           dropout_360[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_361 (G (None, 8)            0           dropout_361[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_362 (G (None, 8)            0           dropout_362[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_363 (G (None, 8)            0           dropout_363[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_364 (G (None, 8)            0           dropout_364[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_365 (G (None, 8)            0           dropout_365[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_366 (G (None, 8)            0           dropout_366[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_367 (G (None, 8)            0           dropout_367[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_368 (G (None, 8)            0           dropout_368[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_369 (G (None, 8)            0           dropout_369[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_370 (G (None, 8)            0           dropout_370[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_371 (G (None, 8)            0           dropout_371[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_372 (G (None, 8)            0           dropout_372[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_373 (G (None, 8)            0           dropout_373[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_374 (G (None, 8)            0           dropout_374[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_375 (G (None, 8)            0           dropout_375[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_376 (G (None, 8)            0           dropout_376[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_377 (G (None, 8)            0           dropout_377[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_378 (G (None, 8)            0           dropout_378[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_379 (G (None, 8)            0           dropout_379[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_380 (G (None, 8)            0           dropout_380[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_381 (G (None, 8)            0           dropout_381[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_382 (G (None, 8)            0           dropout_382[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_383 (G (None, 8)            0           dropout_383[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_384 (G (None, 8)            0           dropout_384[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_385 (G (None, 8)            0           dropout_385[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_386 (G (None, 8)            0           dropout_386[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_387 (G (None, 8)            0           dropout_387[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_388 (G (None, 8)            0           dropout_388[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_389 (G (None, 8)            0           dropout_389[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_390 (G (None, 8)            0           dropout_390[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_391 (G (None, 8)            0           dropout_391[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_392 (G (None, 8)            0           dropout_392[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_393 (G (None, 8)            0           dropout_393[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_394 (G (None, 8)            0           dropout_394[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_395 (G (None, 8)            0           dropout_395[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 512)          0           global_average_pooling3d_332[0][0\n",
            "                                                                 global_average_pooling3d_333[0][0\n",
            "                                                                 global_average_pooling3d_334[0][0\n",
            "                                                                 global_average_pooling3d_335[0][0\n",
            "                                                                 global_average_pooling3d_336[0][0\n",
            "                                                                 global_average_pooling3d_337[0][0\n",
            "                                                                 global_average_pooling3d_338[0][0\n",
            "                                                                 global_average_pooling3d_339[0][0\n",
            "                                                                 global_average_pooling3d_340[0][0\n",
            "                                                                 global_average_pooling3d_341[0][0\n",
            "                                                                 global_average_pooling3d_342[0][0\n",
            "                                                                 global_average_pooling3d_343[0][0\n",
            "                                                                 global_average_pooling3d_344[0][0\n",
            "                                                                 global_average_pooling3d_345[0][0\n",
            "                                                                 global_average_pooling3d_346[0][0\n",
            "                                                                 global_average_pooling3d_347[0][0\n",
            "                                                                 global_average_pooling3d_348[0][0\n",
            "                                                                 global_average_pooling3d_349[0][0\n",
            "                                                                 global_average_pooling3d_350[0][0\n",
            "                                                                 global_average_pooling3d_351[0][0\n",
            "                                                                 global_average_pooling3d_352[0][0\n",
            "                                                                 global_average_pooling3d_353[0][0\n",
            "                                                                 global_average_pooling3d_354[0][0\n",
            "                                                                 global_average_pooling3d_355[0][0\n",
            "                                                                 global_average_pooling3d_356[0][0\n",
            "                                                                 global_average_pooling3d_357[0][0\n",
            "                                                                 global_average_pooling3d_358[0][0\n",
            "                                                                 global_average_pooling3d_359[0][0\n",
            "                                                                 global_average_pooling3d_360[0][0\n",
            "                                                                 global_average_pooling3d_361[0][0\n",
            "                                                                 global_average_pooling3d_362[0][0\n",
            "                                                                 global_average_pooling3d_363[0][0\n",
            "                                                                 global_average_pooling3d_364[0][0\n",
            "                                                                 global_average_pooling3d_365[0][0\n",
            "                                                                 global_average_pooling3d_366[0][0\n",
            "                                                                 global_average_pooling3d_367[0][0\n",
            "                                                                 global_average_pooling3d_368[0][0\n",
            "                                                                 global_average_pooling3d_369[0][0\n",
            "                                                                 global_average_pooling3d_370[0][0\n",
            "                                                                 global_average_pooling3d_371[0][0\n",
            "                                                                 global_average_pooling3d_372[0][0\n",
            "                                                                 global_average_pooling3d_373[0][0\n",
            "                                                                 global_average_pooling3d_374[0][0\n",
            "                                                                 global_average_pooling3d_375[0][0\n",
            "                                                                 global_average_pooling3d_376[0][0\n",
            "                                                                 global_average_pooling3d_377[0][0\n",
            "                                                                 global_average_pooling3d_378[0][0\n",
            "                                                                 global_average_pooling3d_379[0][0\n",
            "                                                                 global_average_pooling3d_380[0][0\n",
            "                                                                 global_average_pooling3d_381[0][0\n",
            "                                                                 global_average_pooling3d_382[0][0\n",
            "                                                                 global_average_pooling3d_383[0][0\n",
            "                                                                 global_average_pooling3d_384[0][0\n",
            "                                                                 global_average_pooling3d_385[0][0\n",
            "                                                                 global_average_pooling3d_386[0][0\n",
            "                                                                 global_average_pooling3d_387[0][0\n",
            "                                                                 global_average_pooling3d_388[0][0\n",
            "                                                                 global_average_pooling3d_389[0][0\n",
            "                                                                 global_average_pooling3d_390[0][0\n",
            "                                                                 global_average_pooling3d_391[0][0\n",
            "                                                                 global_average_pooling3d_392[0][0\n",
            "                                                                 global_average_pooling3d_393[0][0\n",
            "                                                                 global_average_pooling3d_394[0][0\n",
            "                                                                 global_average_pooling3d_395[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 512)          262656      concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 512)          262656      dense_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 512)          262656      dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 1)            513         dense_26[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 802,817\n",
            "Trainable params: 802,817\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 12s 999ms/step - loss: 99.1745 - accuracy: 0.5422 - val_loss: 93.4172 - val_accuracy: 0.2308\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.41722, saving model to ./mod5.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 91.6273 - accuracy: 0.5422 - val_loss: 86.1115 - val_accuracy: 0.2308\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.41722 to 86.11150, saving model to ./mod5.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 84.3543 - accuracy: 0.5301 - val_loss: 79.0220 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.11150 to 79.02205, saving model to ./mod5.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 77.4809 - accuracy: 0.4699 - val_loss: 72.3503 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.02205 to 72.35035, saving model to ./mod5.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 70.8961 - accuracy: 0.4940 - val_loss: 66.0569 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.35035 to 66.05692, saving model to ./mod5.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 64.6284 - accuracy: 0.6145 - val_loss: 60.0554 - val_accuracy: 0.3077\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.05692 to 60.05537, saving model to ./mod5.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 58.6657 - accuracy: 0.6024 - val_loss: 54.3421 - val_accuracy: 0.3077\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.05537 to 54.34214, saving model to ./mod5.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 53.0053 - accuracy: 0.6145 - val_loss: 48.8983 - val_accuracy: 0.3846\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.34214 to 48.89833, saving model to ./mod5.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 47.6364 - accuracy: 0.5904 - val_loss: 43.7505 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.89833 to 43.75048, saving model to ./mod5.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 42.5608 - accuracy: 0.6386 - val_loss: 38.9699 - val_accuracy: 0.3077\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.75048 to 38.96992, saving model to ./mod5.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 37.7963 - accuracy: 0.5904 - val_loss: 34.4040 - val_accuracy: 0.3846\n",
            "\n",
            "Epoch 00011: val_loss improved from 38.96992 to 34.40403, saving model to ./mod5.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 33.3183 - accuracy: 0.6747 - val_loss: 30.0712 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.40403 to 30.07124, saving model to ./mod5.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 29.1639 - accuracy: 0.6506 - val_loss: 26.1219 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.07124 to 26.12186, saving model to ./mod5.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 25.2901 - accuracy: 0.6506 - val_loss: 22.4976 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.12186 to 22.49756, saving model to ./mod5.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 21.7060 - accuracy: 0.6145 - val_loss: 19.2541 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.49756 to 19.25408, saving model to ./mod5.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 18.4443 - accuracy: 0.6506 - val_loss: 16.2022 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.25408 to 16.20218, saving model to ./mod5.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 399ms/step - loss: 15.4694 - accuracy: 0.6265 - val_loss: 13.4253 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.20218 to 13.42532, saving model to ./mod5.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 12.8034 - accuracy: 0.6265 - val_loss: 11.0532 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.42532 to 11.05323, saving model to ./mod5.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 10.4397 - accuracy: 0.6506 - val_loss: 8.8724 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.05323 to 8.87239, saving model to ./mod5.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 8.3640 - accuracy: 0.6988 - val_loss: 6.9482 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.87239 to 6.94822, saving model to ./mod5.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 6.6181 - accuracy: 0.6627 - val_loss: 5.4672 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00021: val_loss improved from 6.94822 to 5.46717, saving model to ./mod5.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 5.1450 - accuracy: 0.6386 - val_loss: 4.2896 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.46717 to 4.28960, saving model to ./mod5.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 3.9984 - accuracy: 0.6506 - val_loss: 3.3068 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.28960 to 3.30678, saving model to ./mod5.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 3.1528 - accuracy: 0.6988 - val_loss: 2.7653 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.30678 to 2.76528, saving model to ./mod5.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 2.5994 - accuracy: 0.6506 - val_loss: 2.3738 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.76528 to 2.37378, saving model to ./mod5.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 2.3543 - accuracy: 0.6145 - val_loss: 2.2657 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.37378 to 2.26572, saving model to ./mod5.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 2.2761 - accuracy: 0.7229 - val_loss: 2.3375 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.26572\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 2.1317 - accuracy: 0.6145 - val_loss: 1.8707 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.26572 to 1.87072, saving model to ./mod5.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 1.8005 - accuracy: 0.7470 - val_loss: 1.5733 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.87072 to 1.57325, saving model to ./mod5.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 1.5634 - accuracy: 0.6747 - val_loss: 1.5786 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.57325\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 1.4481 - accuracy: 0.6867 - val_loss: 1.3756 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.57325 to 1.37559, saving model to ./mod5.h5\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 1.3797 - accuracy: 0.7229 - val_loss: 1.3157 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.37559 to 1.31572, saving model to ./mod5.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 1.2542 - accuracy: 0.7108 - val_loss: 1.1122 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.31572 to 1.11222, saving model to ./mod5.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 1.2038 - accuracy: 0.6867 - val_loss: 1.0757 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.11222 to 1.07569, saving model to ./mod5.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 1.1714 - accuracy: 0.6145 - val_loss: 1.0350 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.07569 to 1.03500, saving model to ./mod5.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.0839 - accuracy: 0.8072 - val_loss: 1.1865 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.03500\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 1.0758 - accuracy: 0.7229 - val_loss: 0.9967 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.03500 to 0.99673, saving model to ./mod5.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 1.0209 - accuracy: 0.8072 - val_loss: 0.9048 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.99673 to 0.90475, saving model to ./mod5.h5\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 1.0027 - accuracy: 0.7590 - val_loss: 1.0175 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.90475\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.9729 - accuracy: 0.7590 - val_loss: 0.8546 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.90475 to 0.85463, saving model to ./mod5.h5\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.9102 - accuracy: 0.8072 - val_loss: 0.8575 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.85463\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.9057 - accuracy: 0.8072 - val_loss: 0.9057 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.85463\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.9074 - accuracy: 0.7590 - val_loss: 0.7624 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.85463 to 0.76240, saving model to ./mod5.h5\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.8721 - accuracy: 0.7831 - val_loss: 0.7890 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.76240\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.8475 - accuracy: 0.8193 - val_loss: 0.7632 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.76240\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.8577 - accuracy: 0.8072 - val_loss: 0.7380 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.76240 to 0.73800, saving model to ./mod5.h5\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.8080 - accuracy: 0.8313 - val_loss: 0.7930 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.73800\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.7972 - accuracy: 0.8434 - val_loss: 0.6757 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.73800 to 0.67571, saving model to ./mod5.h5\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.8113 - accuracy: 0.8193 - val_loss: 0.7050 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67571\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.7917 - accuracy: 0.8554 - val_loss: 0.7488 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67571\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.8105 - accuracy: 0.8072 - val_loss: 0.7252 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67571\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.7751 - accuracy: 0.8675 - val_loss: 0.6543 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.67571 to 0.65426, saving model to ./mod5.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.7247 - accuracy: 0.9277 - val_loss: 0.7823 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.65426\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.7737 - accuracy: 0.8434 - val_loss: 0.6418 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.65426 to 0.64182, saving model to ./mod5.h5\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.7571 - accuracy: 0.8434 - val_loss: 0.6493 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.64182\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.7200 - accuracy: 0.8795 - val_loss: 0.6653 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.64182\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.6963 - accuracy: 0.9157 - val_loss: 0.6169 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.64182 to 0.61688, saving model to ./mod5.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.7099 - accuracy: 0.8675 - val_loss: 0.7353 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.61688\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.7473 - accuracy: 0.8554 - val_loss: 0.6074 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.61688 to 0.60745, saving model to ./mod5.h5\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.7700 - accuracy: 0.7952 - val_loss: 0.6766 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.60745\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.7965 - accuracy: 0.8193 - val_loss: 0.6297 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.60745\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.6782 - accuracy: 0.9277 - val_loss: 0.6448 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.60745\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.7109 - accuracy: 0.8916 - val_loss: 0.7922 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.60745\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.7717 - accuracy: 0.8313 - val_loss: 0.6334 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.60745\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.7100 - accuracy: 0.8675 - val_loss: 0.6039 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.60745 to 0.60388, saving model to ./mod5.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.6765 - accuracy: 0.9157 - val_loss: 0.6672 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.60388\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.6614 - accuracy: 0.9036 - val_loss: 0.6078 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.60388\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 433ms/step - loss: 0.6659 - accuracy: 0.8795 - val_loss: 0.6399 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.60388\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.7167 - accuracy: 0.8675 - val_loss: 0.6147 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.60388\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.6141 - accuracy: 0.9277 - val_loss: 0.6130 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.60388\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6620 - accuracy: 0.8795 - val_loss: 0.6375 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.60388\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.6350 - accuracy: 0.9157 - val_loss: 0.5767 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.60388 to 0.57670, saving model to ./mod5.h5\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.5897 - accuracy: 0.9277 - val_loss: 0.5909 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.57670\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.5972 - accuracy: 0.9277 - val_loss: 0.5798 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.57670\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.5849 - accuracy: 0.9277 - val_loss: 0.5571 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.57670 to 0.55709, saving model to ./mod5.h5\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.5628 - accuracy: 0.9759 - val_loss: 0.5847 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.55709\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.5870 - accuracy: 0.9277 - val_loss: 0.5559 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.55709 to 0.55589, saving model to ./mod5.h5\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.5890 - accuracy: 0.9398 - val_loss: 0.5862 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.55589\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6457 - accuracy: 0.9157 - val_loss: 0.5421 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.55589 to 0.54212, saving model to ./mod5.h5\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.5366 - accuracy: 0.9759 - val_loss: 0.5817 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.54212\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.5700 - accuracy: 0.9639 - val_loss: 0.5764 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.54212\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.5891 - accuracy: 0.9157 - val_loss: 0.5724 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.54212\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.6076 - accuracy: 0.8916 - val_loss: 0.5525 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.54212\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.6095 - accuracy: 0.9157 - val_loss: 0.5425 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.54212\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.6523 - accuracy: 0.8795 - val_loss: 0.5563 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.54212\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.5634 - accuracy: 0.9518 - val_loss: 0.7068 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.54212\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.6313 - accuracy: 0.9157 - val_loss: 0.5572 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.54212\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.5912 - accuracy: 0.9157 - val_loss: 0.5173 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.54212 to 0.51733, saving model to ./mod5.h5\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.5368 - accuracy: 0.9518 - val_loss: 0.5571 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.51733\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.5415 - accuracy: 0.9398 - val_loss: 0.5280 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.51733\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.5232 - accuracy: 0.9759 - val_loss: 0.5124 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.51733 to 0.51239, saving model to ./mod5.h5\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.5049 - accuracy: 0.9639 - val_loss: 0.5190 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.51239\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.5028 - accuracy: 0.9639 - val_loss: 0.5092 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.51239 to 0.50924, saving model to ./mod5.h5\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4853 - accuracy: 0.9759 - val_loss: 0.5081 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.50924 to 0.50814, saving model to ./mod5.h5\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4868 - accuracy: 0.9880 - val_loss: 0.5045 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.50814 to 0.50451, saving model to ./mod5.h5\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.4892 - accuracy: 0.9759 - val_loss: 0.5121 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.50451\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4894 - accuracy: 0.9518 - val_loss: 0.5323 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.50451\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.5119 - accuracy: 0.9759 - val_loss: 0.4990 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.50451 to 0.49898, saving model to ./mod5.h5\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4927 - accuracy: 0.9639 - val_loss: 0.4994 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.49898\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.4991 - accuracy: 0.9759 - val_loss: 0.4874 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.49898 to 0.48741, saving model to ./mod5.h5\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4675 - accuracy: 0.9759 - val_loss: 0.5601 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.48741\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.5408 - accuracy: 0.9398 - val_loss: 0.5575 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.48741\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4763 - accuracy: 1.0000 - val_loss: 0.5124 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.48741\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.5069 - accuracy: 0.9639 - val_loss: 0.4924 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.48741\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4504 - accuracy: 0.9880 - val_loss: 0.4921 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.48741\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4457 - accuracy: 1.0000 - val_loss: 0.5280 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.48741\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.4651 - accuracy: 0.9880 - val_loss: 0.5282 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.48741\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4521 - accuracy: 1.0000 - val_loss: 0.4889 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.48741\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4422 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.48741 to 0.48374, saving model to ./mod5.h5\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4401 - accuracy: 0.9880 - val_loss: 0.4732 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.48374 to 0.47317, saving model to ./mod5.h5\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.4347 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.47317\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.4393 - accuracy: 1.0000 - val_loss: 0.4728 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.47317 to 0.47281, saving model to ./mod5.h5\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.4516 - accuracy: 0.9880 - val_loss: 0.5121 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.47281\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4837 - accuracy: 0.9639 - val_loss: 0.5221 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.47281\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.4380 - accuracy: 0.9880 - val_loss: 0.4768 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.47281\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4479 - accuracy: 0.9759 - val_loss: 0.5134 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.47281\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.4358 - accuracy: 1.0000 - val_loss: 0.5036 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.47281\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.4299 - accuracy: 1.0000 - val_loss: 0.4652 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.47281 to 0.46522, saving model to ./mod5.h5\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.4422 - accuracy: 0.9880 - val_loss: 0.5380 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.46522\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4590 - accuracy: 0.9759 - val_loss: 0.4973 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.46522\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.4203 - accuracy: 1.0000 - val_loss: 0.4997 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.46522\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4488 - accuracy: 0.9759 - val_loss: 0.5397 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.46522\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4552 - accuracy: 1.0000 - val_loss: 0.4871 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.46522\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4259 - accuracy: 0.9880 - val_loss: 0.4873 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.46522\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.4255 - accuracy: 1.0000 - val_loss: 0.5578 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.46522\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.4309 - accuracy: 1.0000 - val_loss: 0.4638 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.46522 to 0.46383, saving model to ./mod5.h5\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.4368 - accuracy: 1.0000 - val_loss: 0.4805 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.46383\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4360 - accuracy: 0.9880 - val_loss: 0.4890 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.46383\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 1s 404ms/step - loss: 0.4263 - accuracy: 1.0000 - val_loss: 0.4606 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.46383 to 0.46056, saving model to ./mod5.h5\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.4224 - accuracy: 0.9880 - val_loss: 0.5269 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.46056\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4241 - accuracy: 1.0000 - val_loss: 0.4611 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.46056\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4118 - accuracy: 1.0000 - val_loss: 0.4638 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.46056\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4095 - accuracy: 1.0000 - val_loss: 0.5106 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.46056\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4198 - accuracy: 1.0000 - val_loss: 0.4980 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.46056\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.4058 - accuracy: 1.0000 - val_loss: 0.4525 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.46056 to 0.45252, saving model to ./mod5.h5\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.4061 - accuracy: 1.0000 - val_loss: 0.4706 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.45252\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.4010 - accuracy: 1.0000 - val_loss: 0.4604 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.45252\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4007 - accuracy: 1.0000 - val_loss: 0.4646 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.45252\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.4078 - accuracy: 1.0000 - val_loss: 0.4758 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.45252\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.4062 - accuracy: 1.0000 - val_loss: 0.4525 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.45252 to 0.45249, saving model to ./mod5.h5\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4010 - accuracy: 1.0000 - val_loss: 0.5021 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.45249\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4008 - accuracy: 1.0000 - val_loss: 0.4371 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.45249 to 0.43711, saving model to ./mod5.h5\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4067 - accuracy: 1.0000 - val_loss: 0.4793 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.43711\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.4166 - accuracy: 0.9880 - val_loss: 0.4415 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.43711\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.4176 - accuracy: 1.0000 - val_loss: 0.4468 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.43711\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.4090 - accuracy: 1.0000 - val_loss: 0.5826 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.43711\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.4268 - accuracy: 1.0000 - val_loss: 0.4414 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.43711\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.4211 - accuracy: 0.9880 - val_loss: 0.4540 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.43711\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3960 - accuracy: 1.0000 - val_loss: 0.5056 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.43711\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3967 - accuracy: 1.0000 - val_loss: 0.4514 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.43711\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.3949 - accuracy: 1.0000 - val_loss: 0.4394 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.43711\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3909 - accuracy: 1.0000 - val_loss: 0.4620 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.43711\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3926 - accuracy: 1.0000 - val_loss: 0.4687 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.43711\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3863 - accuracy: 1.0000 - val_loss: 0.4443 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.43711\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.3929 - accuracy: 1.0000 - val_loss: 0.4532 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.43711\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3909 - accuracy: 1.0000 - val_loss: 0.5422 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.43711\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4020 - accuracy: 1.0000 - val_loss: 0.4321 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.43711 to 0.43207, saving model to ./mod5.h5\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.4020 - accuracy: 1.0000 - val_loss: 0.4565 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.43207\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.3920 - accuracy: 1.0000 - val_loss: 0.4816 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.43207\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 1s 399ms/step - loss: 0.3906 - accuracy: 1.0000 - val_loss: 0.4350 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.43207\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.3844 - accuracy: 1.0000 - val_loss: 0.4417 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.43207\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3830 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.43207\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.3815 - accuracy: 1.0000 - val_loss: 0.4691 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.43207\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3850 - accuracy: 1.0000 - val_loss: 0.4520 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.43207\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3844 - accuracy: 1.0000 - val_loss: 0.4297 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.43207 to 0.42968, saving model to ./mod5.h5\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3803 - accuracy: 1.0000 - val_loss: 0.4504 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.42968\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3851 - accuracy: 1.0000 - val_loss: 0.4987 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.42968\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3892 - accuracy: 1.0000 - val_loss: 0.4276 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.42968 to 0.42760, saving model to ./mod5.h5\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3867 - accuracy: 1.0000 - val_loss: 0.4402 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.42760\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3856 - accuracy: 1.0000 - val_loss: 0.4493 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.42760\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3831 - accuracy: 1.0000 - val_loss: 0.4278 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.42760\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.3770 - accuracy: 1.0000 - val_loss: 0.4376 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.42760\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.3782 - accuracy: 1.0000 - val_loss: 0.4559 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.42760\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3770 - accuracy: 1.0000 - val_loss: 0.4311 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.42760\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.3772 - accuracy: 1.0000 - val_loss: 0.4408 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.42760\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3781 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.42760 to 0.42116, saving model to ./mod5.h5\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.3865 - accuracy: 1.0000 - val_loss: 0.4323 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.42116\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.3788 - accuracy: 1.0000 - val_loss: 0.4900 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.42116\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3807 - accuracy: 1.0000 - val_loss: 0.4273 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.42116\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3760 - accuracy: 1.0000 - val_loss: 0.4385 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.42116\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 1s 405ms/step - loss: 0.3759 - accuracy: 1.0000 - val_loss: 0.4465 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.42116\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.3752 - accuracy: 1.0000 - val_loss: 0.4503 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.42116\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3742 - accuracy: 1.0000 - val_loss: 0.4558 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.42116\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3747 - accuracy: 1.0000 - val_loss: 0.4632 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.42116\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 1s 401ms/step - loss: 0.3737 - accuracy: 1.0000 - val_loss: 0.4484 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.42116\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.3712 - accuracy: 1.0000 - val_loss: 0.4300 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.42116\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3716 - accuracy: 1.0000 - val_loss: 0.4309 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.42116\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 1s 399ms/step - loss: 0.3699 - accuracy: 1.0000 - val_loss: 0.4427 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.42116\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3738 - accuracy: 1.0000 - val_loss: 0.4564 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.42116\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3712 - accuracy: 1.0000 - val_loss: 0.4329 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.42116\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3694 - accuracy: 1.0000 - val_loss: 0.4322 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.42116\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3720 - accuracy: 1.0000 - val_loss: 0.4431 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.42116\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.3691 - accuracy: 1.0000 - val_loss: 0.4484 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.42116\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3685 - accuracy: 1.0000 - val_loss: 0.4335 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.42116\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3769 - accuracy: 1.0000 - val_loss: 0.4520 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.42116\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3816 - accuracy: 1.0000 - val_loss: 0.5015 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.42116\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 1s 401ms/step - loss: 0.3769 - accuracy: 1.0000 - val_loss: 0.4160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss improved from 0.42116 to 0.41597, saving model to ./mod5.h5\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.3780 - accuracy: 1.0000 - val_loss: 0.4791 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.41597\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3762 - accuracy: 1.0000 - val_loss: 0.4406 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.41597\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.3716 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.41597\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3Sc9X3n8fd3LrpfLcu2bBnLBoO5BQMOuEC6CTQtkBDTJUBSuiHdnNJ20wbS0sRpz2nSc3L2kN30kpyWZElDS/YASQphYVNypRBOlkuxiYkNNhg7vsi2ZF1sSbY1mtt3/3geGWEk29JIM9Izn9c5OjPzXOb56pnRR7/5ze95HnN3REQkWmKlLkBERKafwl1EJIIU7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S5lw8z+xcw2lLoOkWJQuIuIRJDCXUQkghTuUrbMbLWZPWVmx8zskJk9aGYLT1jmc2b2ppmlzKzbzH5oZovCeUkz+7KZ7TGzETPbb2aPmVlFaX4jkbckSl2ASCmYWSvwDLAV+B2gDrgH+ImZrXH3tJl9DPgL4LPAq0ALcDVQGz7N54DbgPXAr4BFwPVAvHi/icj4FO5Srv4svP0tdx8EMLPtwAvATcDDwGXAj9393jHrfW/M/cuAh9z9gTHTvjtzJYucPnXLSLkaDe7B0Qnu/iKwC7gqnLQJuN7M/trMLjOzE1vkm4CPm9lnzOxdZmbFKFzkdCjcpVy1Ad3jTO8G5oX37yfolrkFeBHoNrMvjgn5LwL/CPw34BVgr5ndOaNVi5wmhbuUqwPAgnGmLwT6Adw97+5/5+7nAmcAXyboZ//9cH7K3f/K3TuAs4HvAH9vZtcWoX6Rk1K4S7l6EfgtM6sfnWBm7wY6gJ+fuLC773X3e4A3gfPGmb8duBsYGW++SLHpC1UpV38L/BHwIzP7Em+NltkMPApgZv+LoBX/AjAAvA9YSTB6BjN7DNgI/AIYBj5M8Df1bDF/EZHxKNylLLl7j5m9D/gbgpExaeBJ4NPung4Xe56gC+YPgCqCVvvvu/v/Cec/B9wK/DnBp+DXgJvcXac4kJIzXWZPRCR61OcuIhJBCncRkQhSuIuIRJDCXUQkgmbFaJn58+d7R0dHqcsQEZlTNm7c2OvurePNmxXh3tHRwYYNGj0mIjIZZrZ7onnqlhERiSCFu4hIBCncRUQi6JR97mZ2P/BB4KC7XxBOm0dwBrwOgvNf3+Luh8LzWX+F4Go0x4CPu/vLM1O6iJS7TCZDZ2cnqVSq1KXMqKqqKtrb20kmk6e9zul8ofovwD8A3xozbT3wlLvfY2brw8efBa4jOLHSSuBy4GvhrYjItOvs7KS+vp6Ojg6ieq0Ud6evr4/Ozk6WL19+2uudslvG3Z8lPL/1GOuA0UuLPQDcOGb6tzzwAtBkZm2nXY2IyCSkUilaWloiG+wAZkZLS8ukP51Mtc99obsfCO93EVzgAGAJsHfMcp3hNBGRGRHlYB81ld+x4C9UPTit5KRPLWlmd5jZBjPb0NPTM6Vtv7Srny/9cBs6s6WIyNtNNdy7R7tbwtuD4fR9wNIxy7WH097B3e9z9zXuvqa1ddwDrE7pl50DfO2ZHQwMZ6a0vohIIQ4fPsy999476fWuv/56Dh8+PAMVvWWq4f4EcHt4/3bg8THTP2aBtcDAmO6babewoRKArsFof1MuIrPTROGezWZPut6TTz5JU1PTTJUFnN5QyIeB9wLzzawT+DzB5ci+a2afAHYTXB0egivZXE9wxZpjwO/NQM3HLWqoAqB7cIRVi2ZySyIi77R+/Xp27NjB6tWrSSaTVFVV0dzczLZt23jjjTe48cYb2bt3L6lUijvvvJM77rgDeOuUK0eOHOG6667jqquu4rnnnmPJkiU8/vjjVFdXF1zbKcPd3T86waxrxlnWgU8WWtTpWng83NVyFyl3f/1/X+W1/YPT+pznLW7g8zecP+H8e+65hy1btrBp0yaeeeYZPvCBD7Bly5bjQxbvv/9+5s2bx/DwMO9+97u56aabaGlpedtzbN++nYcffphvfOMb3HLLLTz66KP87u/+bsG1z4oTh01Va33QLdM9oHAXkdK77LLL3jYW/atf/SqPPfYYAHv37mX79u3vCPfly5ezevVqAC699FJ27do1LbXM6XCvSsZprknSPaRwFyl3J2thF0ttbe3x+8888ww//elPef7556mpqeG9733vuGPVKysrj9+Px+MMDw9PSy1z/twyCxuq6BoYKXUZIlKG6uvrGRoaGnfewMAAzc3N1NTUsG3bNl544YWi1janW+4QhPtBtdxFpARaWlq48sorueCCC6iurmbhwoXH51177bV8/etf59xzz+Wcc85h7dq1Ra0tAuFeydYD0/sliojI6XrooYfGnV5ZWckPfvCDceeN9qvPnz+fLVu2HJ9+9913T1tdkeiW6T0yQjaXL3UpIiKzxpwP90V1cfIOfUfTpS5FRGTWmNvh/v++wu/85N1UkKFLwyFFRI6b2+Fe3Yx5ngV2WAcyiYiMMbfDvX4xAIvoU7iLiIwxt8O9IQj3xfHDdA9qrLuIyKg5Hu7BRZ7OrBzUmSFFZNarq6sr2rbmdrhXNUGimmXJAXXLiIiMMbcPYjKDhjaWjBzioLplRKTI1q9fz9KlS/nkJ4OT4X7hC18gkUjw9NNPc+jQITKZDF/84hdZt25d0Wub2+EOUL+Y1pF+dcuIlLsfrIeuzdP7nIsuhOvumXD2rbfeyl133XU83L/73e/yox/9iE996lM0NDTQ29vL2rVr+dCHPlT0a73O/XBvWMy87p0MDGdIZXJUJeOlrkhEysTFF1/MwYMH2b9/Pz09PTQ3N7No0SI+/elP8+yzzxKLxdi3bx/d3d0sWlTcKwpFINzbqE33AE73YIplLbWnXEVEIugkLeyZdPPNN/PII4/Q1dXFrbfeyoMPPkhPTw8bN24kmUzS0dEx7ql+Z9rc/kIVoH4x8XyGeQxpOKSIFN2tt97Kt7/9bR555BFuvvlmBgYGWLBgAclkkqeffprdu3eXpK5ItNwBFlm/RsyISNGdf/75DA0NsWTJEtra2rjtttu44YYbuPDCC1mzZg2rVq0qSV1zP9zDo1QX2iGFu4iUxObNb32RO3/+fJ5//vlxlzty5EixSopAt0zYcj8joXAXERk198O9biFYjBWVg3Spz11EBIhCuMeTULuApQmdGVKkHLl7qUuYcVP5Hed+uAM0tLFIfe4iZaeqqoq+vr5IB7y709fXR1VV1aTWm/tfqALUL2b+oa10D6Vw96IfCSYipdHe3k5nZyc9PT2lLmVGVVVV0d7ePql1ohHuDW00Zp4llckzOJylsSZZ6opEpAiSySTLly8vdRmzUjS6ZerbqMwOUcUIBwaHS12NiEjJRSPcG5YAwYFMBw6r311EJCLhPnqU6iH2D6jlLiISjXAPj1Jtix1Sy11EhKiEe9hyP6tyUC13EREKDHcz+7SZvWpmW8zsYTOrMrPlZvaimb1pZt8xs4rpKnZClfVQUU9HxYBa7iIiFBDuZrYE+BSwxt0vAOLAR4AvAX/n7mcBh4BPTEehp9TQxpL4IQ6o5S4iUnC3TAKoNrMEUAMcAK4GHgnnPwDcWOA2Tk/DYlq9n/0DqUgfrSYicjqmHO7uvg/4MrCHINQHgI3AYXfPhot1AksKLfK01C+mKdtLOpun72i6KJsUEZmtCumWaQbWAcuBxUAtcO0k1r/DzDaY2YZpOXS4oY3qdC8x8up3F5GyV0i3zG8Av3L3HnfPAN8DrgSawm4agHZg33gru/t97r7G3de0trYWUEaovo2Y55jPgEbMiEjZKyTc9wBrzazGgjN1XQO8BjwNfDhc5nbg8cJKPE2NSwFYYr0cOKxwF5HyVkif+4sEX5y+DGwOn+s+4LPAn5rZm0AL8M1pqPPUGoMzpp2R6OfAgLplRKS8FXRWSHf/PPD5EybvBC4r5HmnpClouZ9TPcBWhbuIlLloHKEKUNUIlQ0sT/arW0ZEyl50wh2gsZ12U7eMiEjkwn2B99A1mCKX14FMIlK+IhbuS2lKd5HLOz1DI6WuRkSkZCIW7u1UZgaoIcU+9buLSBmLWLgHI2barE8nEBORshaxcA/GugcHMulLVREpX5EM9+UJXW5PRMpbtMK9vg0sztlVh9VyF5GyFq1wjyegYTHLEv3qcxeRshatcAdobKeNXvbrQCYRKWORDPf5uYP0Hhkhnc2XuhoRkZKIYLgvpS59EPM8XWq9i0iZimC4txP3LK0c1oFMIlK2Ihjub120o/PQsRIXIyJSGhEM92Cs++JYH52H1HIXkfIU2XBfVTWgcBeRshW9cK9qgKpGzqw8pG4ZESlb0Qt3gMalLFW3jIiUsYiGezsL8sFFO7I5jXUXkfIT2XBvzHSTy7suuSciZSmy4V6ZGaSWYXXNiEhZimi4B2PdF1ufvlQVkbIUzXBv7gDgjNhBtdxFpCxFM9yblgFwftUhhbuIlKVohnvtfEjWcHalumVEpDxFM9zNoLmDZbEetdxFpCxFM9wBmpaxKN+tse4iUpaiG+7NHTSN7CeXz2usu4iUnQiH+zKSuWPMY0hdMyJSdqIb7uGImaV2UF+qikjZKSjczazJzB4xs21mttXMfs3M5pnZT8xse3jbPF3FTorGuotIGSu05f4V4Ifuvgq4CNgKrAeecveVwFPh4+JrOgOA8zTWXUTK0JTD3cwagV8Hvgng7ml3PwysAx4IF3sAuLHQIqeksg5qW1lZobHuIlJ+Cmm5Lwd6gH82s1+Y2T+ZWS2w0N0PhMt0AQsLLXLKmpZxhmmsu4iUn0LCPQFcAnzN3S8GjnJCF4y7O+DjrWxmd5jZBjPb0NPTU0AZJ9G8jAW5Lo11F5GyU0i4dwKd7v5i+PgRgrDvNrM2gPD24Hgru/t97r7G3de0trYWUMZJNHfQkO6CfFZj3UWkrEw53N29C9hrZueEk64BXgOeAG4Pp90OPF5QhYVoWkbMc7RZP3v71e8uIuUjUeD6fwI8aGYVwE7g9wj+YXzXzD4B7AZuKXAbU9f81lj33f3HuKJkhYiIFFdB4e7um4A148y6ppDnnTbhWPeOWA+7+o6WthYRkSKK7hGqAA3tYHHOr+lnT5+6ZUSkfEQ73OMJaGznrEQfuxTuIlJGoh3uAM3LWGIH2dN3lGBkpohI9EU/3JuWMT9zgKPpHL1H0qWuRkSkKKIf7s0dVKf7qSbFnn59qSoi5aEswh1gqfWwq1f97iJSHsog3JcDsDzWxW4dyCQiZSL64d6yAoB3VfexR2PdRaRMRD/cq5uheh6rKns1HFJEykb0wx2g5Uw66GKPumVEpEyUR7jPO5OF2X30H00zmMqUuhoRkRlXHuHeciZ1I91UMaLTEIhIWSiPcJ8XfKm6zLp1AjERKQvlEe4tZwLQYV3sVstdRMpAeYT7vCDcL6jqZbda7iJSBsoj3KsaoLaVcyt61HIXkbJQHuEOMO/M4ChVhbuIlIHyCfeWYDhk12CKVCZX6mpERGZU+YT7vBXUpXupIaWLZYtI5JVPuI8ZMbOzV1+qiki0lU+4zxsT7j0KdxGJtjIK9+BApvOr+tjZc6TExYiIzKzyCffKOqhbxHmVPexQuItIxJVPuAO0BMMhd/ToYtkiEm3lFe7zVrAgs4+B4Qx9R3WxbBGJrvIK95YzqUn3UccxdhxU14yIRFd5hXs4YmaZdWs4pIhEWnmFe8tZAJybPKCWu4hEWpmF+5lgcS6p0YgZEYm28gr3RCXMW8F5if3s0IFMIhJh5RXuAK3nsDS3l72HjukEYiISWQWHu5nFzewXZvb98PFyM3vRzN40s++YWUXhZU6j1lU0p/aS8KxO/ysikTUdLfc7ga1jHn8J+Dt3Pws4BHxiGrYxfVpXEfMcHdalfncRiayCwt3M2oEPAP8UPjbgauCRcJEHgBsL2ca0az0bgLNsn0bMiEhkFdpy/3vgM0A+fNwCHHb3bPi4E1hS4DamV8tKwLi0plstdxGJrCmHu5l9EDjo7hunuP4dZrbBzDb09PRMtYzJq6iB5mVckDygETMiElmFtNyvBD5kZruAbxN0x3wFaDKzRLhMO7BvvJXd/T53X+Pua1pbWwsoYwpaV7HcO9nZc0QnEBORSJpyuLv759y93d07gI8A/+7utwFPAx8OF7sdeLzgKqdb6znMH9lDKp2me3Ck1NWIiEy7mRjn/lngT83sTYI++G/OwDYK07qKuGc4ww7ypr5UFZEISpx6kVNz92eAZ8L7O4HLpuN5Z0zrOQCstE62HxziqpXzS1yQiMj0Kr8jVAHmB8Mh31XZxbYDQyUuRkRk+pVnuFfWQ0M7q6u62datcBeR6CnPcAdoPYcz6eSNriHyeY2YEZFoKeNwX0XryB5SmQx7+nWOGRGJljIO93NI5FMssV62dalrRkSipYzDfRUA58Q6eV3hLiIRU8bhHgyHvLymm21dgyUuRkRkepVvuFc3QUM7F1XuV8tdRCKnfMMdYOF5rMjvZlffUYbTuiqTiERHeYf7gvNoGd5F3LNsP6jWu4hER3mH+8ILiHmWFbZfI2ZEJFLKPNzPA+DCpEbMiEi0lHe4t6yEWIK1td0KdxGJlPIO90QFzD+b8xOdGg4pIpFS3uEOsPB8lqZ/Re+RNL1HdOEOEYkGhfuiC6kb6aKZQXXNiEhkKNzbLgLg/NhuXtuvrhkRiQaF+6J3AXBF9V627B8ocTEiItND4V4zDxrPYE1VJ1v2KdxFJBoU7gBt72Jlfic7e49yZCRb6mpERAqmcAdou4jm4T3U+jH1u4tIJCjc4fiXqufaHnXNiEgkKNzheLj/WvVehbuIRILCHaB+EdQv5orq3WxWuItIBCjcRy25hHNy23mz54i+VBWROU/hPmrJpTSn9tLgR9jcqda7iMxtCvdRSy4F4KLYDl7pPFziYkRECqNwH7X4YsD4T7V72LRH4S4ic5vCfVRVA7Sew+UVu9i0V+EuInObwn2sJZdyZnobXYPDdA2kSl2NiMiUKdzHWno51ZlDrLADar2LyJw25XA3s6Vm9rSZvWZmr5rZneH0eWb2EzPbHt42T1+5M2zZlQBcEX+dl/ccKnExIiJTV0jLPQv8mbufB6wFPmlm5wHrgafcfSXwVPh4bmg5E2oX8P66HWzY1V/qakREpmzK4e7uB9z95fD+ELAVWAKsAx4IF3sAuLHQIovGDDquZHX+VTbvO0wqkyt1RSIiUzItfe5m1gFcDLwILHT3A+GsLmDhdGyjaJZdSWO6m4X5Hl5Rv7uIzFEFh7uZ1QGPAne5+9vOl+vuDvgE691hZhvMbENPT0+hZUyfZVcAcJltZcNu9buLyNxUULibWZIg2B909++Fk7vNrC2c3wYcHG9dd7/P3de4+5rW1tZCypheredCVRPvr93BS+p3F5E5qpDRMgZ8E9jq7n87ZtYTwO3h/duBx6deXgnEYrDsCt5tW9m46xC5/LgfPEREZrVCWu5XAv8FuNrMNoU/1wP3AO83s+3Ab4SP55ZlVzA/3Un1SI+uzCQic1Jiqiu6+88Bm2D2NVN93lkhHO9+WWwbz+34NS5sbyxxQSIik6MjVMez6F1QUcf7a3fw3I6+UlcjIjJpCvfxxBOw9HLWxrfx0q5+Mrl8qSsSEZkUhftEOq5kYWonNel+fqnzu4vIHKNwn8iZVwPwnvhmfr5dXTMiMrco3Cey6CKoaeGGum387I1xh+qLiMxaCveJxGKw4n1cnt/EK3v7OXwsXeqKREROm8L9ZM66htpMP2ezl2e395a6GhGR06ZwP5kV7wPg2qotPLNNXTMiMnco3E+moQ3aVrOu8hf87I0e8joVgYjMEQr3Uzn3BjpSr5E8ekBXZxKROUPhfirnrQPg+uRGfrClq8TFiIicHoX7qcxfCa2ruKX2F/xwSxfBKepFRGY3hfvpOPcGzk5tZuRwF1v26SyRIjL7KdxPxwU3ESPPBxMv8m+bD5x6eRGRElO4n44F58LCC7it9j94fNM+jZoRkVlP4X66LvwwK0deIzG4mxd26lwzIjK7KdxP1wU3AXBzxYs88nJniYsRETk5hfvpajoDll3FbZXP8qMt+zkyki11RSIiE1K4T8alH6clvZ/V2V/yyIa9pa5GRGRCCvfJOPcGqG7mj+p/zj8/t0tfrIrIrKVwn4xkFVz0Ua5Iv8Bw3z7+XScTE5FZSuE+WZf/AWbOZ2q+z73PvKkjVkVkVlK4T1ZzB3bJx/ht/wkH927nmdd7Sl2RiMg7KNyn4j13E4vF+XzNo3z5x6+r711EZh2F+1Q0LsGuuov3556ltetnPPQfe0pdkYjI2yjcp+o9f4a3ruJvqv+Zf3hyA3v7j5W6IhGR4xTuU5WoxNb9I/P8MP/d7uVTD21kOJ0rdVUiIoDCvTDta7Df/CJX2wau7/oan3rwJUayCngRKT2Fe6Eu/0O49OP8fuLf+INf/Ql//pVvsblzQEMkRaSkbDaE0Jo1a3zDhg2lLqMwv/xX0t+/m4r0YV7Jr6A/uYjWZIrh2qV0rr6L915yAc21FaWuUkQixMw2uvuacecp3KdRapDhn9/L4a1PY4OdDOarWZ7dyTCV/KtfTabjai5d0cqSVWtYuKCNeMxKXbGIzGFFD3czuxb4ChAH/snd7znZ8pEJ93HkD77BkSf/itpdPyZO0B+fd2MnixlKNNNVs4q+eauJVTWSqK6nNuG0pXZgdfNJL15LZdMi6pJOQ36IiqZFVFUkqEzEMNM/BpFyV9RwN7M48AbwfqATeAn4qLu/NtE6UQ73444cJNO1lR0H+snsfpGKvq0khw+yNPUGSSY+ffCwV1BBhrg5fV5PrzdSSYZqSzNkdey0M8jFKojHjJgZaZLELU+9DTOYmM+R5HxiniXmWeKeIe45Msk64olKFuc6OWa17Eu0k49VEIvHicXiWCxBY66fhkwPvck2jnolyfQAR/NJYolKFtU4zUd34OlhflVxFiOV86hNxmiyIUjUkIlXk/Q06WyOjMdpqqsmTo5MJkO2shlLVpGMG4mYkYjHSMQTJBIJ3GKksk5uZJh0Ls9wrIb5tUkaK5xcZoR0DjJ5J51znBiYYUDtkV1UjvRyrPFsUjVtxJLVNMZHqIyDxRNYvIJYPIHFE8RjMWIGxAwj2GdmBhbDYnHMYlg8hlnw/LHhQ8TSQ5CowhKVwSipWPytZT1LLJ/B8jmIJyFRBYkKwMDsrVvPk0+nGEkdxR2q65swgHwO8llIVuEYfqSHfD5HLFFJLFEBuRHIjuDxiqCGWByO9kKiEurbgjdJPgO58CefCZ6zqgkq68MaCOoY9baGgb1z2skaDtk09G0PttW6KjjfEoB78IOD58PH+eAxBrE4WAyO9UFqABrbIVkNuSxkU8FzxCuCfWgWrJ/Pgefefjt6P54MfseJah2tJzbJrxVH1xut3T2oO3w/nHTflMDJwj0xA9u7DHjT3XeGG/82sA6YMNzLQt0CkmctYNVZwHt++63p6WPBH8vIETLDg6QyOYYaz2akfz+xAxuxgb0MU8VQrIGGgddJpAcZtgoOk6R6pJ9LU7swz+HueN5JksExjlFDc+olqkkd31SWODniVJIGoI8GaklxVfj4RBmPk7TxR/8MeTUZS3AJ35+2XRR1MaD6JPPDfwXjjnIodaTkwwpivL0xmCNGnPyUni9PjATvfH+d7nNmiZMljuHEgn+NGP62dfMYOeLkiJEjDnB8uWCdfPg7BY9P/P3Gk5vCOBQf5xXMEyNNku0X/wUXr/uTST/nqcxEuC8Bxp7svBO4fAa2Ew0VNdB2EQDJ8KceYNnZcPF7p/y0TRC0OjLHIJaEeJKEWfCCZ0cgm6KlqjFoCQ0dCFping9+8jmobiZZtwAGOiGXhupmyAxDboQREtQ2LiEWiwXrpoLRQalkI7n0MJ4+CvEqKpMJzLP0DhyFeILKRAI/1kc2M0ImlyebJ7jNZsnlspjnqIgZiapqKuNGMnuM3mNZjmRiJCoqSMaNipiRjIOFrSt3J123hHT1Air7XyeR6sUzKYbylaTzQC6L57NB6ziXIe/gHvzxj35q9fxoKy2Pje6D8PlTiQZGEnXE8hni+RHiuTR4DvMcuJOzJDlLBIGUzxDzNPF8+ngrNkYQJm4xcrFKYhVVgOOpoWATsSRuMRI+QszzDFe04BYnlk+TS6fIxpJ4opoKsiTyKTyX5UiiiXhuhLp0bxBeliRncfKWIGcJ8sSozg1RkTsWtpvD39PH3B8znbd9evdx7vrxW7c4vRVLyVmcBaldxDx3PBoxIx8+62h0jjLyxDzPsXg9w7E6mjNdJD1NxpJkrQLHSHiGuGeIeT74fYjhFiMfBrPbW7cJz1CfPYSRB4uF/3zCyDYL9zzEyBHzMNo9F/4WhtuJ0R6skx+d50Z+tJXuwZLmb/0DMfLjBvZ4bIJ/GDHPkfAMLfNXntbzTNZMhPtpMbM7gDsAzjjjjFKVEW1mUFH7zulh9wIQfFxubJ/4OZqWvmNS5dgHDYuhYTHGxK3StoWnWe84Wie19LKpb0gkYmZinPs+YGwitIfT3sbd73P3Ne6+prV1cn/CIiJycjMR7i8BK81suZlVAB8BnpiB7YiIyASmvVvG3bNm9sfAjwiGQt7v7q9O93ZERGRiM9Ln7u5PAk/OxHOLiMip6dwyIiIRpHAXEYkghbuISAQp3EVEImhWnBXSzHqA3VNcfT7QO43lTKfZWpvqmhzVNXmztbao1bXM3cc9UGhWhHshzGzDRCfOKbXZWpvqmhzVNXmztbZyqkvdMiIiEaRwFxGJoCiE+32lLuAkZmttqmtyVNfkzdbayqauOd/nLiIi7xSFlruIiJxA4S4iEkFzOtzN7Foze93M3jSz9SWsY6mZPW1mr5nZq2Z2Zzj9C2a2z8w2hT/Xl6C2XWa2Odz+hnDaPDP7iZltD2+bi1zTOWP2ySYzGzSzu0q1v8zsfjM7aGZbxkwbdx9Z4Kvhe+6XZnZJkev6n2a2Ldz2Y2bWFE7vMLPhMfvu60Wua8LXzsw+F+6v183st2aqrpPU9p0xde0ys03h9KLss5Pkw8y+x9x9Tv4QnE54B7ACqABeAc4rUS1twMfRLnYAAAOSSURBVCXh/XqCC4SfB3wBuLvE+2kXMP+Eaf8DWB/eXw98qcSvYxfBZZRKsr+AXwcuAbacah8B1wM/ILis6VrgxSLX9ZtAIrz/pTF1dYxdrgT7a9zXLvw7eIXgAl7Lw7/ZeDFrO2H+3wB/Vcx9dpJ8mNH32FxuuR+/ELe7p4HRC3EXnbsfcPeXw/tDwFaCa8nOVuuAB8L7DwA3lrCWa4Ad7j7VI5QL5u7PAv0nTJ5oH60DvuWBF4AmM2srVl3u/mN3z4YPXyC40llRTbC/JrIO+La7j7j7r4A3Cf52i16bmRlwC/DwTG1/gpomyocZfY/N5XAf70LcJQ9UM+sALgZeDCf9cfjR6v5id3+EHPixmW204Lq1AAvd/UB4vwso4CqnBfsIb/9jK/X+GjXRPppN77v/StDCG7XczH5hZj8zs/eUoJ7xXrvZtL/eA3S7+/Yx04q6z07Ihxl9j83lcJ91zKwOeBS4y90Hga8BZwKrgQMEHwmL7Sp3vwS4Dvikmf362JkefA4syXhYCy7D+CHgX8NJs2F/vUMp99FEzOwvgSzwYDjpAHCGu18M/CnwkJk1FLGkWfnaneCjvL0hUdR9Nk4+HDcT77G5HO6ndSHuYjGzJMEL96C7fw/A3bvdPefueeAbzODH0Ym4+77w9iDwWFhD9+jHvPD2YLHrCl0HvOzu3WGNJd9fY0y0j0r+vjOzjwMfBG4LQ4Gw26MvvL+RoG/77GLVdJLXruT7C8DMEsB/Br4zOq2Y+2y8fGCG32NzOdxnzYW4w768bwJb3f1vx0wf20/228CWE9ed4bpqzax+9D7Bl3FbCPbT7eFitwOPF7OuMd7Wkir1/jrBRPvoCeBj4YiGtcDAmI/WM87MrgU+A3zI3Y+Nmd5qZvHw/gpgJbCziHVN9No9AXzEzCrNbHlY138Uq64xfgPY5u6doxOKtc8mygdm+j02098Uz+QPwbfKbxD8x/3LEtZxFcFHql8Cm8Kf64H/DWwOpz8BtBW5rhUEIxVeAV4d3UdAC/AUsB34KTCvBPusFugDGsdMK8n+IvgHcwDIEPRvfmKifUQwguEfw/fcZmBNket6k6A/dvR99vVw2ZvC13gT8DJwQ5HrmvC1A/4y3F+vA9cV+7UMp/8L8IcnLFuUfXaSfJjR95hOPyAiEkFzuVtGREQmoHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiETQ/wcwU2ET6Sik2gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydeZhcVZ33P6f2qu7qfUnSTdIJ2cNOgACyo2wCKiq4vKOjo84oKu74zriOM/qqMzqOqAMMio6AiKOiA+KC6KgEEtaEJJCQENKdpLfqvdZ767x/nHvr3qq61V1JupN053yeJ09V3XvuPefeTn3vr77nd84RUko0Go1GM/vxHekGaDQajWZ60IKu0Wg0cwQt6BqNRjNH0IKu0Wg0cwQt6BqNRjNH0IKu0Wg0cwQt6BqNRjNH0IKu0Wg0cwQt6BqNRjNH0IKuOSYQQpwthLhfCLFPCDEhhHhaCPGWkjKLhBB3CyEGhBBJIcSzQog3u/ZHhRBfFkLsFkJkhBC7hBBfPPxXo9F4EzjSDdBoDhOLgD8D3wHSwLnAd4UQeSnl3UKINuBRIAl8FNgDnAAcByCEEMDPgbOBfwSeADqA8w7zdWg0FRF6LhfNsYYlzn7gFmCZlPJiK9L+ALBUSrnP45jLgF8B10op7z+sDdZoqkRH6JpjAiFEI/A54FpUZO23dvVYrxcDv/ISc9f+hBZzzdGM9tA1xwrfA64HvgK8CjgDuAOIWPubgUpiXs1+jeaIoyN0zZxHCBEBXg28T0r5Hdd2d0AzCMyf5DRT7ddojjg6QtccC4RR/9cz9gYhRBy4xlXmd8BlQoj2Cuf4HdAkhHj1jLVSozlEdKeo5phACPE40IrKYMkDN1uf66SULUKIVuApVJbLP6GyXFYBNVLKL1sdqQ8C5wCfB55EReznSynfc7ivR6PxQgu65phACLEU+A9gHco++SYQA26UUrZYZRYBX0Z57GFgO/BFKeU91v4oKmXxBtTDYC9wl5Ty7w/v1Wg03mhB12g0mjmC9tA1Go1mjqAFXaPRaOYIWtA1Go1mjqAFXaPRaOYIR2xgUUtLi+zq6jpS1Ws0Gs2s5IknnhiQUrZ67Ttigt7V1cXGjRuPVPUajUYzKxFC7K60T1suGo1GM0fQgq7RaDRzBC3oGo1GM0c4qmZbzOVydHd3k06nj3RTZpRIJEJnZyfBYPBIN0Wj0cwhjipB7+7uJh6P09XVhZoLae4hpWRwcJDu7m4WL158pJuj0WjmEFNaLkKIO4QQfUKIzRX2CyHEN4QQO6xFdU872Mak02mam5vnrJgDCCFobm6e879CNBrN4acaD/17wOWT7L8CWGb9ezfw7UNp0FwWc5tj4Ro1Gs3hZ0rLRUr5RyFE1yRFrgW+L9W0jeuFEA1CiPmTrM2osXn+QZh3EtR3FG/f9b9Q2w6ty49Mu+Yi+Tw8/UM4+Qbwl/RdPPtjWP4qiNTDcz+DRedCrRq3MTCeYf3OQV590oJC8d9v66OtLsyaBfX84YV+nngpQTQU4B2LhxBCcseuJpIZg9MWNXLhirbiepa9EqIN/GjDy4zs380a3y7OveqvCkUe3tbL4pZaFrfU8IcX+mmLh1k1v44/vNDPUzv7WNX/AFvaruKspW2cc3wLzz/zKOnxEU4+93K2b3mCp5/byp76M4ourzbTR9vENnY2nT/lbZo/+iw5f4SJxtW8Y+F+/DsfZuPeDI+3vgHDF+bEwQc586p3EI/X8YP1uxkcV2uGLBt8mO74KaRCTRXP3TnyBMlgE4mYshp9eaNwPVL4y8o3Jl+iNtdfdD2l9URzQ3SOPMH2lksnvZ6BGuu7JPOs6fslW1uvIO+b+T6slokdhMxx9tadAoA/n2Hdnv+k8fTXsfy0C6a9vunw0DtQiwHYdFvbvFZOfzcqimfhwoXTUPX0Mjw8zF133cV73/veAzruyiuv5K677qKhoeHAKvzRW+Hcm+CSTxVvv/9GWHgOvPaQfuxo3HQ/ru5rTSuscP3gHN4D//03cOVX4cQ3wI/fBpd8Gs77CAA/2rCHrzz0PGd0NdFeF2HfSIr3/OAJOpui/Pg9Z/Pe/3qCiawJwGvnfRUhTb7U+wkAYiE/f/rExTTVhGCku1DPnqVv4RM/2cTHAvewzv8LnjnhCk5e1Eoqa/K3P3iSS1e38e9vOo0b73qS1ni4UM8ZxkZuCn2F27f4uPOxE3nkoxfS//N/oN3cz8DJF7H7p5/j3Owmzs1+s+jSP+q/h6v9v2Bl9k6MKb7yDwQ/z37ZyF/nPsFr27/CvJGnWAfcvi3Ey7Tz4dBn+WlWEjvjzXzm/ucAqBNJng3fzFeMN/It8zUVz/2/oU/xZH4ZnzPeD8AFvqe5Kfhl7tgqeFyuKiv/tcAtnOXbyjnW9cQpr+c9/l/wd4G7OWVTCyPUel5PL418Jqf+JqeJF/hw6Av8eFuK3+VPn/ReTAe3Bb7KQtHLJ3NfAWABA3ww/D0ebeyCo1TQq0ZKeStwK8DatWuPuonYh4eH+da3vlUm6IZhEAhUvlUPPPDAgVcmJeQNyE6U78ulwMyUb9ccPON96nWiv3i7/XliAJKDznuLfSMpALb3jtNeF+E7j7xI1syzs3+Ct393A8mcya8/dD6f/vlmMnv3I8hz1uImvvCaE3jV1//If/5pJx+7bGVRPet3qnquXxXBv11y5++e4l/f8SqeenmIrJnn8V0Jnts7wljaYCxt8PbvbmAia/LFy+fDI/D/rpjPxQ/k+D//+Tj/aCRoFKP89Xc38PH0IO3BMXb985XgtvV+/gA8Jdn+92dDresXgxdfuYlVdYKzRTOZvb1sEss5Ub7A7a/vgoZF8H3Y9uJOfpd4nq7mGL/98AUEhnfBv8PHzm3mY1dcVfncX3gHnccFueZtVpmnhuHn8KO3LIU1Hsd9/zZ4yXU9gy+W1/PQX+BRePojp0HLsvJzfPVDrIrDrvdY5bfm4Udw+3VdcNokbZ0ubvsXGEqz6+NWXfuegf+As0+YmV/f05GH3gMc5/rcaW2bddx88828+OKLnHLKKZxxxhmcd955XHPNNaxevRqA17zmNZx++umsWbOGW2+9tXBcV1cXAwMDvPTSS6xatYp3vetdrFmzhle96lWkUinvyuyFRXLJ8n1GRom9ZvpIJQAY7N/P5p6Rsu2kEpC03tuvQN+oerBu7xujdzTN3Rv28IbTO1nWVsumnhGuPGE+y9vjfPCS5cTlKHVyjA9euoxl7XGuPHE+d/5lN8PJbOGcufEBHtuVoDEWpNk3DsCz23exuWeE9btUmYHxLHc//jIAHQ1RVc+J85gfUv9XlsQynL+8lU09I7QHkjSKcTb3DDEvmMSfz5UHCcmhsuvyREp1H1IJPnjpMuJylOeNec6x1r2qk6Ps6BvnfRctJeD3QWqo+F56kU2CkXbaUnrvvUglIJ+D7Lj12aMej79Z0fUkE8V1TlZ+JkgmVLvzeeuzFTTEmmekuumI0O8HbhRC3AOcBYxMh3/+uV88x5a9o4fcODerF9TxmavXVNz/pS99ic2bN/P000/zyCOPcNVVV7F58+ZCeuEdd9xBU1MTqVSKM844g+uuu47m5uI/zPbt27n77ru57bbbeOMb38hPfvIT3vrWt3rUZv2Bcx6Cb+Ygbx7sZWq8sL5Iv35iC19a/xh/vvliasMB1xd80Pmy2a9A35gt6OP87KkeskaeGy9eytZ9Y3zwnqd4/yVLATh7cQN5MYEPydld9QC898Lj+Z9n9/HLZ/dxqbGXecC2nS/xWGaQMxc3Iay6jwsn+cbvtjOSytFSG2ZgPMN9T3TT1Rzjk1eu4gN3P8UHLlkGm39RaN+HLl3G+p2DtPrH8efztAbTdIZTkLLaH3bZDx7X5UlmVAUSyQTrutT1+BuOQ6YiCNc517ZJlhm1vObUjurP71VmquPcf5tw/MDPkRlTD4QDqXO6SSZA5iE9DLEm55qilfsaDoVq0hbvBh4FVgghuoUQ7xRC/K0Q4m+tIg8AO4EdwG3AgRnQRzFnnnlmUa74N77xDU4++WTWrVvHnj172L59e9kxixcv5pRTVAfI6aefzksvveR98skidDOjBX26sb5IvmSCkVSOO//ykrXd9QX3+LL3W4K+o3ecx3YlWNJaw6LmGi4/YR6bPnsZK+fVqYKpYXyov6lIDQOwen4dbfEwj+1KsKdbdTONDu5nTyLFWYubC/W8bmWUX2/p5YndQ7zmlAW0xcPkTMlZi5u5bI2rnkL7Epy6sJHNn7qIQE5Fr396/ylEc8Nl7S+7xknvkbU/Ow5j+/EhufbckxCxZivSVffwzDbJAx88j6DfV/353WXs//uu65nymEr1TFa3vS034QROU9U5nZg5yIwU13ekI3Qp5Zum2C+B901biywmi6QPFzU1NYX3jzzyCL/97W959NFHicViXHjhhZ655OFwuPDe7/dXtlywBb1kfz6voqRjwHLpHVX3r70uUrR9aCLLWNpgYXPsoM/dPZQkHPDTGld/j/zEAD5gYSTFBQtauf1/d/L2c7qosb5gcmJQRaFAcqSPnT0jrFlQR9+YauPzvWPk85KrT3GyXUIBVzxUKjK1rQghWLekmfU7B7kivheABsYAWLekGf6sjrlkUYD41gBjGYN1S5rpG8tw/zN7OWtJU3E9JaIQyg47bRnvASNVXK60bVMKuuu4QRWs+GLNKpp0RegilXDEvNrz2/vMjLKEwrXlIucmm3SCndJyVQt6iTVT3zF5ndNNUf2DwFLrVUD0ABMoqkTP5eIiHo8zNjbmuW9kZITGxkZisRjbtm1j/fr1h1aZrCDoZla9HgOC/pF7n+HD9z5dtv1LD27j6m/+idF07qDP/Z4fPMHb7ngcexH0/l4lqCvqsnzgkqUMJXM8sGkfY4leAMaGekkMKKfQHB/kum//hd7RDDlTclxTlJFUjrGMwVmLK/xU9hIZ4KwlTfSNZRge2A9ARyhFe12YlW2xgiccyw3zrvOXEA36OaOriYtWthIK+Djn+BbvOrxEbGB7eTlQv/TSFSL3ya7BPl+sybIKvH/BFLdnkqg35faxJ7mOQvlJfPJSoa50Dve21CQPhZmi0oMn2gi+8jTN6UALuovm5mbOPfdcTjjhBD72sY8V7bv88ssxDINVq1Zx8803s27dukOrrJLlYme3HAOC/mL/ODv7y7N8Xk4klS3y55cO6rw5M88LvWNs2TfKb7b0ks9LhixBbRRjnLawkaaaEOt3Jhi2RDyYGWbT9l0AxEWKvJHlsV3qS/iKpY6wrltS4adyJUFfrMrXSdUfVM8oD910Pr7MCIVfackEN160lP/9xEXUx4K85pQOHr35YubVF/9ymVQIB3d4tyU9ojxcq55J8TpfrFn9K+o0riDo2XHVoT/VuasR9MnK2/V4WRqHUud041l/YsbsFjjK5nI5Grjrrrs8t4fDYR588EHPfbZP3tLSwubNzgwJH/3oRyepyfoyGyW2jWlFpXPcQ88aefZblkvWyBfZF7bNcfufdvHXr1isOi8PgN2DE+RMdX//7Xfb6RlOcVFuBHyojkghOLOrifU7B0n7VIpiVGQRI91gBU4NjPGXHepLeO7SFu5+fA9dzbEye6iAO6J0vT++tYaW2jCNGeV1CyNNQ8CA0eLyPp+gpVbZQ0IImmsd666ALVopj0h14AXvtnhFqZXwOp8t6MlBCNU65aR0UiNLI+a6+R7n9oqWJ8k4KbMrSs6RTBRHuV7X5nUfpsqsmU68/k8kB9UvnhlCR+hHikoRunFsROj7R9JIqW7D/pHih1rfWIZTjmtgJJXjgWcPPGFqe68Sz786exHP7R3lc7/YQovPlfqWN1m3pIme4RRywhGJpT6nrhbfGH/aocT+xI56WuNhXrGsxAJxUyFCF0Jw3rIW2gITxfsrlK9IPu8hhFVYLgdSj6flYkfow04uvzRV5F84rsIDpNK5kwnv66lY3sPSqeYeltbp3pYaBnOGv2NHIELXgn7EqJC2eIxYLt1DSc/3qazJWNrgkpVtBHyClwY9Bl5NwfY+Jd6fuHwld79rHXe+7RRqmbC+SBJSw5xlWSf1jJIONgKwQAwUvmwr6nL0DKu/TVs8wk/few43X1E+mrFAchACUQjGygTqs9esYUks7XyR3WIUa65O0NPDyjqJNav0QsPJbSfaBCN7vM9nl6mmHtvfBXW+QARCMSuilDC2t/ga3Md5bS86d6K4jPt6smPlVo273e6o+kDuoft6kgn16zc94vw/SA+XHzOd2G2yO5XtbTpCn4NU7BS1LZc5LujDznV3DznvbbtlfkOUBQ3Ron1upJRkjGJbKmvkMcw82/vGOa4pSk04wNnHN3NBp2XZtFij85KDrGiP0xAN0MgY+WbXCEOrzIo69XeIhwNEQ346G2OTWz+2YHmIS30kgD+dKKq/UKZleXWCbgucfY5UwrJB4lBnZ94IaDreO2Kupp7kINTOU3PagCOe7oiycA0ldbivrdK5m5aA8FnXX3I9npk5JddTWs9U1+a+nuSg0zE7VVuni2RC2VR1HY5N5X74zQBa0I8UbstFumZBKFguc9tD7x5KIQT4RLG42wN52uJhOhqihSi5lJ893cO6f/4dyazz4Hv9d/7Cx3/yLNt7x1jWFncKF774ywqffT7BhV1RQsIkOn+lU9YqsySmHixtdR5ethd25BVrKheK7LjKXirUnyhuUzU50R7XUFQnqFS42jbvn/rV1ON+KIFzXndE6a4fHOukdLvXuWtanWjV63pKr9d9Pfm8Okfp9dvnSA2Xf2dKH7JT1TndlP6fyE6oX+Ba0OcilojLvJOqCMdM2mL3UJJ5dRHm1UWKLBd7qH1bXZjOxmjRPjfP7x9nKJnjxT5lyaRzJpt7RvjpUz3s6BtnWZtrpKT9k92OxK3Pf39xOwDCPQdIsxr52Rlx7JaqSCbUFzfaVC6cSY/6kwlladQfpywUc4oUzdJrsIfi23nioF5jJfUnB8Efrq6eVAJijc75vCL0kntIxsqise5bUXpi0T0YLG5fhb9JUVuiLjHMjCjv3l1P0X1VVlpxna7rcf8qcN/DmaTwQCm55hkaJQpa0I8c7qjc3TF6jAh6z1CKzsYonY2xIlulMNgoHqGjMUrvaKbMWgFITCjhf6FXjRt4sX+cvNXJauQlS9s8hr6X/NRutTtKm5YAVsZG7TwI19FudWIeWITubbkUPtv12OJif9mhiui5UoTeXCy8dv3SSYmsup7S87kfFDal0a19vtp5EK73jnoLVkOTR7Rcwf4ovZ+FetqdegqWxoIqz3EkInSP+nWEfnRSW1s+XWf1uAXdZSscI1ku3UMpOhqidDRG6RlK8fH7nuFd399I31iGkN9HQyxIZ6MaKbpvuHxEbmJCPfjsDtAd1utFK9Q85svbJ7dc1KstEm1O55klfg1yDJ+gPBe8EkVf3goRul1PkV0yRWdi2TWUeMhegp7PqXlMwInip6rHtjRKz+d+BWjsAl/AdQ9dIuVlN4EKWGyroTCNQBWWS+F6DBh6qbyeUsvJfQ63Xx1rtiJ6a3/zERD01BCM9zvXMEPoPPQjhawg6MdAHrphqhz0zsYYQsDPnk5x78ZuAj6BXwha42GEEHQ0RAEl/l0tNUXnGLQEfUefEq7tveP4fYKvX38qDz/fy0md9U5h+4tb36kyUTzFyMqmsEQ2kE7w3b8+k5XzXA+GSpiGNflSMyCUPWDmnIU0SuuxxSh6gIJuWydAYW4V90OhVLgjddU/OGxLI9bsBBX2MaGYum9GCmItxb9CCtfWOHm2SaF9TdC90ft6io5JwLyTnTYUjVy16hH+YsvJXXe65HrcEXJ9h5WNdBg6RQt/YwmJF61r0BH6YeHmm2/mlltuKXz+7Gc/yxe+8AUuueQSTjvtNE488UR+/vOfT1NtlQR97kfo+0fTmHlpWS7RwrPNyEv+8EJ/Yf6VzkYl6D3D5T56aYS+vW+MruYY9bEgrz21s3iZP/uneSBcHEEXhMZDFJODXLC8tfJAIje2b1zJ2iitxyu6rkbQY80QCEG4TqUQZsdL2u4W7kTxcdWkFbqv335vU7BhGivcwwp2U6UypdfjGaG7rsdroNNk11ZaZy4Joz0QrIFg1PuX1HRiZFU6prt97ofSDHH0RugP3gz7N03vOeedCFd8qeLu66+/nptuuon3vU/NNXbvvffy0EMP8YEPfIC6ujoGBgZYt24d11xzzaGvC2oPx4bDarm874dPcvHKNq47vbPqY0ZSOd5y+3q+8vqTWTW/rqpj3vvDJ7hoRRtvWKsisA0vJfi7/3qSnJnHzCsF72iM4rPu49vOXsR/PfYyqZxJu+Vbz6+P4PcJz9TFxLgS9JcTSdI5k+194yx3Z7a4sSNZUJGke4Si8ClP1p3REW2C/m1VXSfg6uxqdEZPphIQb3feu+sZ3lPeoTnlKM4hp43RxpKh+bZd1FR+PlsYp6rHndNuj152C0+sUQmUP1Dc8Vv0IGiCvq0e53Y90KJNyhIafrn4nrvF1Z473d1ua7KwQkdp31bw+VQnqS2YRSMzh8qvZ2B78T2cydGi9rljTY6dN7hd/T+IzMzEXHA0C/oR4NRTT6Wvr4+9e/fS399PY2Mj8+bN40Mf+hB//OMf8fl89PT00Nvby7x58w6tsoqdoiV56BvvUP8h1rxWfU6PwM/eqzIW3HSdDxd8DHb8Dv78bxT9ArDI1y/kV5svZ2F+D2z9oNMBG22E1/6H8l3v/4CabtSFL2Pw4b4kTz7zBVa1nwMPfhzWvVd18t3/fhjeXXZpb905SHN3CJ6LI4FIzwjfNvM014Z4qvnV7Jx/JeuGfoF/y0/507wU8xIRrqsZZSxtMK83AnfWEAB+FBmi7ukg7HP6K/IIVmQvYHzeGVw9cBu5O77JF0ZG6PBF4U6PGRp7n4MGa8nDWDO8/Be482q1Ak60SQlDrAn8IRXJx5phdK8qUw3pUefctqD/9G+V5QHl9bz4sCVYroj+z9+A535auY59z8D8k5169mxw3hf9urDO9+tPwV++oYStmnrcvzLsX4lFgt7sLJwRa1L/z+68WnnbRfetp/y+2StAudu6ZwMcd4azfcdvnePs74C73d0bwRdU86Lb9QgBy69wLKENd6h7W+l6ujc66/TGmmH3o9X/jQ8UO0grveZoo/p/MEMcvYI+SSQ9k7zhDW/gvvvuY//+/Vx//fX88Ic/pL+/nyeeeIJgMEhXV5fntLkHjlQDHtIjFSwXy0N//Ha1YLEt6L1bYNsvoW21MwAksQv6tilBf+6/4eX10HFacXXjvfh2/ZF6+QoWDT0Did9C55lKxHf9EV7xIbXu5QsPKuEIOsIo0uNc7N/ED19eDyMLYcPtasDHSW+Ep36ghL22vVA+Z+YJYGDkADPHWDpHKp1mUVOM9tR2lvj+AJd9GG79O0jspLNtFUiDxgikMwZRn1n4Utf485i5DJhOtonY8xiX+uPsX3I5fzX0GxioJUATtYG8d1pey3I44Tr1/qQbnImdGhZC13lq+wmvh/gCJRIrr4J9T0+dSmgTjMLSV6r7JgQse5W6r/bx7npWvxYGd6p5SJa9StlAa9+hIs7J6mtbrdoOcOpbYNN9qt7OM5VInPxmWHIRxOfB6tfAeK86X9d5sOyyqesJ1SpxbFupOj5Puh46Tnf2n/p/HGE+8fVqST0zpwbNrLxaXfeKK2Gvx32L1MPqa9USdv4gLLlQ/Q3s6znlLbDpx8XHLblQLdbdsNC5ngWnFdeDVH8rgDP/Rgm2fQ739dR3wvGXqMBpzevU/pNvUA/Vav/GB4ovAMdfDMdZf58VV6movesVM1OfjZTyiPw7/fTTZSlbtmwp23a42bx5szz77LPlsmXL5N69e+XXv/51eeONN0oppXz44YclIHft2iWllLKmpuag69ny2MNS/tspUn6mTspNP3F2PHar2vaP7erzv6+V8tvnOvtf/L3av+tPzrbffk7KzzZKmc9LedcNUn7LVd7m2R9L+Zk6efHN/yF//OV3S/nZBilNU8rd66X8TJ3873u/Jx+996vq3MPdRYf+fsPTUn6mTn7zK/9Xyt6tqsxvPy+N/er9tt/cUVR+U/ewXPSJX8rT//HXUkop33LbennmP/1GprKGlD+83rmer50o5X1/Uzju4a29ctEnfinveXx3YduH7nlKrvvn30oppXzk+T550z1Pydw/L5Tf+/vr5M+f6pa5TzfIb3/6bXLRJ34pn+sZqe7mazSzGGCjrKCrulO0hDVr1jA2NkZHRwfz58/nLW95Cxs3buTEE0/k+9//PitXrpz6JNUgpfNTbDIP3cwV+4t2RBFw5UfHmp0Jk2zPtBSrribGCGeHXRaA2v7Elh3s2P2yVbb4+NGc+iE3OjpKPuus/DLYr+YYv+OpUfJ5x+Kxc8kHxrNMZAw2vJTg1SctIBL0l3SoFU9UdM7SZt5z/hIuWeVE+yvnx9k3kqZvNM1dj+3mp0/1kBURIuSYX+snIPLMb2nir85exPL2Q0kj1WhmP1VZLkKIy4F/Q00ueruU8ksl+xcBdwCtQAJ4q5Sye5rbetjYtMnpjG1paeHRRx/1LDc+Pn4ItbgF3e2hlwh63ijuvbcF306Jg+Je/uQgzDupvDpb0MUYMWMY6l2ZEUAwO4QQw04WgIsRU9XlNzMMDA/RZtU1NLCfNmBTIsCvntvPlSeqaVPt4fsA63cOkjHyrLDzwu0cYiPjZAFYhAN+Pnll8QRY9vzjj+4c5HFrEeWJfJCoyNAcVh3L165dyrVnn1B+zRrNMUY1a4r6gVuAK4DVwJuEEKtLin0V+L6U8iTg88AXp7uhcw57pjnwzkNHqsEeZk55fVlL9G3B97si9EIebmKSCF1taxRj1JojSLtMpAEpfDSKMSK5YaTdI+9iNCswpSAiMuwbcFaQH7dW+8lHGvnG77YXVgeyh+8D/P55NeXqUjt6jllZByPW8z5WXp+b1fPrqA0H+MGjuxlKqnszbPiJkqUpZD30glUO/tFo5jjVWC5nAjuklDullFngHuDakjKrAat7md977NeUIZ10Ji/LBay1RS2Bt6P0guUScsrZD4aJfjWfhdfABUv0GxmnnnHMiFW3z0cmUE8j4zQyRtyPXvkAACAASURBVDZcLrBjGZM0IaJk6Ru0BD2VID2iOsneeP7JbNs/Vkgv7BtL4/epbI/fb1Oj4wpD8e22udPuJiHg97G2q5GNu1W9Qb9g1FARep3fFvSDX3tUo5lLVCPoHcAe1+dua5ubZwCr+5jXAnEhRNk3VQjxbiHERiHExv7+fs/K7ChvLiPzeRWhh2pVypfhjtBdE3XlDWcSflvQC5aLW9CtaDvxIkVWjptQjIyI0CjGaBJjZEKOcI/56grbk4HyHNnxTI6MCNMYNBgYsiP0QYzxflKEecVqlRK4fqdqY+9ohuNbawj6BT3DKebVRaiLWBaR10CRKbCXcetoiHJSZwMpGSLuy+EzrftWYhFpNMcq09Up+lHgAiHEU8AFQA9QNnZdSnmrlHKtlHJta2tr2UkikQiDg4NzWtSllAwODhAZ2amEKBgtsVxKBL0sQrf2+0s6RWFKkRwRcZrFKI2MkQw4Q+MHZS3tgQkaGWNUlA/OGU8bZESYtkieoZHRQntEcpBxXx3L2moLa3QC9I+laa+LsMAaur/M3Vl5MIK+RD2wzlrcxLK2WlKEqfHlnPumI3SNBqiuU7QHOM71udPaVkBKuRcrQhdC1ALXSSkPeDmQzs5Ouru7qRS9zxUiQR+dT/4/uOTvlRi5O0VLLRfbYrEzQwqC7uoUDcfVoIsphhYnZJylwQGCeZMJXz32I3VfroYVoQHi6TG2yziLSo4bzxjkRITGkEFydKzQtvpMD+lgI8Jao9NeVLlvLMPStjh5Kdk9mCyem7xsGPTUgn5iRz2Xrmrj9Ws72bJ3lDQhYiLr3DcdoWs0QHWCvgFYJoRYjBLyG4A3uwsIIVqAhJQyD3wSlfFywASDQRYvXnwwh84uhvdAdrhChO4a6JA3nQjdHkpsC747bVEIJYyTiGQ+L+k3azk5oFITR3xqFONwMsv+XA3rAi8QFSn252rKjh1LG+T8EWp8OfLZJFjPkk6zh9G4yko5a0kTv3puP3sSSfrHMrTXhfFbv/+8I3TXUO4pCPp93P42NaowZ0p6ZZioyLgidC3oGg1UYblIKQ3gRuAhYCtwr5TyOSHE54UQ11jFLgSeF0K8ALQD/zRD7Z0buIUoGPMeKVr6vrRT1O2hg5UOaI3k8xDJRDJLQtZSl1c/nIakEtntfeMMU0s0p7bvzZaL43jGwPRFiIksYeFYQs1iFBGz7RAl1A9u3oeRl7TFw4Xpb4sWm4jUo+YEH1DLpwVKrmMKlOUSIiTdEbq2XDQaqDIPXUr5APBAybZPu97fB9w3vU2bw7iFKBj1XuACIOeaYqAg6Bk1bajPX3xOr5nxXPSNZkhIx/oYyKv323vHi7a/lCxPARzPGJiBCFEyRMkW7QvElXGzcl6ceXURvvvnlwBoq4twQkeYX22uK57Qy+d3JkY6iFnn5tdHaKpvIJLWEbpGU4oeKXokKIrQSywXwyWY7uwXd6dowGMVHVscA1E1WVEJvWNphlzC3W8qa2V73xgTfqeDdFcyQs7MFx07njbIB6KEZIYoxauzR+vbAPD5BO8+fwn7RqwVh+rCrO1q4oEPnkdN6eLKXtOzVokQgqvXHo/fTDuTRekIXaMBtKAfGYoi9Jj3SFEoidBtDz1b3CFqM4VI9o9mSOAI+r6cEsEdfeNEGtoK2wdlnP0jxZOPjWUMZCBKMJ8mKrJkA8556pqcYfpvPmshLbXqYTPpWpyHIOiAM5DInlEvoAcWaTSgBf3IMFmE7u4ULYrQ7SyXTHHKoo17kQMP+lwRuomP/Wl1jhd6x6h3iXJCxnnEGt0JkDFMskYeEYzhN9NEyTAWbMZEWT7BeEuhbCTo58OvXM68usjka3EesqBbEbn9q0VbLhoNoAX9yGBPuF+I0F0RseEVoYviTlEvy6WwUnslQc+QDalBQ2O+OobTBiOpHL2jGVrbFhTKdR3XybcfeZGsoWyXiYwaTiBCUUQuTV3AIEWYUVHnWd+bz1rIo5+8mHCgxON3416Q4WCwBTyZUOmaXr9YNJpjEC3oRwJ3/vRknaJ2hF7TSmEldyNTteUyms6xZa8aCNQ7msZXo/Yl/fWMJHOFhZUXLLAG/oZqee+lq9k7kua+J9RcK+NpNVLVF6qBXJI6f45kPshgvqasPpspV3Oa4tfElLgjdO2fazQFtKAfCSa1XLKOJ2xH6LXtymrJJdX+SS0XR2C/88iLvO7bfyZn5ukbyxCqUxkp6WADw6lsYYHlxZ3zrQV3m7hgeSsr58X52VNq7NhYRllA/nAU8jnqfCkGs34GZW1ZfVVzyJaLFaGnEtpu0WhcaEE/Enh1ihZWSs44UadtzdhrUyYHLUH3itBty8URya37Rknn8uwfSdM3miFap/zubLiR4WSO7b3jhAM+OptqsRfkFUKwYl6cfaPqIWNH6MGwisjr5TijRtBJdaxiYFB5W6dJ0JODWtA1Ghda0GeS9d+Gn7yrfLsdkQfCVjQuHfE2c+WCXmutX2rPI+7lodeo6DsbbqJnWJ3/hV5lqewZUqM3GxvqIFyHGWkmY+R5/KUES9tq1cyINa0QU4LfFg/TN5pBSsl4Rgl6IKIEvdYcIUWIQVlHPlR7cFPXWm216ztgCpZLQlsuGo0LLegzyZ7HYfdfyrcbGTXSUwhnxKftnZsZJ4/cFv6oNQNidkIJvpfl0nAcXPst/u+OFVz9739iNJ0rCPuWvaNkzTzt8Qi87jZ6Vr8TgGe7R1izwOrcvPIrcPE/ACrlMGPkGU0bBUEPRZWgh/JJUjLMj0OvwXfd7Qd3X5ZcBFf9Kyxcd3DH21F5dlxH6BqNi6N3kei5gJl15mJxkzdUdgY49ok9Ta6ZLY/Qw5bo5lKW4Hsvtfb8/Gu470d/BOAXz+wtbH/yZZWv3VYXhhWXc/HSPP+9aISckWdNhzWoyLV4rZ1y2D+WZsyyXMJRp840IcLty2DF2VPfAy8CITjjnQd3LBRH5VrQNZoCWtBnEjPrvaq4mQO/det91qst/IZL0O0IPWILerLySFHgm7/fQSToI53L81/r1SRcfp/gyd1qnhZ7sE/A7+O0hZVXCrLL9Y1mChF6JOYMJkoRLp6f5XDjFnFtuWg0BbTlMpOYWTVjYin5nEeEbgm623LxitArjBTtHU3zy2f38vZzFtPVHGPrvlGCfsEJHfXstxZtbotPMtjHhR2h946lGU8b+H2CUMQRzjQhVi+oq3T4zBNwC7oeJarR2GhBn0mMCpaLmXNE2Rb2vKHEX+adCNQzQvceKbp+5yBSwqtPml+Y+XBxSw2Lmx0hnnT0pgtb+O0IvTYcQIScaXVvOGc5b1x7XKXDZx4doWs0nmhBn0nMjLfl4uWh5w1nlGjQEk9PD714pGg+r9Id1+9MEI8EWDW/jnXHq1TCZW1xOhqV+MXDAWKh6hy22nCAWMhP31iGsbQSdLeILmhpIug/gv913HO3aA9doymgBX0msTtFS5fU8/LQzZyT6VKa5eKO0F0jRXtH05z0uV/zmy29PLZzkDO7mvD7RCFCX94eL8xJ3lpldA5qpGdbPEzfWIbhZJZ4JHB0RcU+n2O7HOm2aDRHEVrQZxJ7KtxSH90zQncJemmWSzCmRnLmUkUjRZ/tHmE8Y/D5Xz7HzoGJwtqbCxqifP8dZ/L2c7rosNb1bJ9s9kMP2uIRekfSPL1nmNXz646+zBK7DUdDWzSao4SqBF0IcbkQ4nkhxA4hxM0e+xcKIX4vhHhKCPGsEOLK6W/qLMQW6FIfPW+Ue+im23IpidB9AWdlI9dI0e3W0P09CVXOjswBzl/eSn0sSKdluVTrn9u01YV5pnuYwYks65Y0H10RursNWtA1mgJTCroQwg/cAlwBrAbeJIRYXVLsH1BL052KWnP0W9Pd0FlJYbBQiaCbOcdqsayXL/3PJn751G4A7t9izfNtRei7hrIMZH3c85fnMXPOSNHtveO014VZ0lJDTcjvDBJysaAhihDQXnfgEXrGmnHxrCVNR3GEfhQ8XDSao4RqesnOBHZIKXcCCCHuAa4FtrjKSMBWk3pgLxon4s4bxdvz5Vku23oG8UUbeDWwddDkGihMzrV5f5KT8yGaAjn8ebNguWzvG2PFvDo+ftkKekfTBDw6KiNBP7e8+TRO6qwv2zcZdkQ/ry7CwqaYGtUq/CDNo0NEteWi0ZRRjeXSAexxfe62trn5LPBWIUQ3au3R909L62Y7hdxyrwi92EOXZo5USk3alchaz1lr+tyeUTUHeVetOo/0B8nnJTv6xlnWVssJHfVcsqqdSlx54vxC52i12KmL65Y0OdPhHk02R6EtR8HDRaM5SpiuTtE3Ad+TUnYCVwI/EEKUnVsI8W4hxEYhxMb+/v5pqvooxl5OztNDt7NclKALaZBKKwFPSjW/i7Qi9D0jOQx/hAafNQOi6aNnOEU6l5+xEZu2RXPWEteMiEdTVGwPKNLLz2k0BaoR9B7APYqk09rm5p3AvQBSykeBCFA2lZ6U8lYp5Vop5drW1taDa/FsYlIP3Y7QlbAHMcnYgo6KjqXVKdo9kkMGotTkVSfocMbHC73q/bL2mRH0tV2NvP/ipbz6pPnOxqNK0HWErtGUUo2gbwCWCSEWCyFCqE7P+0vKvAxcAiCEWIUS9GMgBJ8E01CjPqEqDz2ASTajIvJUiaDvHs7iC8UIG0rEE2nJdmu1oaWtcWaCcMDPR161gnjENc3AUWW5HEUPF43mKGFKQZdSGsCNwEPAVlQ2y3NCiM8LIa6xin0EeJcQ4hngbuDtUpaOpjnGMF1rg5ZF6IYry0UJZtAt6FIJus/KctkzauAPx/Bn1XJyA2m1uHNbPEx97DCup3k0ZZZoQddoyqhqLLiU8gFUZ6d726dd77cA505v02Y57rVByzx0d4Su/gQBDPxSlUujPHRBHin8mHkIRWoQCSXw/UnJ04PDnNBxYJkrh0wwBsLnzOF+JNGWi0ZThh4pOlMYLkE3SywXjyyXgDAJocplCWBI9aeRluBHYo5Xvn0wq0aGLj7IRZYPlmDUEvUpFoE+XG1xv2o0mlku6Pk87N8MPU86CypXQy6tjtm/WZ2jmnrGS7oEMmOQTRZvG+9z3pdG6EYGUsPW5/KRokFMQkJF6BmC5IVfnQb1Gq1xBP3FhCq3bslBrsl5sNiLWh8N6Ahdoyljdgv6lp/Cd86F2y6CX/9D9cf95lPqmO+cC8/999Tlt/0Svn6CI8gAP3orPPgx5/PAdvjqcvWggGJBN3Pwp6/B7Zc4n31KqB3LxSRoReihUMQRdKH219Y4o0CzBKgNBzxHhs4otW3OeqBHmppWlbIYPoILbWg0Rxmze8WiZEK9huIw0Td52dLjalphoh+Gd09dfmyfGoafSjjre47uczo2AUb3AhLGe9Vnw9Upms+p/WP7nc8laYsBHFsmEomCEQATctJPazxMwLXARFYGOH1Ro+fI0Bnl4k+pdU2PBk59Kyy58Oj5xaDRHAXMbkG3E2miDc5EVtWQNyDapMTJfihMhi3O7jrMTLFo2/sKo0NLPHQz65Q3vS0XH+p6YrEojCtBT5mCjqZokbWQI3D47RZQ99l+oB1pAmFoPv5It0KjOaqY3ZaLtKalDdUeuKD7AhBrrk7QbXEuEvRccTpizvLT8x6Cbk+Na8+Nns+VpS26LZeaWBR/QG3PEeCSlW1FkejCtgYuP2Fe9der0WiOCWa3oNvzjIdrHUGt9jifH2JNkBycunxB0F11GJniXPNChG4UHwNK+AvRebZkCTprpKgwCQt1bG20hqAl6B1Ncd5/ybKiCP3f33o2i1ucJeE0Go0GZrugT0uEXoWge1ou2eLUxNIIvchDNxyBNzLFHroQ5EUAvxWhZ6Wf+ppQWQRf5BV7LBKt0Wg0s1vQDzpCtwQ9Wm2Ebom0uw4zWxyF22Kf94jQ3YJul3OJcl4ECGBSH8yTJUhDNOjKgrEF3ZWeFziwxSo0Gs2xwewW9EKEHp9hD70kQpdyEsul3EOX7mg+Z2WJuDJk8iJAEJP6kCRLgIZYsGwBjKJZBY+GkZoajeaoY5YLupXlEqo5QEG3PfRmyIyUz7VSSqmHnjcBWaFT1IrQXXZMJpNxzmGn/bkidFP4CWAQD+bJEaAh6rJcvCJ0LegajcaD2S3oRZbLwUTo1tD51NDk5Y0Su8SOzKtMW0ym0s4x9uhSn1vQVYReG8iTlUE14dZkHrq2XDQajQezW9ClCQgI1lipgVNE2jalgj6Vj15qubgzVmzK0hYdsU+m085DIaumvS1YKShBD/vyRH2mslyKPHSrnFvQfbpTVKPRlDO7Bd22TuzVa6qN0t0eOlQh6Han6CSDh6ypbpPpDL2j6SLLJZV2LJe9/QNqoztCx0/IZxLxGZaH7pXlEnOO883uP5tGo5kZZrcySFMtXGxHr1ULustDh6kFvTRt0XRF6LaPb+3747a9vO+HTxaJfTqdLnz+1/95Wm10eeiGCBAWeWr9eQwRpL0u7OGhW9eo/XONRlOB2T30vxChW9FrtamLZRH6FJkupZ2ibmvHzEEgVNiXTmfolekiyyXt6hSNSCX8IxmJPZu5iZ+QMGmNBWjsaCZYFKGXWC4BLegajcabWR6hS7XgwgFH6K48dKjCcinpFHV3hpb463kjy3jaIG9ZLob0qSwX63MUVf43zzsPkRwBQsJEmFmCIcs+Ks1D9wfVe7/uENVoNN5UJehCiMuFEM8LIXYIIW722P81IcTT1r8XhBDDXueZdgqWix2hH6CgByOqQ/WAI/SSYf2ufXkzx3jGwMimycgABn6yWSdnvd6vjn1k+zDpnMrSMfATEnl1XttSKfXQQV2ntlw0Gk0FprRchBB+4BbglUA3sEEIcb+17BwAUsoPucq/Hzh1BtpaTt5UHYSFCL1ay8V0BLOa4f9laYvuztDiCF2aOXKmJJNOA9YEWzlnVGlHzIQ0TJiCZ/YMc9aSZnLST1CYYJhOSmLBQ3f9iYIRbbloNJqKVBOhnwnskFLulFJmgXuAaycp/ybUQtEzT2mEblS5alHecCyNKibokpXSFgEjmy7eZ0XsqXSSHH4M/JjZNEi1MlJbREXlJn4e26V+GeSwBN3MlEfoRYIe1RG6RqOpSDWC3gHscX3utraVIYRYBCwGHq6w/91CiI1CiI39/f1eRQ6MQqfogUboRnGEnprcckmllFgbGWuUpytCv+prD7NrYKJQt0+qkaKZdJosQUz8+HLOohAtYbV/QVOc9TvVg8SQPoKYJZaL9cDRlotGo6mS6e4UvQG4T0p7kpVipJS3SinXSinXtrZOw1JmB522aByQ5WLmVESeTZcLushn2bJ3tFB3AHXpmUyanAwgfUF8pvOgaQioCH7FgkaefHmIrJEnW7Bcso6lUpq2COo69ShRjUZTgWoEvQc4zvW509rmxQ0cLrsFnCyXwCF46NGG4rVCvbAEPJ8p7xQNYtAz5ETo9lJyRiZNlgD4g0TyjhVU51MPh5UdzaRzeZ7tHiYn1Vwu3p2iLsulbTW0LK/uGjUazTFHNXnoG4BlQojFKCG/AXhzaSEhxEqgEXh0Wls4GWWdolVG6NJ0WRqh4k5OD/x5ey5zS9Bdo0BD5NifGCl8DloRei6XQRJE+KFGOIIeker9ms5moI8NLw2xUvpVZG9mnbRErwj92m9Wd30ajeaYZMoIXUppADcCDwFbgXullM8JIT4vhLjGVfQG4B4p7aGTh4GytMWD8ND9oeK8cg/8UtkkAdMSZtegoZAwGEg4k3vZlks+pyJ0fyBIDKe8sNpYVxOlPhpk30iKrPQRkIZqR2ASD12j0WgmoaqRolLKB4AHSrZ9uuTzZ6evWVVid4r6g0rYD8ZDD4TVgyHvitpd5AyToDRAQEhmlM1juiN0g8SwE6E7gp7BIIA/6CfGmHPCwuRcQZprQwxOZMnk/fjLOkU9InSNRqOZhFk+UtSK0IVQUXo1gp7Pg8zz661Wlo0toBVsl/1D4/iEJEUYH5LE6HiZ5ZIYKRd0aWbJyQDBYIiYcP0CKEyfG6C5JkRiPEtG+gjKDCDLLRf/7J6dQaPRHD5muaDnVacoKB+9GsvFSsB5dt8EI6nclIK+d3AUACNYB8CufQNFZbsagkUPkoBQ5/fls2QJEglHaAi4zu1a4KKpJsTgRIZs3k/ILFmaTkfoGo3mAJndgp7POzZJMAq5KgYWWSsKGdLHhl0JJw3Q8Bb0fVaHpz/WCMBL+/uLPPSVrWEiqGMlgiAmkaCPECamL4DwBwmarl8OOWeBi6aaMPuG0+TwE8pbZQojRbWHrtFoDozZLejSdEXoseoidFvQ8fPYrkFHME3vjtG+YRWhR+JK0Lt7B4vEf3lziKhlqWT9MYKYtNSGCZEj7wt5WCZWn7E/SHNNiLGMgYHLu59spKhGo9FMwuwWdHdHZjBapYeuBN3Ep4be2551Bculf0h1aPqsCP3Fff3FlktjkKgVoad8tYR8JvFIkBA5TF+osmXiC9BUo8TbcPdNTzZSVKPRaCZhdgu63SkKB9ApanVaigCbe0ZISev4CpbLwIiK0Ik0ANDTN0gm49RTGzBpDKqHRMoXIyRM4uGAGvnpC1UWZCvLBdRcLgXKJufSgq7RaKpjdgt6UYQeqcpySWeUPbKwJU5ewr89slvtMLM8+fIQX/7VtqLyg6NWmmFELUcRJkv/8DhpoeYtF2aOeTFlo0xQQ0jkqY0ECJFD+kPFlomdLw+Wh25F6NJtuZR0iuoIXaPRVMnsFvSyLJepI/TRpOo4Xb6gkUtXtTNgPQMymRT3btjDtx55kT0JtXFoIktipFjQ474sL/UmGM1bC1GYWRZYgj6G8tBrwwGCGBAIFgtyqNZ573cEvShC95d0imoPXaPRVMkcEHS35TJ1hD6WVKIfj0W4/W1red2ZSwDYNzjC9j4l3va0to+/lCBkzc1CVFkuK5oD9A2PkSGEFD4wMiyoUYI+nI8QEAa1kQBhDCXObsskHFevVu58c40S76JO0dLJuXSErtFoqmR2C7o9lwtMGaH/ZccAv9q8j9EJFaFHw0pMFzSr/PK9gyNs71UdoI9Z09qu3zlIbcCaONKK0Fe1BAhhEIlGEf4wmFnao2qu833pIAFM4iE/YZFDBMLFEXbYitAtkW6sUa/enaI6y0Wj0RwYs1vQD6BT9BsPb+eLD25jPGUJekQJ5/wmJdTP9wwymjbwCSdCf2xngtVtlrVidYqeOj9MW0zQEK8pTOzVHDZJyyAZGSAgTepCAgBfoCRtMWRF6FbUHg74iYcDFSwXLegajebAmN2CXpq2aFQW9O6hFHuHUwUPPRZRQh0Oq9dNL6upAC5e2cbLiSTb9o+ydf8oq9usmRytCH1eVHLmcbUE7eXgzCyNQZMUYXIE8GNQF1IRu4rQ3ZaLHaE7It1UGyrJQ7c7RXXaokajOTBmt6AXRehRlWNuL9rswjDz7BtJkzMlewZUGmJt1IqErTTBXEYJ/VvWLQLg5p9sQkpY2WZZIJaHTi6tBiEFwiqaNrIE8mlyvjAGfvzSIB5Unro/GC7pFK1Rr66ou6mmRNB12qJGozlIZrmg54tHioJnx2jvWAYzr0R2d78S9FDQEmrLsw5hUB8Ncv6yVlbOi7Nl7yiLW2roarDKBaJKwHNJ9dDwWxksZhZyKfL+CDlL0OuDynf3B10euj/kslMckW6uCZWkLZZOzqUFXaPRVMfsNmhL53IB5aNb9ohNd8IR+d39o+AD4XcJLRAUBsvaavH7BL+66Xzn4Kf+yyoXtHLdU2re8lCtiqbNDJg5RCiGkVJtiftVZkwgFAG/NWDJH3YyWPzFEXrK03LRHrpGozkwZnmE7prLxVqG7oXuXl7sV+mHW/eNsm3/KD3Djreey1mWjHs+dNQ0uMvarU7LP38D9m9W7+3FLwJhJzXSnrfcH1QjTHNJAuGaQrZKrbXMnIrQLYH2B10ZLK4IvVZ57wVKLRcdoWs0miqZ3eGfu1PUEsKvPbiJbfkhfvn+V/C2Ox6nuTbMFSfMAyAeCRDIWmmIJYLZFIbFS5qUgP/mUzDeC/P+yfHk/SGVR54eUYIesCwUMwvpUWK19YhBdc55URWhtzXVA+NO+2xBd4n0CQvqGYxFsNPdC2U618KKK6Fx8fTdL41GM6epKkIXQlwuhHheCLFDCHFzhTJvFEJsEUI8J4S4a3qbWQF3p6glkqlMhl0DE/zNnRvpG8uwbf8om3tGaI2HWdJSgx+VgeIIunoQ3HTRIq49pQOSKmWx8GrPwugPQawZUkNK9P1hy3LJQnKQaEM7H7/yRAAarDVEl8xvLY7QA+Ue+lUnzefL1691rskW9MYueNPdEHJNF6DRaDSTMKWgCyH8wC3AFcBq4E1CiNUlZZYBnwTOlVKuAW6agbaW447QLZHMZZUAP7pzkJbaMFLCI8/309kYpbMxVlhRqKizEpzJuZKDxa/2zIqBMESblNCXdoqmhiDW5HjjGWtCr2DUicb97gi95IeRz8Ny0Wg0mgOkmgj9TGCHlHKnlDIL3ANcW1LmXcAtUsohACll3/Q2swJSOh66JZJGLsvy9lqEgH9548mEAz6yZp6OhigdjVG1die4HgQ+Jai2cKcSxa+20PsCSrSTg8Vpi5lxJeCxZkeY09aSdMGoK0IPeXroap/rs+4E1Wg0B0k1gt4B7HF97ra2uVkOLBdC/FkIsV4IcbnXiYQQ7xZCbBRCbOzv7z+4FrtxWy62SJoGV5+0gCf+4ZVcsLyVUxeq/PHOxhidjdFyywUcLxw8InTLXhFCiXbSWuDCzloZ26fKxZqcNhQi9JgTjQdCTvRd2tHpc0XxQhzCDdFoNMcy05XlEgCWARcCbwJuE0I0lBaSUt4qpVwrpVzb2tp66LW653KxRDIgDGrCzuIR65Y0A9DRGKWjoZKgB51sljJBd607GmuGfA6yY07Wih3Jx5odoU67LBefh+VSGoWXpFBqNBrNwVCNoPcAx7k+d1rb0O/3EQAAFP1JREFU3HQD90spc1LKXcALKIGfWTwi9CAmNWEnr/u8ZerBsaytlqVttYVFnMt860KEbgl0ekSJuZFx8sdjzcXH+F1+t9tyyahJvlSE7pG2WClCD2hB12g0B081gr4BWCaEWCyECAE3APeXlPkZKjpHCNGCsmB2TmM7vXF3ilpRbgCTmrAj1qcvauThj1zAWYubWNRcw6evXKF2+EpGZ5YKOqjOTjPrCLdb0O1OURt3hO7uFHXnu0/loft1h6hGozl4phR0KaUB3Ag8BGwF7pVSPieE+LwQ4hqr2EPAoBBiC/B74GNSysGZarTTuHxZhF4q6ABLWmsRljfdGvNYOMLOVgHHarHfm1lHcGNNrmPCxRkpUZeHblsuAXeWS8hzpKi77XoQkUajORSqSqmQUj4APFCy7dOu9xL4sPXv8OGey8VvWy4GNaFJLstaJLrMcin10EFF60bGEe4yy8VlkZSmLfqtqXOLslzK89DVvuJRqxqNRnMwzO6h/0V56G7LxT/JMR6Cbs1rDihBj8933hd1irojdJcnHoorMXZH6PbcMnbUXWGkqGqLS/Q1Go3mIJndgu6ey6WQ5aLW9KxI3qNTtEjQE9Bi9efaOee20IbrXSNTXZaLLfQFD32kMLdM0RQDgak8dC3oGo3m4Jndgu4RoQcxiVVjuQjXpQfCzgCiVAKa3YKedQ3Z97nEO+Ty1i0rZrIIvZqRotpy0Wg0h8DsFnSPtMUAxhQRupflYs9rnobsONQtUNPjJhPWICJXRB21BD3g8sQLIu9KW7TnZ6/KQ9edohqN5tCZ5YKeLxv6HxImkeAkl+Up6Na85u5BQtEm9dmdtmjvs48p7Sy1hVqaToReiL7dWS6TjBTVaDSag2T2CnreHvFZHKFH/RRSFL2P8xpYFFKRuJ3hEmtyzduSLc9mgeI89ILIu4S6YLm4VyyyPfSSTtvSicI0Go3mIJi9gi4tYS6ZPjcWyE9+XCFCdw8ssjpFk64IvTBvS6Z4BKct3u6RorbIux8SZZbLJAOLCnO6a0HXaDQHz+wV9EKkbV1CIUKXUxxnqIeAO4q3R4oWInSXoE9quZRMCeAZobuG9VeanEsI1X5tuWg0mkNg1s3VevfjL/Mff3iRKBkeBFenqI88PqL+KiL0ssmxrMm5ygQ9AaGa8iH+9jH+EkF3R96FCN3LcvHo/CydSkCj0WgOkFkXobfFw5x8XAN+oYQ7K51I28BfLugTA3D7pTDSrT57CXogrAYQ2ZZLtFGJdGYUxvYXpxMWLJeI+ufe5hWh28cGwpUjdLCmBohMdfkajUZTkVkXoV+yqp1LVrXz9AuNcBc8sXuEs89T+wwCRHwlgt6/Dbo3QO9zUN9p5a6XRughleWSHFSDh/xBOPE6GN+vyp/2V07ZlVfCpZ+DluXQuAgu+TQcd5baV+ShW4Ienw+XfRFWXq3WJL3qX2H5ZeUXduVXoX3Nod0cjUZzTDPrBN3mlM46AP60M8Fphkk44MfAXy7ouZR6tedqyRvlWSZ2p2gq4XRwNi2Bq/6lvOJIPbzCWmHPF4XzPuLs8+oUFQLOfq+z/Yx3el/QSW+ocKUajUZTHbPOcilgdYqOZiR7Ekq0c/gJl1ouuaR6tYf2V7JcZB7Ge4sn4DpQiiwXbZ9oNJrDy+wVdCttMY+PvrE0ADnpJ2wvYGFjR+iTCbrdWTm679AE3atTVKPRaA4Ts1fQrQjdxEf/WAYzL8lJPyFRIUIvWC4VPHRQHaCHJOg+Z+Sq7aFrNBrNYWL2Crp0BL13NM1E1lCWi69ShJ5Tr14eup1Pnh0rniL3YLCjdB2hazSaw8wsFnQViQf8fvpGM0xkDAz8BKnkobs7RStE6HDogm776DpC12g0h5mqBF0IcbkQ4nkhxA4hxM0e+98uhOgXQjxt/fub6W9qCZblUhsJ0TeWYSJjYhAgKIzicp5ZLqWC7pFnfrDY59aCrtFoDjNTpi0KIfzALcArgW5ggxDifinllpKiP5JS3jgDbfTGitBro2E2jaWZyBiAn2DFTtGcc1xZlovHXC0Hi19bLhqN5shQTYR+JrBDSrlTSpkF7gGundlmVYEdocfCRZZLQJYKupfl4pGHbnPIEbq2XDQazZGhGkHvAPa4Pndb20q5TgjxrBDiPiHEcV4nEkK8WwixUQixsb+//yCa68IS7ng0rCyXrKkEnWo6RWfQcrGny9URukajOcxMV6foL4AuKeVJwG+AO70KSSlvlVKulVKubW1tPbQaLculLhphPGOwo2+cnJzEcpnUQ/dYkehg0RG6RqM5QlQj6D2AO+LutLYVkFIOSiktxeR24PTpad4kWJZLfUxF1794Zi/+QIhQRQ99kjx09+Rb0cZDa5f20DUazRGiGkHfACwTQiwWQoSAG4D73QWEEPNdH68Btk5fEytgWS51MTXEfsu+UeI1UUQ+V1yumjx020OPNJQv4Hyg6Ahdo9EcIaZULymlIYS4EXgI8AN3SCmfE0J8Htgopbwf+IAQ4hrAABLA22ewzQprCbr6Gie6rq+NgTlUXK5spKhRHj2Xzmt+KNgPhIAWdI1Gc3ipKhyVUj4APFCy7dOu958EPjm9TZuqUSpCb6iJoJ4j0BSPQaJShD7F5FwwPYJurzzkm71jtjQazexk1k6fa3voNZEwoUCexliQWCTirBlqU81si6WLPR8K/qC2WzQazRFh9gq6leUifD6WtNRwcmcDwh8Ec6qRoqaHh16y2POh4AvoDlGNRnNEmMWCbmWzCD/3vPsMwgE//CoAVXWKVrJcpkvQdYSu0WgOP7NX0C3LBZ+fhpjVqekPOsINIGV1k3MFo8r7ji849HZF6qbnwaDRaDQHyOwVdMtyQbjsE1+w2EM3c04kb7g99BLLJRiFd/4aWlccersu+2cw0od+Ho1GozlAZq+gFyJ0VzaJP1AcodvRObg6RT08dICO06anXXXTEOVrNBrNQTB7c+tcHnoBX7DYQ7f9c5jcctFoNJo5wCwWdNtycUfoluUipfpsWILuC07eKarRaDRzgNkr6K5O0QL2sHvbR7cj9Ej95JNzaTQazRxg9gq6l+ViD7u3o3Fb0KMNJR66FnSNRjP3mL2Cbs3l4h2h24JudYpG6ktGinp0imo0Gs0sZ/YKeiFCL/HQwRktWrBcGpTlIqW2XDQazZxl9gq6p4duCXVphB5tACwx14Ku0WjmKLNX0CtluUC5hx6pL/6sBV2j0cxBZrGgV8hDB28PHVyCrj10jUYz95i9gu5luUzmoYMj8DpC12g0c5CqBF0IcbkQ4nkhxA4hxM2TlLtOCCGFEGunr4kV8JzLpdRDL7VctKBrNJq5y5SCLoTwA7cAVwCrgTcJIVZ7lIsDHwQem+5GejJphO6yXHxBCNVYn7WHrtFo5i7VROj/v737j5GivOM4/v4AB+WUquDFGIGChmpobZRc1CZqGyWtaAu2TRtsm9rUhJhKorFNS2NDjP0LTf2jCamhqak1WtS2piSl0dbYNv1Dyw9RQEUQaYQgIBjRQuXHffvHzHKzy+zdHtzO3sx9XsnlZp+du/3m2bnvPft9Zp65AtgWEdsj4giwEliQs9/PgGVAMUsN5k2K5l0p2tXdn+iP/DfdzzV0M6ueVhL6BcDbmcc707YTJM0BpkXEnwf6RZIWSVorae2+ffuGHGyd3PPQc64U7ZrYf0cil1zMrMJOe1JU0hjgQeAHg+0bESsiojcient6ek7vhQdcy6UhoY9Lb4DhhG5mFdZKQt8FTMs8npq21UwCPg38XdIO4CpgVdsnRnPXcmksuRxKR+hpQj/ihG5m1dVKQl8DzJI0U9J4YCGwqvZkRLwfEedGxIyImAG8AMyPiLVtibhmoLVcsqctuuRiZqPEoAk9Io4Bi4FngNeAJyNis6T7JM1vd4DNAxtgtcVayeXIh/WToh8dTPfrKiZGM7MCtTRUjYjVwOqGtqVN9v386YfVghNnuai/bUzDpOihA3Dep2BcOkL/YE/yfaJv4mxm1VPuK0U1piGhN9TQDx+A7in9JZcPdiffu6cUF6eZWUHKm9DjeH25BepPW+w7Doffg+7J/SWWEwndI3Qzq57yJvS+4ydfIJQ9bfF/7ydlme4p/SWXg2lCd8nFzCqovAk9+nJG6JlL/w/tT7a7p/SftvjhHpjw8f7z0s3MKqS8CX3AEfqxZEIU0pJLmsDjuMstZlZZ5U3o0Vc/IQr1NfTsCL1Wcqk9NjOroBIn9JxJ0WwNvZbQJ06uv5DICd3MKqq8CT2v5JK9wUV2hC71n7roCVEzq6jyJvTcEXrmStFD+5MkXlsLvVZH9wjdzCqqvAm9r+/kEbqUJPXjR/svKqrV2WtntnhS1MwqqrwJPW+EDkkdve9ocpZLdjReK7l4hG5mFVXihJ5zlgskdfRaDb37nPp2cEI3s8oqb0LPmxSFpORSq6Fnk3ft1EWXXMysosqb0JuVXMZ2peehN5ZcPClqZtVW3oTedITeBcePpAtzOaGb2ehR3oSet5YLJFeLfrgXiPySy8RzTv4ZM7MKKHdCH5MT/piuZBEuaBihd8HHzvLdisysslpK6JJukLRF0jZJS3Kev13SRkkbJP1L0uzhD7VB7QYXjcZ2wYG3ku3saHzsBF8lamaVNmhClzQWWA7MA2YDt+Qk7Mcj4tKIuAy4H3hw2CNt1GxSdOa1MGESTJmV3H6u5qLrYHbnboFqZtZurdxT9ApgW0RsB5C0ElgAvFrbISIOZvY/A4jhDDJXs0nRecuSr0af/X7bQzIz66RWEvoFwNuZxzuBKxt3knQHcDcwHrgu7xdJWgQsApg+ffpQY63XbIRuZjZKDdukaEQsj4iLgB8DP22yz4qI6I2I3p6entN7wby1XMzMRrFWEvouYFrm8dS0rZmVwM2nE1RLoi9/UtTMbJRqJSOuAWZJmilpPLAQWJXdQdKszMObgK3DF2IT0eQsFzOzUWrQGnpEHJO0GHgGGAs8HBGbJd0HrI2IVcBiSXOBo8B7wK3tDBpoPilqZjZKtTIpSkSsBlY3tC3NbN85zHG1EJQnRc3Msspbs/AI3cysTnkTerO1XMzMRqmSJ/ScG1yYmY1S5U3oLrmYmdUpb0L3pKiZWZ3yJnSP0M3M6pQ3oXuEbmZWp7wJ3Wu5mJnVKW9C91kuZmZ1SpzQXXIxM8sqb0L3pKiZWZ3yJfT1j8LyK+HwAY/QzcwyWlqca0Tpngw9F0PPJXDp1zsdjZnZiFG+hH7JTcmXmZnVKV/JxczMcjmhm5lVhBO6mVlFtJTQJd0gaYukbZKW5Dx/t6RXJb0i6TlJnxj+UM3MbCCDJnRJY4HlwDxgNnCLpNkNu70E9EbEZ4DfA/cPd6BmZjawVkboVwDbImJ7RBwBVgILsjtExPMRcSh9+AIwdXjDNDOzwbSS0C8A3s483pm2NXMb8Je8JyQtkrRW0tp9+/a1HqWZmQ1qWCdFJX0b6AUeyHs+IlZERG9E9Pb09AznS5uZjXqtXFi0C5iWeTw1basjaS5wD/C5iPhosF+6bt26dyX9p9VAG5wLvHuKP9tuIzU2xzU0jmvoRmpsVYur6UkniogBf1LSOOAN4HqSRL4G+GZEbM7scznJZOgNEbH1FAIcEklrI6K33a9zKkZqbI5raBzX0I3U2EZTXIOWXCLiGLAYeAZ4DXgyIjZLuk/S/HS3B4AzgackbZC0ajiDNDOzwbW0lktErAZWN7QtzWzPHea4zMxsiMp6peiKTgcwgJEam+MaGsc1dCM1tlET16A1dDMzK4eyjtDNzKyBE7qZWUWULqEPtlBYgXFMk/R8uijZZkl3pu33StqVnu2zQdKNHYhth6SN6euvTdsmS/qrpK3p93MKjuniTJ9skHRQ0l2d6i9JD0vaK2lTpi23j5T4RXrMvSJpTsFxPSDp9fS1n5Z0dto+Q9LhTN89VHBcTd87ST9J+2uLpC+2K64BYnsiE9cOSRvS9kL6bID80N5jLCJK8wWMBd4ELgTGAy8DszsUy/nAnHR7Esm5+rOBe4EfdrifdgDnNrTdDyxJt5cAyzr8Pr5DcoFER/oLuBaYA2warI+AG0mWsxBwFfBiwXF9ARiXbi/LxDUju18H+iv3vUv/Dl4GJgAz07/ZsUXG1vD8z4GlRfbZAPmhrcdY2Ubogy4UVpSI2B0R69PtD0jO0R9ojZtOWwA8km4/AtzcwViuB96MiFO9Uvi0RcQ/gQMNzc36aAHw20i8AJwt6fyi4oqIZyO5HgQ6tPhdk/5qZgGwMiI+ioi3gG0kf7uFxyZJwDeA37Xr9ZvE1Cw/tPUYK1tCH+pCYYWQNAO4HHgxbVqcfmx6uOjSRiqAZyWtk7QobTsvInan2+8A53UgrpqF1P+Bdbq/apr10Ug67r5H/eJ3MyW9JOkfkq7pQDx5791I6q9rgD1RfwV7oX3WkB/aeoyVLaGPOJLOBP4A3BURB4FfAhcBlwG7ST7uFe3qiJhDsob9HZKuzT4ZyWe8jpyvKmk8MB94Km0aCf11kk72UTOS7gGOAY+lTbuB6RFxOXA38LikjxcY0oh87xrcQv3godA+y8kPJ7TjGCtbQm9pobCiSOoiebMei4g/AkTEnog4HhF9wK9o40fNZiJiV/p9L/B0GsOe2ke49PveouNKzQPWR8SeNMaO91dGsz7q+HEn6bvAl4BvpYmAtKSxP91eR1Kr/mRRMQ3w3nW8v+DEOlRfBZ6otRXZZ3n5gTYfY2VL6GuAWZJmpiO9hUBH1o1Ja3O/Bl6LiAcz7dm611eATY0/2+a4zpA0qbZNMqG2iaSfbk13uxX4U5FxZdSNmDrdXw2a9dEq4DvpmQhXAe9nPja3naQbgB8B86P/RjJI6lFyRzEkXQjMArYXGFez924VsFDSBEkz07j+XVRcGXOB1yNiZ62hqD5rlh9o9zHW7tne4f4imQ1+g+Q/6z0djONqko9LrwAb0q8bgUeBjWn7KuD8guO6kOQMg5eBzbU+AqYAzwFbgb8BkzvQZ2cA+4GzMm0d6S+Sfyq7gaMk9crbmvURyZkHy9NjbiPJ7RaLjGsbSX21dpw9lO77tfQ93gCsB75ccFxN3zuSpbTfBLYA84p+L9P23wC3N+xbSJ8NkB/aeoz50n8zs4ooW8nFzMyacEI3M6sIJ3Qzs4pwQjczqwgndDOzinBCNzOrCCd0M7OK+D9Oi3FnzHvYBwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 2s 119ms/step - loss: 0.3785 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.4122 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.4160 - accuracy: 1.0000\n",
            "(None, 19, 19, 19, 1)\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 19, 19, 19,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_793 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_795 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_797 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_799 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_801 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_803 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_805 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_807 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_809 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_811 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_813 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_815 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_817 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_819 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_821 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_823 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_825 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_827 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_829 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_831 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_833 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_835 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_837 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_839 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_841 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_843 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_845 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_847 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_849 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_851 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_853 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_855 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_857 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_859 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_861 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_863 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_865 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_867 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_869 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_871 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_873 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_875 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_877 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_879 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_881 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_883 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_885 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_887 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_889 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_891 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_893 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_895 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_897 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_899 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_901 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_903 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_905 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_907 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_909 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_911 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_913 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_915 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_917 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_919 (Lambda)             (None, 19, 5, 19, 1) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_792 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_793[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_794 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_795[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_796 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_797[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_798 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_799[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_800 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_801[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_802 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_803[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_804 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_805[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_806 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_807[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_808 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_809[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_810 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_811[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_812 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_813[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_814 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_815[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_816 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_817[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_818 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_819[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_820 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_821[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_822 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_823[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_824 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_825[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_826 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_827[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_828 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_829[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_830 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_831[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_832 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_833[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_834 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_835[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_836 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_837[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_838 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_839[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_840 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_841[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_842 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_843[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_844 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_845[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_846 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_847[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_848 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_849[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_850 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_851[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_852 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_853[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_854 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_855[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_856 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_857[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_858 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_859[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_860 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_861[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_862 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_863[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_864 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_865[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_866 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_867[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_868 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_869[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_870 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_871[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_872 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_873[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_874 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_875[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_876 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_877[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_878 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_879[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_880 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_881[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_882 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_883[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_884 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_885[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_886 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_887[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_888 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_889[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_890 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_891[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_892 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_893[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_894 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_895[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_896 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_897[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_898 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_899[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_900 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_901[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_902 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_903[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_904 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_905[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_906 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_907[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_908 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_909[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_910 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_911[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_912 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_913[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_914 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_915[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_916 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_917[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_918 (Lambda)             (None, 19, 5, 5, 1)  0           lambda_919[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_396 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_792[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_397 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_794[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_398 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_796[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_399 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_798[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_400 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_800[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_401 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_802[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_402 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_804[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_403 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_806[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_404 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_808[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_405 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_810[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_406 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_812[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_407 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_814[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_408 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_816[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_409 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_818[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_410 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_820[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_411 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_822[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_412 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_824[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_413 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_826[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_414 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_828[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_415 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_830[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_416 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_832[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_417 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_834[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_418 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_836[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_419 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_838[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_420 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_840[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_421 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_842[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_422 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_844[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_423 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_846[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_424 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_848[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_425 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_850[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_426 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_852[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_427 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_854[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_428 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_856[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_429 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_858[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_430 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_860[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_431 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_862[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_432 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_864[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_433 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_866[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_434 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_868[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_435 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_870[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_436 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_872[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_437 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_874[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_438 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_876[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_439 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_878[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_440 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_880[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_441 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_882[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_442 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_884[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_443 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_886[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_444 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_888[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_445 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_890[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_446 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_892[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_447 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_894[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_448 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_896[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_449 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_898[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_450 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_900[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_451 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_902[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_452 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_904[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_453 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_906[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_454 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_908[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_455 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_910[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_456 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_912[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_457 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_914[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_458 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_916[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_459 (Conv3D)             (None, 17, 3, 3, 8)  224         lambda_918[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_396 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_396[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_397 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_397[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_398 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_398[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_399 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_399[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_400 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_400[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_401 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_401[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_402 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_402[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_403 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_403[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_404 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_404[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_405 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_405[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_406 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_406[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_407 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_407[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_408 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_408[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_409 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_409[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_410 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_410[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_411 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_411[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_412 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_412[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_413 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_413[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_414 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_414[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_415 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_415[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_416 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_416[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_417 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_417[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_418 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_418[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_419 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_419[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_420 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_420[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_421 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_421[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_422 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_422[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_423 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_423[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_424 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_424[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_425 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_425[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_426 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_426[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_427 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_427[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_428 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_428[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_429 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_429[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_430 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_430[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_431 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_431[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_432 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_432[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_433 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_433[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_434 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_434[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_435 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_435[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_436 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_436[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_437 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_437[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_438 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_438[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_439 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_439[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_440 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_440[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_441 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_441[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_442 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_442[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_443 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_443[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_444 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_444[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_445 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_445[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_446 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_446[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_447 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_447[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_448 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_448[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_449 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_449[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_450 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_450[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_451 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_451[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_452 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_452[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_453 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_453[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_454 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_454[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_455 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_455[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_456 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_456[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_457 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_457[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_458 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_458[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_459 (Dropout)           (None, 17, 3, 3, 8)  0           conv3d_459[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_396 (G (None, 8)            0           dropout_396[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_397 (G (None, 8)            0           dropout_397[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_398 (G (None, 8)            0           dropout_398[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_399 (G (None, 8)            0           dropout_399[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_400 (G (None, 8)            0           dropout_400[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_401 (G (None, 8)            0           dropout_401[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_402 (G (None, 8)            0           dropout_402[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_403 (G (None, 8)            0           dropout_403[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_404 (G (None, 8)            0           dropout_404[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_405 (G (None, 8)            0           dropout_405[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_406 (G (None, 8)            0           dropout_406[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_407 (G (None, 8)            0           dropout_407[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_408 (G (None, 8)            0           dropout_408[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_409 (G (None, 8)            0           dropout_409[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_410 (G (None, 8)            0           dropout_410[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_411 (G (None, 8)            0           dropout_411[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_412 (G (None, 8)            0           dropout_412[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_413 (G (None, 8)            0           dropout_413[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_414 (G (None, 8)            0           dropout_414[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_415 (G (None, 8)            0           dropout_415[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_416 (G (None, 8)            0           dropout_416[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_417 (G (None, 8)            0           dropout_417[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_418 (G (None, 8)            0           dropout_418[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_419 (G (None, 8)            0           dropout_419[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_420 (G (None, 8)            0           dropout_420[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_421 (G (None, 8)            0           dropout_421[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_422 (G (None, 8)            0           dropout_422[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_423 (G (None, 8)            0           dropout_423[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_424 (G (None, 8)            0           dropout_424[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_425 (G (None, 8)            0           dropout_425[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_426 (G (None, 8)            0           dropout_426[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_427 (G (None, 8)            0           dropout_427[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_428 (G (None, 8)            0           dropout_428[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_429 (G (None, 8)            0           dropout_429[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_430 (G (None, 8)            0           dropout_430[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_431 (G (None, 8)            0           dropout_431[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_432 (G (None, 8)            0           dropout_432[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_433 (G (None, 8)            0           dropout_433[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_434 (G (None, 8)            0           dropout_434[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_435 (G (None, 8)            0           dropout_435[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_436 (G (None, 8)            0           dropout_436[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_437 (G (None, 8)            0           dropout_437[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_438 (G (None, 8)            0           dropout_438[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_439 (G (None, 8)            0           dropout_439[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_440 (G (None, 8)            0           dropout_440[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_441 (G (None, 8)            0           dropout_441[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_442 (G (None, 8)            0           dropout_442[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_443 (G (None, 8)            0           dropout_443[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_444 (G (None, 8)            0           dropout_444[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_445 (G (None, 8)            0           dropout_445[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_446 (G (None, 8)            0           dropout_446[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_447 (G (None, 8)            0           dropout_447[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_448 (G (None, 8)            0           dropout_448[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_449 (G (None, 8)            0           dropout_449[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_450 (G (None, 8)            0           dropout_450[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_451 (G (None, 8)            0           dropout_451[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_452 (G (None, 8)            0           dropout_452[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_453 (G (None, 8)            0           dropout_453[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_454 (G (None, 8)            0           dropout_454[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_455 (G (None, 8)            0           dropout_455[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_456 (G (None, 8)            0           dropout_456[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_457 (G (None, 8)            0           dropout_457[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_458 (G (None, 8)            0           dropout_458[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_459 (G (None, 8)            0           dropout_459[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 512)          0           global_average_pooling3d_396[0][0\n",
            "                                                                 global_average_pooling3d_397[0][0\n",
            "                                                                 global_average_pooling3d_398[0][0\n",
            "                                                                 global_average_pooling3d_399[0][0\n",
            "                                                                 global_average_pooling3d_400[0][0\n",
            "                                                                 global_average_pooling3d_401[0][0\n",
            "                                                                 global_average_pooling3d_402[0][0\n",
            "                                                                 global_average_pooling3d_403[0][0\n",
            "                                                                 global_average_pooling3d_404[0][0\n",
            "                                                                 global_average_pooling3d_405[0][0\n",
            "                                                                 global_average_pooling3d_406[0][0\n",
            "                                                                 global_average_pooling3d_407[0][0\n",
            "                                                                 global_average_pooling3d_408[0][0\n",
            "                                                                 global_average_pooling3d_409[0][0\n",
            "                                                                 global_average_pooling3d_410[0][0\n",
            "                                                                 global_average_pooling3d_411[0][0\n",
            "                                                                 global_average_pooling3d_412[0][0\n",
            "                                                                 global_average_pooling3d_413[0][0\n",
            "                                                                 global_average_pooling3d_414[0][0\n",
            "                                                                 global_average_pooling3d_415[0][0\n",
            "                                                                 global_average_pooling3d_416[0][0\n",
            "                                                                 global_average_pooling3d_417[0][0\n",
            "                                                                 global_average_pooling3d_418[0][0\n",
            "                                                                 global_average_pooling3d_419[0][0\n",
            "                                                                 global_average_pooling3d_420[0][0\n",
            "                                                                 global_average_pooling3d_421[0][0\n",
            "                                                                 global_average_pooling3d_422[0][0\n",
            "                                                                 global_average_pooling3d_423[0][0\n",
            "                                                                 global_average_pooling3d_424[0][0\n",
            "                                                                 global_average_pooling3d_425[0][0\n",
            "                                                                 global_average_pooling3d_426[0][0\n",
            "                                                                 global_average_pooling3d_427[0][0\n",
            "                                                                 global_average_pooling3d_428[0][0\n",
            "                                                                 global_average_pooling3d_429[0][0\n",
            "                                                                 global_average_pooling3d_430[0][0\n",
            "                                                                 global_average_pooling3d_431[0][0\n",
            "                                                                 global_average_pooling3d_432[0][0\n",
            "                                                                 global_average_pooling3d_433[0][0\n",
            "                                                                 global_average_pooling3d_434[0][0\n",
            "                                                                 global_average_pooling3d_435[0][0\n",
            "                                                                 global_average_pooling3d_436[0][0\n",
            "                                                                 global_average_pooling3d_437[0][0\n",
            "                                                                 global_average_pooling3d_438[0][0\n",
            "                                                                 global_average_pooling3d_439[0][0\n",
            "                                                                 global_average_pooling3d_440[0][0\n",
            "                                                                 global_average_pooling3d_441[0][0\n",
            "                                                                 global_average_pooling3d_442[0][0\n",
            "                                                                 global_average_pooling3d_443[0][0\n",
            "                                                                 global_average_pooling3d_444[0][0\n",
            "                                                                 global_average_pooling3d_445[0][0\n",
            "                                                                 global_average_pooling3d_446[0][0\n",
            "                                                                 global_average_pooling3d_447[0][0\n",
            "                                                                 global_average_pooling3d_448[0][0\n",
            "                                                                 global_average_pooling3d_449[0][0\n",
            "                                                                 global_average_pooling3d_450[0][0\n",
            "                                                                 global_average_pooling3d_451[0][0\n",
            "                                                                 global_average_pooling3d_452[0][0\n",
            "                                                                 global_average_pooling3d_453[0][0\n",
            "                                                                 global_average_pooling3d_454[0][0\n",
            "                                                                 global_average_pooling3d_455[0][0\n",
            "                                                                 global_average_pooling3d_456[0][0\n",
            "                                                                 global_average_pooling3d_457[0][0\n",
            "                                                                 global_average_pooling3d_458[0][0\n",
            "                                                                 global_average_pooling3d_459[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 512)          262656      concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 512)          262656      dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 512)          262656      dense_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 1)            513         dense_30[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 802,817\n",
            "Trainable params: 802,817\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 13s 990ms/step - loss: 99.3261 - accuracy: 0.4940 - val_loss: 93.4759 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 93.47591, saving model to ./mod6.h5\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 91.7551 - accuracy: 0.5060 - val_loss: 86.1647 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00002: val_loss improved from 93.47591 to 86.16474, saving model to ./mod6.h5\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 84.5036 - accuracy: 0.5060 - val_loss: 79.1812 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00003: val_loss improved from 86.16474 to 79.18119, saving model to ./mod6.h5\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 77.5993 - accuracy: 0.5904 - val_loss: 72.5064 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00004: val_loss improved from 79.18119 to 72.50642, saving model to ./mod6.h5\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 71.0039 - accuracy: 0.6988 - val_loss: 66.1606 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00005: val_loss improved from 72.50642 to 66.16058, saving model to ./mod6.h5\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 64.7210 - accuracy: 0.6867 - val_loss: 60.1555 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00006: val_loss improved from 66.16058 to 60.15552, saving model to ./mod6.h5\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 404ms/step - loss: 58.7697 - accuracy: 0.5542 - val_loss: 54.4113 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00007: val_loss improved from 60.15552 to 54.41128, saving model to ./mod6.h5\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 53.0776 - accuracy: 0.6747 - val_loss: 48.9314 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00008: val_loss improved from 54.41128 to 48.93136, saving model to ./mod6.h5\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 47.7275 - accuracy: 0.6747 - val_loss: 43.8249 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00009: val_loss improved from 48.93136 to 43.82492, saving model to ./mod6.h5\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 399ms/step - loss: 42.6237 - accuracy: 0.7108 - val_loss: 39.0630 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00010: val_loss improved from 43.82492 to 39.06299, saving model to ./mod6.h5\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 37.8795 - accuracy: 0.6024 - val_loss: 34.4284 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00011: val_loss improved from 39.06299 to 34.42840, saving model to ./mod6.h5\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 33.4317 - accuracy: 0.6747 - val_loss: 30.1994 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00012: val_loss improved from 34.42840 to 30.19938, saving model to ./mod6.h5\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 29.2243 - accuracy: 0.6988 - val_loss: 26.3360 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00013: val_loss improved from 30.19938 to 26.33596, saving model to ./mod6.h5\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 25.3687 - accuracy: 0.6506 - val_loss: 22.6186 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00014: val_loss improved from 26.33596 to 22.61859, saving model to ./mod6.h5\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 21.7804 - accuracy: 0.6988 - val_loss: 19.2768 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00015: val_loss improved from 22.61859 to 19.27683, saving model to ./mod6.h5\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 18.5018 - accuracy: 0.7470 - val_loss: 16.2826 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00016: val_loss improved from 19.27683 to 16.28256, saving model to ./mod6.h5\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 15.5150 - accuracy: 0.7470 - val_loss: 13.4860 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00017: val_loss improved from 16.28256 to 13.48604, saving model to ./mod6.h5\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 12.8479 - accuracy: 0.7229 - val_loss: 11.0835 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00018: val_loss improved from 13.48604 to 11.08354, saving model to ./mod6.h5\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 10.4660 - accuracy: 0.7711 - val_loss: 8.8916 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00019: val_loss improved from 11.08354 to 8.89160, saving model to ./mod6.h5\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 8.3921 - accuracy: 0.7590 - val_loss: 7.0727 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00020: val_loss improved from 8.89160 to 7.07272, saving model to ./mod6.h5\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 6.6251 - accuracy: 0.7349 - val_loss: 5.6393 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.07272 to 5.63925, saving model to ./mod6.h5\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 5.2089 - accuracy: 0.7470 - val_loss: 4.4112 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00022: val_loss improved from 5.63925 to 4.41123, saving model to ./mod6.h5\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 4.0254 - accuracy: 0.7590 - val_loss: 3.4297 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00023: val_loss improved from 4.41123 to 3.42965, saving model to ./mod6.h5\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 3.2462 - accuracy: 0.6988 - val_loss: 2.8223 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.42965 to 2.82228, saving model to ./mod6.h5\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 2.5966 - accuracy: 0.7711 - val_loss: 2.4956 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.82228 to 2.49563, saving model to ./mod6.h5\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 2.3239 - accuracy: 0.7470 - val_loss: 2.3173 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.49563 to 2.31726, saving model to ./mod6.h5\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 2.1945 - accuracy: 0.8193 - val_loss: 2.0970 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.31726 to 2.09702, saving model to ./mod6.h5\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 1.9713 - accuracy: 0.8072 - val_loss: 1.9271 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.09702 to 1.92714, saving model to ./mod6.h5\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 1.7121 - accuracy: 0.8072 - val_loss: 1.5870 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.92714 to 1.58704, saving model to ./mod6.h5\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 1.4882 - accuracy: 0.7952 - val_loss: 1.4263 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.58704 to 1.42628, saving model to ./mod6.h5\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 1.3199 - accuracy: 0.8193 - val_loss: 1.5931 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.42628\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 1.3730 - accuracy: 0.6627 - val_loss: 1.3816 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.42628 to 1.38157, saving model to ./mod6.h5\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 1.3435 - accuracy: 0.6627 - val_loss: 1.2141 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.38157 to 1.21414, saving model to ./mod6.h5\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 1.1733 - accuracy: 0.7229 - val_loss: 1.1870 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.21414 to 1.18700, saving model to ./mod6.h5\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 1.0608 - accuracy: 0.8313 - val_loss: 1.1219 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.18700 to 1.12188, saving model to ./mod6.h5\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 1.0056 - accuracy: 0.8193 - val_loss: 1.0679 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.12188 to 1.06789, saving model to ./mod6.h5\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.9691 - accuracy: 0.8193 - val_loss: 1.0211 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.06789 to 1.02112, saving model to ./mod6.h5\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.9357 - accuracy: 0.8072 - val_loss: 1.0744 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.02112\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.9260 - accuracy: 0.8434 - val_loss: 0.9746 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.02112 to 0.97461, saving model to ./mod6.h5\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.9224 - accuracy: 0.7952 - val_loss: 1.0308 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.97461\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.8529 - accuracy: 0.8434 - val_loss: 0.9391 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.97461 to 0.93913, saving model to ./mod6.h5\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.8401 - accuracy: 0.8193 - val_loss: 0.9049 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.93913 to 0.90495, saving model to ./mod6.h5\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.8231 - accuracy: 0.8193 - val_loss: 1.0241 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.90495\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.8330 - accuracy: 0.8072 - val_loss: 0.8855 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.90495 to 0.88553, saving model to ./mod6.h5\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.7851 - accuracy: 0.8554 - val_loss: 1.2464 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.88553\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.8770 - accuracy: 0.7831 - val_loss: 0.8980 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.88553\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.8977 - accuracy: 0.7590 - val_loss: 1.1485 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.88553\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.9821 - accuracy: 0.6506 - val_loss: 0.9177 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.88553\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.9108 - accuracy: 0.7470 - val_loss: 0.9183 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.88553\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.8659 - accuracy: 0.7711 - val_loss: 0.9769 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.88553\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.8318 - accuracy: 0.8434 - val_loss: 0.9992 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.88553\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.7931 - accuracy: 0.8554 - val_loss: 0.8456 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.88553 to 0.84557, saving model to ./mod6.h5\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.7668 - accuracy: 0.8313 - val_loss: 0.8241 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.84557 to 0.82415, saving model to ./mod6.h5\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.7211 - accuracy: 0.8916 - val_loss: 0.8941 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.82415\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.7009 - accuracy: 0.8554 - val_loss: 0.8341 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.82415\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.6871 - accuracy: 0.9036 - val_loss: 0.9835 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.82415\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.7002 - accuracy: 0.8675 - val_loss: 0.7668 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.82415 to 0.76676, saving model to ./mod6.h5\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 402ms/step - loss: 0.6881 - accuracy: 0.8675 - val_loss: 1.2218 - val_accuracy: 0.5385\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.76676\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.7632 - accuracy: 0.8434 - val_loss: 0.7908 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.76676\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.7770 - accuracy: 0.8072 - val_loss: 0.9081 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.76676\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.6664 - accuracy: 0.9157 - val_loss: 0.7194 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.76676 to 0.71945, saving model to ./mod6.h5\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.6810 - accuracy: 0.8795 - val_loss: 0.7080 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.71945 to 0.70799, saving model to ./mod6.h5\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.6156 - accuracy: 0.9518 - val_loss: 1.1688 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.70799\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.7079 - accuracy: 0.8795 - val_loss: 0.7124 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.70799\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.7276 - accuracy: 0.8193 - val_loss: 0.6935 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.70799 to 0.69354, saving model to ./mod6.h5\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.6569 - accuracy: 0.9277 - val_loss: 0.8696 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.69354\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.6160 - accuracy: 0.9518 - val_loss: 0.6767 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.69354 to 0.67669, saving model to ./mod6.h5\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.6309 - accuracy: 0.9036 - val_loss: 0.8010 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67669\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 400ms/step - loss: 0.6066 - accuracy: 0.9398 - val_loss: 0.6627 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.67669 to 0.66274, saving model to ./mod6.h5\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.5780 - accuracy: 0.9639 - val_loss: 0.7236 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.66274\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.5838 - accuracy: 0.9398 - val_loss: 0.6787 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.66274\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.5663 - accuracy: 0.9639 - val_loss: 0.6761 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.66274\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.5576 - accuracy: 0.9639 - val_loss: 0.6315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.66274 to 0.63150, saving model to ./mod6.h5\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.5493 - accuracy: 0.9639 - val_loss: 0.6409 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.63150\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.5489 - accuracy: 0.9518 - val_loss: 0.7425 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.63150\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.5317 - accuracy: 0.9759 - val_loss: 0.6156 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.63150 to 0.61560, saving model to ./mod6.h5\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.5385 - accuracy: 0.9518 - val_loss: 0.7400 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.61560\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.5164 - accuracy: 0.9880 - val_loss: 0.6007 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.61560 to 0.60066, saving model to ./mod6.h5\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 401ms/step - loss: 0.5399 - accuracy: 0.9518 - val_loss: 0.6829 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.60066\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.5206 - accuracy: 0.9759 - val_loss: 0.7905 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.60066\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.5389 - accuracy: 0.9398 - val_loss: 0.5740 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.60066 to 0.57403, saving model to ./mod6.h5\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.5179 - accuracy: 0.9639 - val_loss: 0.6153 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.57403\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.5024 - accuracy: 0.9880 - val_loss: 0.6199 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.57403\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4790 - accuracy: 0.9880 - val_loss: 0.5560 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.57403 to 0.55604, saving model to ./mod6.h5\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.5214 - accuracy: 0.9639 - val_loss: 0.9619 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.55604\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.6483 - accuracy: 0.8916 - val_loss: 0.5979 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.55604\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.5440 - accuracy: 0.9398 - val_loss: 0.6563 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.55604\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.5034 - accuracy: 0.9759 - val_loss: 0.5356 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.55604 to 0.53555, saving model to ./mod6.h5\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 407ms/step - loss: 0.5341 - accuracy: 0.9277 - val_loss: 0.5917 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.53555\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.5175 - accuracy: 0.9277 - val_loss: 0.6366 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.53555\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 402ms/step - loss: 0.4691 - accuracy: 0.9880 - val_loss: 0.5216 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.53555 to 0.52165, saving model to ./mod6.h5\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 405ms/step - loss: 0.4571 - accuracy: 0.9880 - val_loss: 0.7018 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.52165\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4812 - accuracy: 0.9880 - val_loss: 0.5134 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.52165 to 0.51341, saving model to ./mod6.h5\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4671 - accuracy: 0.9880 - val_loss: 0.5926 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.51341\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4611 - accuracy: 1.0000 - val_loss: 0.5089 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.51341 to 0.50888, saving model to ./mod6.h5\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.4626 - accuracy: 0.9880 - val_loss: 0.5153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.50888\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4498 - accuracy: 0.9880 - val_loss: 0.5982 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.50888\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.4503 - accuracy: 1.0000 - val_loss: 0.4959 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.50888 to 0.49595, saving model to ./mod6.h5\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.4652 - accuracy: 0.9759 - val_loss: 0.5590 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.49595\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4531 - accuracy: 0.9880 - val_loss: 0.5925 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.49595\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.4368 - accuracy: 1.0000 - val_loss: 0.4853 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.49595 to 0.48525, saving model to ./mod6.h5\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.4527 - accuracy: 0.9759 - val_loss: 0.5455 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.48525\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.4347 - accuracy: 1.0000 - val_loss: 0.4767 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.48525 to 0.47669, saving model to ./mod6.h5\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.4277 - accuracy: 1.0000 - val_loss: 0.5399 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.47669\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4288 - accuracy: 1.0000 - val_loss: 0.5094 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.47669\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.4278 - accuracy: 1.0000 - val_loss: 0.4763 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.47669 to 0.47627, saving model to ./mod6.h5\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4360 - accuracy: 1.0000 - val_loss: 0.4736 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.47627 to 0.47357, saving model to ./mod6.h5\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.4257 - accuracy: 0.9880 - val_loss: 0.5964 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.47357\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.4329 - accuracy: 0.9880 - val_loss: 0.5219 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.47357\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 1s 399ms/step - loss: 0.4240 - accuracy: 1.0000 - val_loss: 0.4897 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.47357\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.4158 - accuracy: 1.0000 - val_loss: 0.4700 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.47357 to 0.47004, saving model to ./mod6.h5\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4191 - accuracy: 0.9880 - val_loss: 0.5618 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.47004\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4231 - accuracy: 1.0000 - val_loss: 0.5256 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.47004\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4281 - accuracy: 1.0000 - val_loss: 0.4736 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.47004\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4238 - accuracy: 1.0000 - val_loss: 0.5636 - val_accuracy: 0.8462\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.47004\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.4210 - accuracy: 1.0000 - val_loss: 0.4755 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.47004\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4408 - accuracy: 0.9880 - val_loss: 0.5095 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.47004\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 1s 404ms/step - loss: 0.4236 - accuracy: 1.0000 - val_loss: 0.4643 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.47004 to 0.46431, saving model to ./mod6.h5\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.4258 - accuracy: 0.9759 - val_loss: 0.4516 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.46431 to 0.45165, saving model to ./mod6.h5\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.4128 - accuracy: 1.0000 - val_loss: 0.4489 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.45165 to 0.44892, saving model to ./mod6.h5\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.4046 - accuracy: 1.0000 - val_loss: 0.4455 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.44892 to 0.44550, saving model to ./mod6.h5\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.4043 - accuracy: 1.0000 - val_loss: 0.4794 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.44550\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 1s 402ms/step - loss: 0.4109 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.44550\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.4117 - accuracy: 0.9880 - val_loss: 0.4431 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.44550 to 0.44307, saving model to ./mod6.h5\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 1s 400ms/step - loss: 0.3978 - accuracy: 1.0000 - val_loss: 0.5257 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.44307\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.4138 - accuracy: 1.0000 - val_loss: 0.4451 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.44307\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.4122 - accuracy: 0.9759 - val_loss: 0.5252 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.44307\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.4188 - accuracy: 1.0000 - val_loss: 0.4447 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.44307\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.4204 - accuracy: 0.9880 - val_loss: 0.4458 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.44307\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4013 - accuracy: 1.0000 - val_loss: 0.5031 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.44307\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 1s 401ms/step - loss: 0.4092 - accuracy: 1.0000 - val_loss: 0.4426 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.44307 to 0.44260, saving model to ./mod6.h5\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3972 - accuracy: 1.0000 - val_loss: 0.4507 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.44260\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3967 - accuracy: 1.0000 - val_loss: 0.4296 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.44260 to 0.42963, saving model to ./mod6.h5\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.4060 - accuracy: 0.9880 - val_loss: 0.4250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.42963 to 0.42496, saving model to ./mod6.h5\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3937 - accuracy: 1.0000 - val_loss: 0.5203 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.42496\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.4004 - accuracy: 1.0000 - val_loss: 0.4246 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.42496 to 0.42459, saving model to ./mod6.h5\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.4087 - accuracy: 0.9880 - val_loss: 0.4302 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.42459\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3958 - accuracy: 1.0000 - val_loss: 0.4911 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.42459\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.3952 - accuracy: 1.0000 - val_loss: 0.4207 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.42459 to 0.42075, saving model to ./mod6.h5\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3994 - accuracy: 1.0000 - val_loss: 0.4204 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.42075 to 0.42037, saving model to ./mod6.h5\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3846 - accuracy: 1.0000 - val_loss: 0.4599 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.42037\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3894 - accuracy: 1.0000 - val_loss: 0.4277 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.42037\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3855 - accuracy: 1.0000 - val_loss: 0.4232 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.42037\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.3824 - accuracy: 1.0000 - val_loss: 0.4168 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.42037 to 0.41680, saving model to ./mod6.h5\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.3830 - accuracy: 1.0000 - val_loss: 0.4210 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.41680\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3801 - accuracy: 1.0000 - val_loss: 0.4279 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.41680\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 1s 385ms/step - loss: 0.3795 - accuracy: 1.0000 - val_loss: 0.4168 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.41680 to 0.41676, saving model to ./mod6.h5\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 1s 400ms/step - loss: 0.3760 - accuracy: 1.0000 - val_loss: 0.4146 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.41676 to 0.41459, saving model to ./mod6.h5\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.3759 - accuracy: 1.0000 - val_loss: 0.4499 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.41459\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 1s 399ms/step - loss: 0.3809 - accuracy: 1.0000 - val_loss: 0.4272 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.41459\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.3783 - accuracy: 1.0000 - val_loss: 0.4128 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss improved from 0.41459 to 0.41282, saving model to ./mod6.h5\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3862 - accuracy: 1.0000 - val_loss: 0.4324 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.41282\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.3808 - accuracy: 1.0000 - val_loss: 0.4539 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.41282\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 1s 425ms/step - loss: 0.3793 - accuracy: 1.0000 - val_loss: 0.4272 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.41282\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.3850 - accuracy: 1.0000 - val_loss: 0.4171 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.41282\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 1s 384ms/step - loss: 0.3708 - accuracy: 1.0000 - val_loss: 0.4209 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.41282\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 1s 383ms/step - loss: 0.3730 - accuracy: 1.0000 - val_loss: 0.4169 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.41282\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3732 - accuracy: 1.0000 - val_loss: 0.4166 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.41282\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3734 - accuracy: 1.0000 - val_loss: 0.4124 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.41282 to 0.41236, saving model to ./mod6.h5\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3690 - accuracy: 1.0000 - val_loss: 0.4148 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.41236\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3725 - accuracy: 1.0000 - val_loss: 0.4156 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.41236\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.3691 - accuracy: 1.0000 - val_loss: 0.4166 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.41236\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.3689 - accuracy: 1.0000 - val_loss: 0.4176 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.41236\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3688 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.41236\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.3694 - accuracy: 1.0000 - val_loss: 0.4276 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.41236\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.3686 - accuracy: 1.0000 - val_loss: 0.4126 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.41236\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 1s 400ms/step - loss: 0.3718 - accuracy: 1.0000 - val_loss: 0.4207 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.41236\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 1s 386ms/step - loss: 0.3696 - accuracy: 1.0000 - val_loss: 0.4234 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.41236\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3714 - accuracy: 1.0000 - val_loss: 0.4087 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.41236 to 0.40869, saving model to ./mod6.h5\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 1s 389ms/step - loss: 0.3723 - accuracy: 1.0000 - val_loss: 0.4272 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.40869\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3668 - accuracy: 1.0000 - val_loss: 0.4101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.40869\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.3663 - accuracy: 1.0000 - val_loss: 0.4058 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss improved from 0.40869 to 0.40579, saving model to ./mod6.h5\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3632 - accuracy: 1.0000 - val_loss: 0.4142 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.40579\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.3664 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.40579\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 1s 399ms/step - loss: 0.3663 - accuracy: 1.0000 - val_loss: 0.4045 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.40579 to 0.40445, saving model to ./mod6.h5\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3661 - accuracy: 1.0000 - val_loss: 0.4099 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.40445\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.3656 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.40445\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3665 - accuracy: 1.0000 - val_loss: 0.4346 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.40445\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 1s 402ms/step - loss: 0.3703 - accuracy: 1.0000 - val_loss: 0.4038 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss improved from 0.40445 to 0.40384, saving model to ./mod6.h5\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 1s 399ms/step - loss: 0.3702 - accuracy: 1.0000 - val_loss: 0.4100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.40384\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3653 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.40384\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3661 - accuracy: 1.0000 - val_loss: 0.4210 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.40384\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3682 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss improved from 0.40384 to 0.39837, saving model to ./mod6.h5\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.3678 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.39837 to 0.39684, saving model to ./mod6.h5\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3646 - accuracy: 1.0000 - val_loss: 0.4293 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.39684\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 1s 397ms/step - loss: 0.3676 - accuracy: 1.0000 - val_loss: 0.4206 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.39684\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 1s 401ms/step - loss: 0.3657 - accuracy: 1.0000 - val_loss: 0.3956 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.39684 to 0.39556, saving model to ./mod6.h5\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3647 - accuracy: 1.0000 - val_loss: 0.4085 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.39556\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 1s 392ms/step - loss: 0.3671 - accuracy: 1.0000 - val_loss: 0.3955 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.39556 to 0.39546, saving model to ./mod6.h5\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 1s 394ms/step - loss: 0.3627 - accuracy: 1.0000 - val_loss: 0.3954 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss improved from 0.39546 to 0.39544, saving model to ./mod6.h5\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 1s 391ms/step - loss: 0.3673 - accuracy: 1.0000 - val_loss: 0.4222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.39544\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.3675 - accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.39544\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.3638 - accuracy: 1.0000 - val_loss: 0.4011 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.39544\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.3659 - accuracy: 1.0000 - val_loss: 0.4217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.39544\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 1s 403ms/step - loss: 0.3669 - accuracy: 1.0000 - val_loss: 0.3930 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss improved from 0.39544 to 0.39300, saving model to ./mod6.h5\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3624 - accuracy: 1.0000 - val_loss: 0.3977 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.39300\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3648 - accuracy: 1.0000 - val_loss: 0.4287 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.39300\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.3652 - accuracy: 1.0000 - val_loss: 0.3955 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.39300\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.3629 - accuracy: 1.0000 - val_loss: 0.3956 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.39300\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 0.3597 - accuracy: 1.0000 - val_loss: 0.3894 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.39300 to 0.38941, saving model to ./mod6.h5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9e3gkV33n/TlV1TepW9KMNPexPePL+H7DF8zFQCCAwdgmAWMg7Ca7LLybQAh5w2a9YZcFll2yyQK7BHgJZAkhiXFYswl2uJtLuBljG2zi28zYYxvPaEaWNCOpJfWtqs77x6lTdbq6utUzaEayfD7PM49aVdVV1TOjb331Pb/zO0JKicVisVie/jgrfQMWi8ViWR6soFssFssawQq6xWKxrBGsoFssFssawQq6xWKxrBGsoFssFssawQq6xWKxrBGsoFssFssawQq6xWKxrBGsoFueEQghniOEuFUIcVAIsSCEuFcI8RupY04RQnxeCDElhFgUQvxcCPFGY39JCPEnQognhBANIcRjQogPnvhPY7Fk4630DVgsJ4hTgB8CnwTqwPOAvxRChFLKzwshNgJ3AIvAu4AngfOAkwCEEAL4EvAc4L8A9wDbgCtP8OewWLoibC8XyzONSJxd4OPAGVLKF0dO+x3A6VLKgxnveTnwNeA6KeWtJ/SGLZY+sQ7d8oxACLEOeB9wHcpZu9GuA9HXFwNfyxJzY/9hK+aW1YzN0C3PFD4L3AD8KfAy4DLgM0Ax2j8KdBPzfvZbLCuOdeiWNY8Qogi8CniblPKTxnbT0EwDW3qcZqn9FsuKYx265ZlAAfV/vaE3CCEqwLXGMd8CXi6E2NTlHN8C1gshXnXc7tJi+SWxg6KWZwRCiJ8AG1AVLCFwY/T9kJRyTAixAfgZqsrlv6KqXM4GBqWUfxINpH4VeC7wfuCnKMf+Ainl/3OiP4/FkoUVdMszAiHE6cCfA1eg4pOPAQPA26WUY9ExpwB/gsrYC8Be4INSypuj/SVUyeLrUQ+DceAmKeW7T+ynsViysYJusVgsawSboVssFssawQq6xWKxrBGsoFssFssawQq6xWKxrBFWbGLR2NiY3LFjx0pd3mKxWJ6W3HPPPVNSyg1Z+1ZM0Hfs2MHdd9+9Upe3WCyWpyVCiCe67bORi8VisawRrKBbLBbLGsEKusVisawRVlW3xVarxf79+6nX6yt9K8eVYrHI9u3byeVyK30rFotlDbGqBH3//v1UKhV27NiB6oW09pBSMj09zf79+9m5c+dK347FYllDLBm5CCE+I4R4Sghxf5f9QgjxUSHEI9Gius861pup1+uMjo6uWTEHEEIwOjq65n8LsVgsJ55+MvTPAlf12P8K4Izoz1uB/++XuaG1LOaaZ8JntFgsJ54lIxcp5feEEDt6HHId8Dmp2jb+WAgxIoTY0mNtRsszkeYiPPgPcOEb4FgfaHPjcPA+OPMV7dvrs7DnG3DB9cm2X9wJ+UHYfB4c+CkgYdsl7L33+xQf/RonbVgHl78FWRjiJ7d9ijOvfA0j68YAmPjFHubu+CxnbBhkvuHzwPgcWV1JZWkdl7/+j3DDJtz5SQ48Nc2ThxeRuRIXvvbfUyhV+MsfPoZ7+BHKzUmeHL6UkdoveH7t2+zaWKbaaDEx2+D0jeWu1zl1Y5mNGzbz/dHXcM++KS468jWe+5p34ArJz7/4x5w/JnCE4N4nZ2j6Ydt715fz7NpYYbbe4qHxOQDO2lJhpJRn71PzTM83sKwM6591Hbue9cJlP+9yZOjbUIsBaPZH27JWTn8rysVz8sknL8Oll5eZmRluuukmfud3fueo3vfKV76Sm266iZGRkeN0Z2uAvV+Hf/ht2HIRbDrn2M5x91/CDz4C75lq3/7AP8Bt74BTngPD29W2r/47GNoGb/g8fPM9ICX8qy8z+Y//hef6d6pj1u3gXvd8nv3TP+T7CzNc+YZ/D8C9X/ooL5/+a+RDgkHgsowO045QG398169wxVgDbn8v21D/8QG+/a0zYdfL+cCXH+JDuU9wmbObP2z+L97j/hW7vK8jH1bnPlWCfJju19mvNn5U5BhoTPLO/H/nZz85G+G4XPzwhwCQCC7q0gVbPgwV4HK9fz9I4DQJp/X+27YcR+4a2gKrVND7Rkr5KeBTAJdeeumqa8Q+MzPDJz7xiQ5B930fz+v+V/WVr3zleN/a05/movpanz32c7QWIWxBGILjtG/X59aC3lxMtrcWQUr2TFTxG4s85YywUcyAX+c7Dz7GxcDsEfWQqDUDDk4dZoECu9+8m09+91EenZznW3/wovZbefDL5L7wRn7w0JNccUUFgKsb/43fe8UFvOzbr+KfH/0Fjy2OM1zK8WunD+KMCx77d1dz5ObbOPTQOm57ybf5yO17WGwG/N/feW7mdT7+nUe445u38Df5D0J9jt++YhR+BhNPTUC0HOofrf8wtU2X8O2Hn+Kud/8qeU9tf+jgHK/4X9/n3a88m4/cvofrLtqKIwT/96cH+IOX7eIDX36IL7/j+Zy7dfjY/z0sx8yzj9N5l6MO/QBwkvH99mjb044bb7yRRx99lIsuuojLLruMK6+8kmuvvZZzzlGO8tWvfjWXXHIJ5557Lp/61Kfi9+3YsYOpqSkef/xxzj77bN7ylrdw7rnn8rKXvYxarbZSH2d1EUS/3jfnj/0cfnQOGWRvbxjnDhrgN6P9TQia3HbfOAXRYpGSOqRV50e7xwFYqM4A8K2HJxBBkxYeew5V2TNR5czNlY5byeULAPzssQmajejf2M3x3HN2APDU1DRfe+AQrzhvM05zIf78I/kQ6eRiMQe6XufaC7cyL9W9big0uWSLMhVT04eZnJ4G4CcHfb56/0Fecd7mWMwBztpc4YyN5fg611y4lWsv3EqtFfDhb+7htA2DnLNlqPfft+Vpx3I49FuBtwshbkY9eGaXIz9/320P8GCU+y0X52wd4j9fc27X/X/8x3/M/fffz7333st3v/tdrr76au6///64vPAzn/kM69evp1arcdlll/Ga17yG0dHRtnPs3buXz3/+83z605/mda97HV/84hd505vetKyf42mJFtfGsf2b1lsB1GsUAcIAXKOGP9DnrsabpN/Eb9aZm28w3KojkNx23zjXFcFzhqB+kIeenKJWq0ABmguzBKHk1nvHeWVO0gpz3Ld/hicOL/Lqi7fRgasEPfSb/OyxCZ4NXHLqZspD6wEoU6PeCrnmwq1w+1z8+UXQpFAssXgkYGOlQLXud73OSesH2L55IxyB520vkGstADA7cxgZjUPMy1JyHQMhBNdcuJUPf3MPGysFnr1zFAFsGiowMdfgmgu32sH5NUg/ZYufB+4AzhRC7BdCvFkI8W+FEP82OuQrwD7gEeDTwNEF0KuYyy+/vK1W/KMf/SgXXnghV1xxBU8++SR79+7teM/OnTu56KKLALjkkkt4/PHHT9Ttrm60QzdE1+Q9X7qff/3ZuzL31VsBV3zwW3z53sfVqYJW+wGxQ08eFguLC+wdn+aSD9zO+NQM41OzPD69yGgRysPrALjtp48zklcDiQNykfsPzPLd3ZPsXOcRunm+dv8hpFRutwNPCfqmAfj7ux8D4FfO3Q65ARAOpw2HbKgUuOLUUfWZ9ef3m5QHBgG4+oIt7NpU7nmd55+r/v9dvjUX/90tzB1hYe4IADu2bUmuk+LaSOSvvmALriNwHME1F6ht6QeAZW3QT5XLG5bYL4G3LdsdRfRy0ieKwcHB+PV3v/tdbr/9du644w4GBgZ40YtelFlLXigU4teu69rIReN3umiTe5+c4cCR7L+rvRPzzCy22DjkQBP2T89zyjZD/DIcuhO2qORC3v+Kcxn7rtr2J9dcwPCdIEbWwwRcdfZ6fu300+BrylH/2bcfoRmEnDzs0ZovcGROPTh2bcoQdDcPwDt/ZQcTTwbwMLzgnO2qgqdQ4eWnD3LuFZfhOkLdlwwh8CFokC8U+es3X84F20f4wD8+yH37Z7te59eecxb8AE4fljCnPl9RLiClAw588PVXsNAM1XVS7BgbjK+jecevnsELdm3gtA3lzL9ry9ObVTVTdKWpVCpUq9mCMzs7y7p16xgYGODhhx/mxz/+8Qm+u6c5GaJrMj5TZ3qhSb0VUMy5bft2T6j3nLY+B4fgkYlZTtm2pee5c7QYdAP+5XN2wPcCQPK6y06CHzVUOSOCi7cOwJh6AJdFjdsfmuCk9SVGCpIj+SIAec/hlNHkwR4TOfQdwx47RBkeBjcXPcwLQwyLOsN6wFHfV9BQv014Ba48Q7Wz1rl5t+sUSkOAQDSq8Xkq1AhxCHJldm7IeNgY6Otohoo5XrArs5W2ZQ1gBd1gdHSU5z3veZx33nmUSiU2bdoU77vqqqv45Cc/ydlnn82ZZ57JFVdcsYJ3+jSkR+TS8AOmoprog7N1do61C9ueiSp5z2HDgHKh+yZmeYl5gN9+7iAIyBGQw4+u3VRli/q1W1CC7Dfih8Go14AmXHPBVsRUk1wk6GdsLGe6X+3QCZrJ9V0t6JUk/vEbRtwSXc8rxqfRgt71Oo6jztecj885KOpIKRDF3mJueeZhBT3FTTfdlLm9UCjw1a9+NXOfzsnHxsa4//6kQ8K73vWuZb+/py0Zg6Jfu/8gUqrBas3BmRqHZuv84vACN1ym5io8fKjKGRvL5KQ6x77J9oHVxdoiA4CszyGAufkF1gE5GWXtpqD7DfDySnyDZizoI66Kz665cCt8o0GhqKpLsipcgNihK5GOrqNFXgswpCpvWup6xaRU8MwoZul6HX2+xlz8wNqYbyIBp2CrVCzt2Pa5lhND0Fla+KFv7OHD39zDgZkkOx+frfPJf3qUD3714XjbnkNVJXzRQ+Hxp9pr2cen1ffVqPRwdl5Vg7gyEvKgmdSvxw49r8TY14Le4PpLtquBSb9JrlDk1y/eFg8sdqDdeNBUn83JJbXx+XLym4hZ1aNLKbXwAxsqBX794m29Byn1+aJznj4kOX1IQsHm4JZ2rEO3nBhSg6INP+CxqQUk8MT0YnzY+EyNPRNVZhZb1JoBTT/k0FydXZsrsFs9FA4cXqDhBxQ8lbU3G8pd+4uRsM+rh4YbJg4cSOKRNoeuzlkMFvnT6y+MjmsgChU+fMNF3T+PF4myfih4yWA4hQrMRpOnzbp7P7qecawQovd19Pka1fhc2wd8NbEobyMXSzvWoVtODKkM/bGpBfxQEoSSH+xVszSHih67D1U5OKsEeny2xp6n1PGmQxfSZ9/kQnxqv6mOD2rKDVcX1APCCVvgG5VIQZRntzn06L5aC6q+XZ2wXaCziB26PmfiumMBNj5vfKzfTN7bL/p85jkbVbXdYjGwgm45LkxWG2oyEDAxV+dwVHK3OD/D7GKL3YcSofve3klGB/PsGBvke3sn4+3jM7X4uDM3V+KHgkvY9v6wFYl2JHiLi4nY08x4HTv0RruDN6tRTIHOQu/3m3HlSkxhKFvQ9QCpt8S50xQqKqrS52pGr22GbklhBd2y7EgpedWffZ8PfWM3AG/41I/56b4JAI4cmeZ3b/4Zuw9V8RxBzhVU6z5bR0psHS5RrfvxebSgVwoeW4aLsUPPiZB9k0mUEbaU0ItmNPFmMYlw2gRVv44dulGhYu7vx6E7jsrN9UMh7dCb8yqzb3Po0fWO2qEPqT411qFblsBm6JZl59BcnYm5Bvc9OUu13mLf1AKnjHkwD+vdBj/YO8nsYpNTNwwiEOyeqLJluMiWEVXON5h3WWwFjM/U2T1RZdfmipqmHjn0St5htqYqSxp+gBM2wQGvpUR+YdGYoJQl6F7hl3fo+jyZDj0SWqPUEDDKFo/BoS88BUg1E7VRjScwWSwm1qH/EpTLtsogCx2H7J5QTacAxlQVIEW5SCgl9+2fZdemihrsBLaOlNg2kpQKbigX4gHSeAZl5KaHCsRO/tBsnXxUb54LlDOv10yHPtf52s2rP10dep+C7poOPUPQzdwbDId+DIIeRr+5DG0FpJp5agXdksIKumXZ0SI+W2vx/WjAc8BRebqQIRduUoJ21uYKZ25SD8WtI0W2DCeCvnWkxH37Z5hZbCU9TiI3Xck7zEWCfmCmRh7l1ovhAkhJrb5E5OJFkYtR5dK2P+gjcoGkUsZP5eLdBL21qDpFHsugqGZoa/Z2iwUr6G3ceOONfPzjH4+/f+9738sHPvABXvKSl/CsZz2L888/ny996UsreIcry233jce5uMkPH5nixi/+PP7+YWPA8tb7xhnMu+RF0lDr1WcrIdq1qcKZm9XA3taRElujyOXMTRW2jhTZMzEfHwfEgl7OQ7Wuzjc+U49nhLqE0KrRqJuRy3zna9cYFPXNyGUuuU5fkUs+bs3bl0PX1z/qyMX4TbBiCLotW7SkWL0Z+ldvhEP/vLzn3Hw+vOKPu+6+4YYbeOc738nb3qZ6jX3hC1/g61//Ou94xzsYGhpiamqKK664gmuvvfYZ2Xr0q/cf5K7Hj/AHLzuzbfvf/+wAt9yzn/98zbmU8i57Jqqct22I+w/MsW9ygYtOGkEYwvlr5w7zeKPC804fQwj4refu4MrTN1DKu/zr5+3kledvYb/RqGvXprIaYIxih3JOMN9Qrw/O1MiLZCCVRpWm2TStl0P3Mxy6vk7fDj0qfcwPJNtjQZ9LPVCMQdmjwaxmGTJ62FiHbkmxegV9Bbj44ot56qmnGB8fZ3JyknXr1rF582Z+//d/n+9973s4jsOBAweYmJhg8+bNK327J5xq3Y+dsYnOzKfmG2wdKbF3Yp5/ccUpTMw1mKw2VA35gWZUzjfHiFPjfdddEr//vdcmnTXfc41aTGRLlKePlQuMlgvQSgS+koPqnBLx8dkaReHTcAcpBAv4tVmazSUGRd1Cu0OP7ovmfCLwRzMoGjTAXZdsb3Poc8n54wfKMWTomqFt2dstFlazoPdw0seT66+/nltuuYVDhw5xww038Ld/+7dMTk5yzz33kMvl2LFjR2bb3GcC1bpPvRXSCkJyrkrrglCyN5r8M73QxA8lDT9k1+YKZ01UlKBvrsAvmjAw2i5sPdgWxS9xfm4MXg7mnbbIpSBaNApjFBYXmJs9TKthOvSsQdFcu0MfWJ/cl75OXw49b0wWMhbcaKtyqRqfW1/fZuiW44PN0FPccMMN3Hzzzdxyyy1cf/31zM7OsnHjRnK5HN/5znd44oknVvoWVwwtovNGrfiThxept9QiEdPzjWQi0KZKnH2fubmihHJwTL2pD0HXA6Tp/BxgMKceLlJKxmdq5KRPUFQrBc3NHCFo9hG5mHGJV0r6pejr9O3Q9WShHhl6+nP387AwsYJu6RMr6CnOPfdcqtUq27ZtY8uWLfzGb/wGd999N+effz6f+9znOOuss1b6FlcMXSpoTv4xB0Cn55s8Gk34OWNTmWfvXE+54Km1K4MmDPQv6DvGBikXPJ59qhJq06EP5MAPJfVWyKG5Gjla8bmPHJnGkUYs1HViUSEZ0PTySUfDo3bozc7p/PmUoKc/97GULWrMQVE7U9SSYvVGLivIP/9zMhg7NjbGHXfckXnc/PwvseDx0xAt5HNGjq5LFAGmFhocmKmxfjDPQN7jpeds4mfveamKZ/wGDEbLpDWW/nsbLuWS90LKoasB6emFBo16HYrgVtSiDdOHpymwhKB7eSMuiWZu6n4psUPvQ9C9gprBmZ7O73rRBKAoxhlYn7r+MQ6KesXkXGC7LVo6sA7d0hetIKQW9WYxHfruiSonrx9gMO8yVW1ycKampumjOgnmXCdqYdswnGp/C0XHYg7tGXpkQ56YXoxr0PNDGwGYmzkcTzRS1+rh0OMa8kISucQOvZ+JRV0cOrQ31CoOqzYBx+rQ8+XknG5ORUT6M1gsBlbQLX2x0EhEct54vftQlTM3VxgtF5heaDA+U2drVKESoxeAKJSVEPURuXRglBeWovHHfVMLsXgXhpRDX6geSVYqgt4OHVTDLjdvOPTU6kO9aMvQUyKdL0N9DppRzxWvcOwOXTcTi4W9bN25JZNVJ+hSryyzhnk6fkbTlevB0VYQ8tjUArs2lRkt55mebzI+W2PrcLH9zaZIFsrHJuhGHftAtOTo41MLsUMXxSFaeFRnj7RNYsqucjHcbWNOvdaCrq/Tl0M3Vj3KcujzE8lrN6/EXb/vaClUkizdfG2xGKyqDL1YLDI9Pc3o6OianbgjpWR6eppisbj0wauB2gwELebqicBV6z40F1g8MkUQSjaV4PTCLHdMFanX6+wqHGk/RyyShnACLB5WXwfWq9cH71M58UmXg+PCofthYRLWndLm0Ise5PCpHnw0mVTkFpD5Ms/emMMtDEC0vkTbAhP6ta5y0dvcfJR5zx+lQ89H0/nDTtddqMD0o+p1vhw59GOcKarPp7P0QkVd02JJsaoEffv27ezfv5/JycmlD34aUywW2b59+0rfRn987UY48gTVF/1NvKlab8H3P8TgvTcDf8oFB27m+gOf5uzFP+eN7j9xw103wYv3JbMnzck6pqD/w+8oYfqNL8BX3gX3f1Ftf/3n4eQr4JPPByRUtsC1H4uvX3Ilr3G/x38+8Ndcz39SG708+YFhLtnsQbEE41FZYmNere6DSE39j3KbxrzxoJlLHj59NefKt5/TpLIFHv9+9Hqz2q8fYEeboQMMb08mFQ2fZAXdksmqEvRcLsfOnTtX+jYsJvNPQXW8rfa82vChPo47fwiQVJpPUQrnKdFgs5jGC2qq+kMLulkKaC7+MHcAkMl1hrapbQtPwcKU2je0DaqHOhz6euYo0WCrZ8QYhaFIoItKoBd1vl1S7WZbi8l9aEetW+WmM/S+B0W7lDle/SG49F+r7Vsugm++5+hmoaa54a/BiX5cX/2Jo3+/5RnBqsvQLauL+cUFWotzVBtJLl2t+9CoImRAkSbFUAllmRoVomn36baxkAinzpLbllWbg3U7Orev26E6FNaThaGLjsRDOdSTi4ZIa5cdNFWEIqL/3uYgKEIJoymq+r3mdfodFNWkRbo4BKc8B7Y9Sy2GYZ7vWKpTSuuS3Lw4rP5YLCmsoFt6cujwHLIxFw+KVopeJOhqgLFCjXyglnariBoV0b4cHJA49HTkkl4ns7wJFY1UkwFMPTNyYSo+nUPAgKcEfVsuWlbOzRkDm8ZC0NA+COoVlFtvE+NCUjWir9Nvcy7NUse31anbckPL8cEKuqUnTtgkjx8v66aWiWvFQlwWiaCXqTGa04tBG9UlumwxPSiaFvTicOeCyFrQFxNBJwwoRZUumzwt6KnJQXqZOX1dU9zNrxDNFB1qv06/7XM1S4l0+noWy3FgVWXoltWHFyoxnpycJOcKRst5laf7SnAHqZHz1cDgoKiz3mtAk1TkkuHQdf02qNd6jcx8ub2Bl57qvjCdnC+MHHoTxhyjtryrQ88rV66PM79C8jAwr3PUDn0JkbYO3XIC6MuhCyGuEkLsFkI8IoS4MWP/KUKIbwkhfi6E+K4Q4mlSwmEBtS5nN1yp8u/xpyapFHNG5KKEtCJq8VqeFRYZdqLIxSwXNAdF8xXw60nFB6jXfl255L4cuk/JVZHLeozacl3jHjQ6Y5YlHXok6IuRoB9Ph34sg6IWSx8sKehCCBf4OPAK4BzgDUKIc1KH/Q/gc1LKC4D3Ax9c7hu1HB/u3DfN+e/9Bodms1sCe1Gjq6empqgUPSrFXBS5KMEuU8ONBH3EbVARvQZFDSdcHU/269eFcg9BNxy6DCi6qjqmEkaDmLqCxq9Bc1Fl6ro00Xy9lENfPNYMvU+H7uTUIKnFchzo53/W5cAjUsp9UsomcDNwXeqYc4BvR6+/k7HfskrZM1Gl6YdxT/M0uUjQvdZ8JOgeC/UmtJLcXERu/J3P38w6NyNDN/ujaOGcO5js16/1DMjGvKqE8YqqugPaBkUJA4qRQx8MZtQ2N+Wy067c6+HQzffq6xx1lctSDt3I8y2W40Q/gr6NZN4dwP5om8l9wK9Hr38NqAghRtMnEkK8VQhxtxDi7rU+eejpwuR8FKnM1DL3e9HU+rKoUSnkqBRzyGYi/kNiEScS962lFo6OWpZ06Kaga4deaXfo5uzIxemkDDEMKDpK0IvNaFaqV2gX9I5B0eh17NS7DYpOq7LGfly0GZ0sFaOYeb7FcpxYrt/93gW8UAjxM+CFwAGgI5iVUn5KSnmplPLSDRs2LNOlLb8M0/PKPY/PZEcu+cihl6lRLnpUCh5lmYj/JmFM82/MtdeYazId+oFkf9UU9KGUoBvrc3pR0y8ZUHBU5KLz+zaX3ZjrHBQ183S9TWM2vmrM9T9omX4o9DzWOnTL8acfQT8AnGR8vz3aFiOlHJdS/rqU8mLg3dG2mWW7S8uy8uThRT7wjw8ShJLpHg49CMK4m2FF1OLIpSySY7c5xuBm9VDyOrPKpZA44TkjQ48d+lCnQ/cKKncGJYrChdCnIFJ+wXTo+lq9BkXTYpy+Tj+kHwo9j7UO3XL86UfQ7wLOEELsFELkgdcDt5oHCCHGhNC/D/MfgM8s721alpPbH5rgL37wGPsm55leUGJ7MGNQtN5o4AjlhMvUGCqqyKVMIuhbHcOhmyJtLmKRbs6VPrYtconKFutzqiJGiOQ9bkHFIWHAxnKq4tZ8WED77FA33zkYmhbj9HX6wTp0yypjSUGXUvrA24GvAw8BX5BSPiCEeL8Q4trosBcBu4UQe4BNwH89TvdrWQZmFqMFlmfrPR16rZ5sKxsOvWI49M0Y1SdmjNKrDj19rH4dRywS5g+1t4sFJYqOcugjBaMbp3DUKkHH7NALndfph7aB1X4duhV0y/Gjr4lFUsqvAF9JbXuP8foW4JblvTXL8WK2pgT94EyNKZ2hz9aQUra1LW6Ygk6NZsGjXPRihx7gspEocnG8xGk7XqrKxXDouhnX3DgglDNOD4rq/dsujbZHzls7dBkms0/1dkhycH0t06GnJxZlDWjG1+lX0HPt1+tFnN3neh9nsfwS2ILYZyAzi0pgH59eZK7us24gR70VcmSx1XZcWtArxRxDRY/ByKHPeKMUiMS6skVNDtKvsxy640FuEBDRRKJKMtEI1D4tqno/JH1WvIJy46Gv/mhiQVRTJn8AACAASURBVK60b+vl0B1P3Yfeb77/WCKXJatcbORiOf5YQX8Goh36/QfUpJzzt48AnbFLo5F8X3HqbFtXUrNFI4c+7YwlB+sJQPp1uspF59SOk73yTr7Svk/vN7+6+ThDbxN07bq7OnQzfom2mQ260g+EYxkU7duh20FRy/HDCvozkJlI0P9ZC/o25YrTgt6sJwOlv7KjyAvOGGO4lAyKTjnGVINegh602gUvS9DTX7O2uUmG3u7Qtet21INBH2uKuCnu6ffFDr3ceUwvrEO3rDKsoD8DmY2iFe3Uz9+mHHq60qVpOPRCsIAQgmLOZdit03RKzMrB5OCKKejbVMyis3O9iER8sh6CbrrszEFRT/UtNzN0L+Pc3WaKZlWmuKnI5WgHRc2MvhvWoVtOAFbQn4Foh645c3OFvOt0OPRWUwl86JXaHPd6r0HdGWAujNZFzQ0kU/RBZeiQNOjym8vk0AtRHXoAYcagaNuxqVJFN2swNBXDHHWGnuH6u5FVXWOxLDNW0J9hSCmZrbUoF5ICpw2VAltGivztnb/g6o9+n6eqSsj9SNCD0miboK9z6yyKAeZkNHPTFGavpBZ9hqTS5WgcullLnt7mFaLIJVB/NMvm0Ic6j+lF+oHQi6MRf4vlGLGC/gxjvuEThJKzNivhK3gOg3mXd7z4DK44dZQHxuf4yWOqFFELOgNjbYI+JOrMU2ImiBx6WpjjKfjRe/zGUTj0HpGLmaEHrWSNzUyH3q2XSz8O/SgHRY/KodvIxXL8sIL+DENPKjpnq3KjY+UCQghec8l2PvbGi3EE7DmkhDgR9FEVn4SqIVZF1KjKEkcCI3vuJehBs4tDH0pcsemO05m26bqFqzL0sJXEPFkPi6V6uehjICND79NFO45qF2AdumWVYAX9GYYeCD17ixLQ0XIiRsWcy46xQR7Wgt5Sgu6Uo/LEKBMfpMZsWGRGC3q+nDjrQjmpNOnm0PX+9Ps06ahFD5Smyxa1oGc9LJaaKWq+7qhDPwoXbZ67F9ahW04AVtCf5nzroQmafrjkcbO1Fv+0ZzIW9B2jgxQ8h9HBdoE5a3OFPRNKiMOWmhDkDEbliZFAl+Qih1t5qnGGnnLamQ69z8gFEnHXQm46Z8dRgh74fTj0Hr1c9PuEq2Ic83pHM3BpnrsX6X7sFstxwAr605i9E1Xe/Fd3880HJ5Y89n/evoff/MxPeHRSuex1gzmedfI6zts23Hbcrk0Vnji8SK0ZEEaRixiMHHok0MVwkTlZYj5rULRrht7noGh6P7RPyddli6EPpfXJ9vR7+3Lo+dTD4Cin/qfP3Qtbh245AdhFop/GTMwpB304msrfjSCU/OPPDwJwZzTgOVLK8/m3XtFx7JmbKkgJe5+qEuo+5gOGoEtJPlhgnhJV+hR0vcanxjxW12+3CfoQICA/2L5PO+rQXzpDd/NJ3xQ3TzzNP91/xfz+aDP0+Dp9PACyBmUtlmXGCvpqwG8CsreQBL5ypsYkFt36tlpvdX8fcOdj00xWG+TwuXffBHlaDOfCzmwb2BVVv+w+VMXTE4MGdOQyB60ajgyY7+XQ81G/ltoRdQ2/kS2chSFD0FPlim37Mqb+Bz4Uh9V1MkU5NSgqUn1bQL2vWw17v/QduVhBtxx/bORyvGkuwP+8AB77fvb+B78EH9io/tzxiexjJh6E/7ZVHXPr76ptt7+Pc378hwDM1/3s90Xcdt841+XvZnfxN/mh/3r2FH+T0p9sUef77n9XB/39v4Xb38eO0UHynsOeiSpSO3Qzcolct3LoA0gElEZUYy0nB8URJZ6lEfjBh9U1pvaoyUca7axLI8nr4kj7/pIRBRUNxx7XoftKkEvr2meX6vPly4nDb3ttzG41t6ev0y/5wfbrdz1OjwscxbktlqPEOvTjzcIUzDyhRG3nlZ37px9FufOSOibi5/tn+O2/+Sm3/e7zqT/+MFuDBqFXxJl+RB0wcT/Dc/sAqC4h6F+7/xDv21jFmZJ8uPVaCoUCb/uV0+BHH4Ppveqggz+HxcO4juCMjWV2T8xzthZ07VyDZtw5sUGOBUrc/8I/5/yLX6oGK9/4d7DpPHXsr/05TNyf3MRZ1ySvd74QXvO/VXtcIdTrU1+U7H/Bv4OF30q+L1TUubdfBnu/oWrQw6gO/XWfg3U7kmPPfCW89i9h9HRYtzO6zrPUvtf8b9jxAuM674L5yezr9MurPtL+sOpGeSPc8Ldw6gv7P7fFcpRYQT/e6AWSZZdKFN1kqrQuORa4+/EjHJipsW9ynnBqhq1AwxumpHuYBC3CQL23V+RSbwUcWWyx8eQcTMEngmvZWR7hbVe+EO77u6QnSthSkQ5wyugADx+qImV0P7lScq/R/QZS/XJX2/HSZGbo6S9JLrzr5epPFq4H5782+d58DTB6mvpjos8lXAhriUNPPyS9Apz36/1dZ/2p6k/WdfpFPyz64exXHd25LZajxEYuxxvtcsOONbMVQQsQkCsmxwIHZ6OOhvNNFhcXAVikkPQwCX1koM7Zy6HPN9S+oqsWlghwGBnQg4W55IEStOJ73DJcUn1dgiYtcknuG7RUdg34kRco5k7wfyHHS/6eHOtHLBYTK+jHG724Qy+H7nhqcC5IBH18RpUMTi80qNWUoFfDQvJgMNxytdFd0LXYF12QCCQOw6VIoPU0emjrMb51pES9FdJs1PBFLlk82bimL1Ttdinn9vkXsUw4rhV0i6ULVtCPN7pSRGY79GqtRku4SC+fHItaEg5ger5JvRatENTKJRFJ0ELKpR26jmMKrown0MQO3cmlIhf10Nk6XIxuvU7g5BLh1OWCQD6nHgrFEy7oXrLCkV3OzWJpwwr68SboHbk8dmiGmi9YDNyUQ9eC3ohXDjri5wh0zh76iFALevcMXYt9wQkRjsfVF2zhuadFZYiOl0Q4QavNoQPk8QlEXmXR+pgocikUVLlfKX+CBV041qFbLF2wPxHHmyUcehj4+LgcrsFgUR3bCkKeqirRmlpo0mgoR7pAEb81jQuEQQuHfhx6JMAuIFw+/kZjEM/NpSKcKEMfUQ49L1oZDl2dr1RQDv3ERy4e+LXktcViibEO/XizRIYeBk18PA7Oh3Hd96HZOlKNYTI938BvKgGryQJBS4l+0GriErJpqMB8w0fqN6TQ7j3vhJ0C6HhG5JKI9dhggbzrkKdF6OSNDL0VO/pCUYn+iY9cbIZusXTDCvoy0/RDFpuGY47EJwiC9u0RMvDxcZj3XeYWFnhqrh4vBVcpeEzPN/GbDUJcQjdP4Ld48vAiQeDjErJjdJAglNRaXTL6yKHnHalqxU3SkUv0W4TjCDYPF8njE+rZmaDiltihF8m7Dq6zxNJry41Z5WIzdIulDSvoy8x//Id/5g2fvjPZEGXeP3l0kms/9sOO42XgEwqPwMkxeWSO5/33b3PX46rfynnbhtl/pIYImgROjqHBEr7f4so/+Q61eh2XgJ1jauZht9hFb8+JDIduli0akQvA1pEiBVpIN68eBMKJFpZQx4+Ui6wbXAFBFS4Q/TbiWEG3WEysoC8zT0wvct+TM3ELWu0mD80scDC1ZicAYQspPK44YwubBwWtQPLZHz0OwPnbh6m1AhV9uHmev2sL5ZxkuJSj0WjgEnLKqBb07IHRar3FQN7FkWEkhgaOF/WIkVEHQ0PQh0vkhZ/0OnFybZHLay/dyU1v6Wzuddxx3OzXFovFCvpyox3xbfeNqw2RQz+yUKcZZOTogU/oeIxUypS9kItOGmGy2mBkIMdJ63S1SQvcApWBIjlCXnHeZlwCPEJ2jqlp53NdHPp8w6dSjBpapQXQ8dqy8/grqtIlrx06JAOo0THlgSKnbeijh8lyY34GG7lYLG1YQV9m9MzM2+4bVwOVkUNvtnxagSQMU4OXkUPHK4Df4NoLtwJqtuZoWbnjgvARutVr2OLaC7fiEeAQMhYd061BV7XuqwWhZYagR+eLB0aNSpwtIypDj7sTOm5UtthK3rsSmL9l2MjFYmnDCvoyU623GCp6PD69yA1//mN+sFs5dQflzjtceugTCldNrw+aXH3BFoSAbSPFWKzztBBePnbUz965npwIyYmASjEXXTdb0OfqLXVM6HePXOJ2AmaGrhy6ox16HLkEyfcrgTkOYKtcLJY2+hJ0IcRVQojdQohHhBA3Zuw/WQjxHSHEz4QQPxdCvHL5b3X1I6WkWvd59cXb+NWzNzI53+Cn+w4B4HYRdBH6SMeLBX3TUJF3vmQXr73kpHi9zzw+bq4Qi6hLSNFR5ykX1D9h9wzdjFwyyhbNwVBD0C89ZR3rCpLhoXLq2Og6K5Vft0UuVtAtFpMlBV0I4QIfB14BnAO8QQhxTuqw/wh8QUp5MfB6oEtj77VNvRXih5LNw0X+4jcv41P/4hJyKOesHXrLzxL0nIo2giZIye/96hlcdd5mxgaj2ZiOj+MVEgELW3jReSt5VTbYzaHPN3yGtEM/isilUsyxoSQoFErJsYG/8pFL26CoFXSLxaQfh3458IiUcp9U/VRvBq5LHSMBveTMMDC+fLf49KHaUGKnY5AzNlXYNKgEt1vkIqRuzqU7Gib9XIZKHp4jGHSDaIFkYwq+Hpz0VEvxbg26qvVWlKF3m1hkRi6pc/iN5L50zbo+ZqUiF5uhWyxd6UfQtwFPGt/vj7aZvBd4kxBiP/AV4HezTiSEeKsQ4m4hxN2Tk5NZhzyt0S65UkiE84zRaIp8tKmZcuiOFnQ9+Gi00BVCMFrOM+BGS89pATOOcQgp573+IheRNbHIrHJJTU4KmsagaOrYlXLH5nVtlYvF0sZyDYq+AfislHI78Ergr4VIqwdIKT8lpbxUSnnphg0blunSq4dY0IuJ6OwcUaKzuaKEvVPQAxWl6HrvoH3B59M3lhn0wsihR+7UN+rZQ59y0cuMXPwgZLEZJIOimRl60nCrQ9BNh+7m2n4zWLH82tahWyxd6UfQDwAnGd9vj7aZvBn4AoCU8g6gCIwtxw0+ndAuWUcuAGVXieSpo6r3ScMQ9CCUuDJAOLlkEWHDfQN8+l9eyvaKqwRfO9JWPTlAhlSK2Q59oRFE99OrbNFw3WYDMSmzHbrO0FfModvIxWLpRj+CfhdwhhBipxAijxr0vDV1zC+AlwAIIc5GCfray1SWYD7DoevmXK5Q9ectI0OvtwI8fIS5+nzQLugDeQ8naCrBjyOXdodeKebi+neTuUjky0Wvi0PPqWw9bvFrnCP0AWnMFE1XuayGDN0OilosJksKupTSB94OfB14CFXN8oAQ4v1CiGujw/4AeIsQ4j7g88BvyW7t/9YwOvYoGxm6bp8bly0aDr3WCvAIEK5nOPT2yAVQgusag6ItU9CDyKF3CrreNlT0IMya+h99rx2/Gbno3xS8dOSi69Bthm6xrDb6+qmUUn4FNdhpbnuP8fpB4HnLe2tPP7QjHjIil7RDN6tcak0l6GEPhw4okfeMhSZaqQy94PH41ELH29oioNCY9alxU45fBipqESLJ8tsceqBEXTidnRtPFLZs0WLpip0puozo2KNc7HTojux06A0/wBMBjpczqlx6OXQtwGaGHrBtXYkDM7W2OMe8n64ZupOdyatrpBy6Wba4ktm1nSlqsXTFCvoyUq37DObd9h7hsUPPiFyaIR4hjpsz6tC7OfTukctZmyu0Atnh0tsioG5VLgCtReN8fvt9aIduVrmspJCaxVM2crFY2rCCvoxUdd8Uk8jpOmRELtGgqOvlM+vQY4KofDCucmkX9F2bKgDs1i17jfsBHbkEnRm6jnBMx68zcv2bQlaVy0pOubcO3WLpihX0JfjB3in2H1nsecwD47M8MD6rOhsWUyITZdF6/c+sQVHHy54pCqjBTJ1/d6lDP21DGdcR7DnULuhzZtVNt/a50JHJq/vQDt2MXPxVELnYDN1i6YYV9CX47b+9h/fe+mDPY95324O8/5Y7+a0D72V7fr59Z+S425pzPfEj+Oq/jwZFw3aHnhZ0U1gzM++A4iNf4Y/K/8jDh6qw+2vw6RfDX7yUwqF7GMi7at3Pnhl6re18/PiT8MV/o7730pFLa2WF1Fa5WCxdsYLeg1ozoFr3+ac9TzG7mD21HmB2sYUz+SDPrn2Pi9jbvjPIGBTd+w2485PUm34UuRhVLulB0XhwstBZlQLKMT94K68Ob1erJD3yTTh4H+z/CZWDd3BGFMdkts/NPF8AD34J5ifgnOtg2yVqexy5+CsrpLYO3WLpihX0HkwvKDFtBZKvP3Co63HVegsZzaAc8VIZeCToQmfofhjPtmw06uRFgJczZoqmB0Xj8kFjsWbToYchhC0KTsgThxfxW00orQdgtjrPmZui9reZ7XPdjPMFyoVvvRhe9zkYjCb8Osas0hV16FbQLZZuWEHvwfR84pZvva+9gaQ5b6pa9/GijHzEqbcdF5ctmhOLopzab6iqFC9XSLLq9KCob0YuGZm3VLXhOREgJcwt1MDNIx0Pv9WIB0yzM/TsMkiCjFjFcZOZoqtB0J2ozaTFYomxgt4D7dCvPGOMHz06xcxiIvD/6rN38f7bHiQMJfPNRNCHnNRC0JHjFjJAiGjqvxb0usrbvVy++6BoYFSbdItcQpXFA1QX6+B6hE6ePC3O2hx1Ne7WywU6B0XDoHPg0yxbXA2Ri3XnFksHVtB7MFVVYvrK87cQSnjooKoi+cX0It/dPckD47MsNH2kJBb0ijDdbrKmqJAhedehESSRS6gFvW1iUT8OvTMicaIeK4HfBCdHS+TI47Nrs45cspag05FLreN8HaWJegm6IGOhjBOJ/juwjbkslg6soPdgKnLozz1tFIDdh+YAuO3nKn6ZrbXiyTtuJOhl0pN0omgmEnQVuahjw6aKXNqbc3WpcmmbWJS6htHWVkZxSVN6VLyADdG6pNkZepdB0czIxUvEfjWULdrWuRZLB1bQezA932Qw73Ly+gFGBnLsnlCO+rb7OgV9W0UJYCk0xNZ022FA3tOCrty0bEYljo6nHLFwMhy60VNFi2w68w6DRNBDH1yPWugxVlKLZMTHda1DT58vo9bc9VZH5KI/gy1ZtFg6sEGkwUMH5/jMDx4D4F89byfT8w1GywWEEOzaVGHPRJU9E1UePlRlqOgxs9iKZ2OePlaEA1CShqCbblsagk5UAtmIjtXC6ha6V7l43WeKErYQMqCUcyBoIZ0cC4HLOrMXV5ZDj8+XcvzRQ6GNtshlJaf+a4duBd1iSWMF3eCL9+znlp/uxxGCvOcwvdBktKwGK8/cVOEffnaAL917AEfArz9rO5/90eNMLyjBPX/rIBxIVblkOfQgBKHctGhFvVe0sHr5zjp0s6dKVlWKjkiASg4IfQJcGtKj7IXtx2UtQdftfFmRiwzVAyZXyvz7OyHEGbr9r2uxpLE/FQbTC022ryuxeajInokq842AbSNKvM7cXKHa8Lnpzl/wvNPHOG3DIAD7jyi3vHFAiWXBjFxMty0D8q6jqlwcJeiOFvReDt03HHrmIGay4lA5BwQ+gSjQwGOYVvtxXZtzZZwvK3IBJf6FCitGHLnY/7oWSxqboRtMzTcYHSywa1OFhw9VmZpvMKYd+mYlYkcWW1xzwVaGB9R23eel6EaDnw2jn4rptqUkpwdFI0ft+KnIxSv0duhZkYtM8vPBXIgIWwS4NMmRw28/rlvZop9qn9ttdSN97RXN0K1Dt1i6YQXdYGq+yVi5wFmbK1TrPpPVRhy57NqoBD3nCl5+3mZGSkrUnjysxLXoRPGGKeix2xZx5NIwJhbFgq4F0s33cOg9Ihct6B4qchGqysWTxsOhp0jX24/LzNANh24zdItlVWIF3WA6cuTx7EpgdFCNLA4P5DhldIAXn7WR4VKO4UjQ9x9ZxBGQF1Hb2SyHniu1D4pGAuzpckHTofdszpXdD107/rIncaSPHzl0T0aRSxg9bLrWoZuDojpDz5hYpK+9GmaK2sjFYunA/lREhKHkcDQI2ibokUMH+Js3PzteXm5kQH09cKRGueAhZCTorcWogZWXiHGuBGFAwXPUKkJa0AMt6IZD79WcK26fm+GogcGcRISGoIe15BigY9m4rMil2/R+89qroWzRRi4WSwfWoUfM1Vv4oWR0sMC6wTwbK8qZj5WT2r+T1g8wHAn5SEkJfbXhqwUkAiOvbkYuXYtxbiCeWNQyZormtOBqkfJ6lC26edW7xPG6ZugDHjgywJcuTTwcHbnoh00/g6LRvXWItpmhr4Yl6GzkYrF0YAU9YipqxDWaGgQ1Bd2kUvTi3lBqAQmjokTHLkF75BIPikbHFrSgu306dFBCpgUakpmiwEAUubQih+6GzeQYyIhcouua59NuvVvNetbg6olE2IlFFks3rKBHTM8r4dQCfmYUu5iRi4njiDh+GSrmEtGETkH3ihCGRoauBDQfavHUdei9HLqx0ISJno4PDLgSRwa0pENTegj93rCLQ8/KobsJ+mpZWMJO/bdYumKDyIi0Q3/tpdsRAkYHswUdVI4+W2sphx5kOPS2yGUyEfTo2JJMDYq6GWWLfkPt1/l3Wshk8oAouSEuPi3p0iCHE6YFvcvU//T1ICNyWSVreTq2ysVi6YZ16BG6Va6uajlr8xDvvvqcpBdKBrp0sVz0Ug496tHSFrmEyUxRnXkLLZ5a0HPZDt01Yp+0kBmRS8mTuDKgKR1aeMnDQXYT9AxRXCpy6fa+E4WtQ7dYumIFPWJqvokQsL6HI08zFAl6pUPQVVfGNoceqpmiDSNDHyAlnl4hu32uZ9xTLGTRg8aIXEquxCWgEToETg6hHw5dM3RTFKPz9RW5rII6dFu2aLF08IwR9EOzdZ6YXui6f3q+wfqBPK7T/yo4I9FsUVXl0mtQtBjXoasql5RDN8sWs+rQXUPQtZB5RfU1bKnYBRW5eAQs+o56j4yu1S1Dd5ykv4s+nxb0blUuWec5kdjIxWLpyjNG0N/zpft559/d23X/9Hyz6wBoN0bSDj0f1a93ZOglNSjqtk8sih26awyKZrXPzYpcMhbEKLkhOXwWfJBxf/WGUYeeMZDY7Xzd6tDN96wENnKxWLrSl6ALIa4SQuwWQjwihLgxY/9HhBD3Rn/2CCFmlv9Wfzken15gZrHVdf/0QiPOz/tFTy6qFCJBLw6rHbFDT9Whew6hBBlFJINo8dQxQpeZolmRS9pRAwVX4hKy0KJ9jdLIwWeKYLfz9czQVzJyif7L2rJFi6WDJX8yhRAu8HHgpcB+4C4hxK1Sygf1MVLK3zeO/13g4uNwr78UB2fqDBa6f9zp+SbnbB06qnMOxw49ily8POTLhkM3+rBEkQsQxzMFET1gYpecX9qhmzXriLbjC05ITgTMt0AUCtBAPSB05JJunwsqwmmhYiHo4dBz7e9ZKWKHbssWLZY0/Tj0y4FHpJT7pJRN4Gbguh7HvwH4/HLc3HIxV29RbfjU/aDrMZPzja6TiLoxnI5cnJxqLasHRYOGEmPhxoOiQCKwmlikCyoTD40+5kEz26G7nnptOPRS1C632iSJUIKmEblkOXT9MFkqQ/c637MS2AzdYulKP4K+DXjS+H5/tK0DIcQpwE7g2132v1UIcbcQ4u7JycmjvdeePHRwjqs/+n0mq42OfQdnlEjVmomQLjR8rvv4D7nniSM0/IBq3e9Zc57FumhQdKiUS7oZFirtDt0rKGcsA3LaoYep6CeOPfKd+/VDIX2s4ylxMzolFlG/ESwGDk6ciTe7ly22XTs6vpWa7KQxXfmKDooaJZ4Wi6WN5R4UfT1wi5Qy0wpLKT8lpbxUSnnphg0blvXCD4zP8cD4HF+690DHvvEZNYGn4YeEoepb/uDBOe57coa7Hz/M4WjVobHK0Tn0558xxn+8+mwuPmkkaTlbqIBeKzRoKOFxlEMvuA6gGmi1YU4sgvbYxU85dC1kTq7DoRejqhkfF1dHKG2DohlC7HZx6L1q1ldD2aIdFLVYOujnp+IAcJLx/fZoWxavB972y97UsVBvqWfIbT8/yL+58tS2feOzSfOpuh8wkPd4+JBy0dMLTab1LFHt0O/5K5jak5zg4jfBxrPbL7jvnygWKvybK5+lvtfLtqUzdB25IMm7ApeQDtIuOWhC7Qj86M/g8D7YcmHnsa6nzmuIfz5qxuXj4uSNh4OMFt9I16FDe2MwfTys4sjFAYQVdIslg35+Ku4CzhBC7EQJ+euBN6YPEkKcBawD7ljWO+wTLej3PTnDL6YXOXl0IN6nHTqo2GUg77EnEvSp+QZTUR+X0XJB5du3/Z4SDK+g3HZrEV71kfYLfv3dMHISvCEaLgijHuL5MixOq226QiUSzbwHHhm/vKRdcmsRfnEnfP9DqkJm+6XJsW2Ri9Pm0E1B93LGw0FPGkq3z4XuGfpqjVwATnsxbHvWyt6DxbIKWTJykVL6wNuBrwMPAV+QUj4ghHi/EOJa49DXAzdLqe3gicXMx2/7+XjbPp2hA9Qi4d89ETn0+cShj5XzUVwi4VffC390ANbtbF+0QuPXOheacHPtlSq+HhRVgloQMlvQtUDqtTob89CYVa/fcS+8yKgU7YhcTIeuPqePi5svJffQrX2ueb4l69BXSXMugH/xf+GcXuPyFsszk76slpTyK8BXUtvek/r+vct3W0dPrRXgOYJTNwzy0yeOtO07YDj0eitESskeLegLjaSPS7kA9ei9WlzNQU4Tv9leMx60ID/QvtCzLmWMoo6Cm+HQhRsLfiLo1eSa6QWZTYcu3DaHngsTh57L6wy9mZy/Z+SSrnLpUbZoSwYtllXJmpkpWmsFlHIu5YKn+qUYjM/WGMgrEaq3AiarDWYWWzgCpqrKoRc8h8G82ymkhaFsQQ8a7YOXOnLxjJ7mukIlEsCckyHoptstRHXwWtCFq2aZmnSULSb34IXRoKh0yRWMuvJuU/+h/5miq6U5l8Vi6cqaEfR6K6CY7WyDOwAAHKpJREFUdynmXBpGvXkYSg7N1jl1wyCghF8PiF6wfYTphQaTVVWDLoQwBD0SV7Ou3MRvtHdGDH0leqZDj8sWowzdzYhcTHGMHfqcuo9CJXHXmrbIpT1Dd6Ml7XxcvLxZ5dKjbNE8n3C6Z+jme1c6crFYLJmsIUEPKeYcCp4TO/QglOx9ap5WIDl1rAyorF3HLc85bZRWIHlseiHp46LFe6nIJWi29y4PfCV6XiHl0JNB0YIryUWLSQdSD1QaQlkoR/cQOfRCxszVtkHRdocuIjFu4VIoaofeZx16+nyrtTmXxWLpypoR9FpTRS4Fz6XRUoL++393Ly//n98DYNemSNBbAXsn5hkr5+Nt6vsocogdejn5qvubm2Q5dCcXdUw0BkX1xCIg74AbOfQFIsF1Mxx6cz4S9HLndePOjJ0Zup4UFOCSy0dVPmYdemaGbkzUMc+XFv/V0svFYrF0Ze0IepShF3JOHLk8cXiRMzaW+R/XX8hLz9kMqGjm8GKTsXIhFvH5hjFLVIt3L4ceBsr1mg49jOrQ3bwS0DCMFqfItwm6jlxq6DVCDXHMpx16akAUaFv1PuXQ8ZPIpWg69J4ZutEbxTxfR+SyisoWLRZLJmtK0Is5ty1yabQCdo4N8tpLtqt+KyhBn6/7VIpeW3fF0Q6Hbgh6egBUvzYdeuAnZYt6n3boelDUleS0Q5eR4KarR/TEpG6C3pahu7GIA7FDb5mCvlT73I5MPrVwtXlvup7dZugWy6pkzQh6vRVQykeRixZ0P6SYUyJWir7WmgHVRotKMafqziPi11rQ80aVC7THLrpc0VzUQvdyMafvB+ZMUcgLGUcui3HkknK7ehC2q0PXkYtuKWC0EWgtqtvCpVA069D7aJ/rRnXtPRt5aTdvBd1iWY2sLUHXDj2aPFRvBRSihlha2GutkGrdp1zwWGc042obFM0NJEJrVp5otKB3lC167dP39fJxUeSScxKHvpgVuejr9RW5uJ2ZuB4UlS6lks7Ql2ifm65rT283id28rUO3WFYja0bQ48gll0Qu9WgbEAt7raU6K1aKHjnXiRepiOOXtJAWUqsQQXvkoifG6sjFXFhCO/RIAMs5wWUnq/MNlqPFMNJuty1yyahySc8UNYlmrga4DBRy6hi/z+ZcOpPX9KpZt5GLxbIqWTuC3gyjDN3FDyV+EMaljACOIyjmHCND10KuBHjUjFyWEvT0DFFIIpdMh64E3SHk3VedAcC5p6hB2g63W6hAbQZaC10cuhmRpN4bCXoLl1I+KqEM+ixbTJ8vS7RtL3KLZVWzZgTdjFxA5ed1P3HooHL02cUWzSCMB0n1YOiGcheHnjdKCTVm1KIHRs0qF31Mqg4dGSa9zvNqolOHcBYqUD2YvE7jdHHUEEcuPi4DOTdZdLqvXNxrF3QbuVgsTzvWhKBLKVXZYt6JBX2+4SMlHYL+VFWJnhZ0LeRxnt6oJuWD0MWhp/qVQzJTVDv05oL6ajTnIgySPFsLetrtFoageig6JqMO3TUzb+OfzyvGDt1xcniukyw6HWfovapcjAzd7C9jYiMXi2VVsyYEvRVIglBGdehKlGZr0bqdXvIRi3mXyahVrhb0k0cH2DZSIqeXh2vOt2fXWYOibTNEG6rmXIbRxCIt6NEDwIhckEES0eSiQcusQVEdkfSKXNKO2ivG7n+4ElW4xA6935mierHqLoJt1sBbLJZVx5r4ydRrhRaNyGVOC3raoc9Fgl5QovW7Lz6d33rujuRkjbk+MvRUTbqOURw3qUPXxxuDoipyieIP7b6zyhbj11lT/82yReO9uQGozwDwsTddrrZph94zQ884X7eM3ByQtVgsq461IehNU9DbHXrRdOg5N17MQjv0gbzHQN74a+jI0AcBkapyMR26kVG7hkNvZDj0MOjM0Dsil4y4xyQrIgHQS84BY0M6ny+0319m5GKIeLy8W5eM3Oz0aLFYVh1rInLRi1aYg6KxoKccerSkaFzl0oaUnYIuRGcL3bRD1zGK4xkOPRpEdZNeLipy0Q69R+SS9VrTNvXfjFyMNrtxS9x8H+1zzan/S0Uuufb3WCyWVcXaEvS8qkOHbEE3X2uH3oZfV242LaTpfi5tVS5mFUmWQzcil7CfyCUjvzdJzxTVGA49EeZCHxl6xvm6Cba7RCRjsVhWlLUh6E3Toacil1zyEUv5JQS92ypBhXL2TFFon7jjGnXo+nijOVdm2eJRO/RUu1sARPIgAWNZuXx7ht6r26J5vm6CbR5rsVhWHWtD0Fudg6LZkUvyccuFXoKeGowsVNp7uaTr0M3IxU0NipoOPbPKJaMOPeu1JitDT7v12HUXjOZcgsxForMy9G4ZeRy52Dp0i2U1siYEXfc/L+aczsjFa8/QAQbyrqrT7jiRXtwiVf+djlzaHHoqcvFSkYubHhTVdeg6cuki6LmBPmZ2GmKc1a9cL4cXBt1dtdlfvZ/Ixcll16hbLJYVZ00IeluGHgl4UrbYXocOXdw5dPZC1/QS9I4qlwyHbg6KxpGLdugp0dZCn+XOIRWROMZrsw+LmaFHDn2pyhVzkLVX5GLjFotl1bImfjrNDF1PEJqrKZE1Hbp+nZmfQ48MfYlB0cCsQ0879EIi4mFgDIr2mCmadQ+arOZc2jlDFMXovuWRQ5dhfwOdZsVLFunfBCwWy6pibQi6UbboOkrMeg2KZpYsQo8MPV222GVQ1Mly6HmIIiGkTMoWcz16uZhf05giLoyIJKsCxcsnDj1rQNQ83s21Z/JZpCczWSyWVcWa+Oms60HRvIsj2gU9PVMUejn01ALRmkJFTeUPQxVzpAdFQ2NQVPcpj6tcCskDIDNySd2LV1Ai21fkkuGuTTF2C0aGvlTk4vZR5eJaQbdYVjFr4qezOLuPlzl3MXBwGHnyc3EIObd2Dxc4iwzsC6BYhp0vopRzuUw8zPN9D6ZGYex01ar2iR8q93zgHnXCtJjqXLs5D8WhjEHRaKBTu2SvkHLo0W8JOnIRblJmmBZIIdT1s6b9Q+cSdPq6WRUo2qHLHoKeXoIu6540NnKxWFY1a0LQr3rgD3hT/nH4q48g33Evz3Ye5n+7/xVc4P9EB/3Wl1nfyvN/Cu+Hg8DNfwNvvwu+96dwx8eSkxVHVKMrEy3wWtD9RjLg2Fa2qOOLfHaViy5bdHNKhMuboLKl8wOtOwVGTsn+sOVNgIDK5vaqlMzIpZjMZO0m0pUtat/A6NJT+4e2qOtaLJZVyZoQ9II/z6wcZFgsIBpVRl3VIvf/DX+PD19/AXzxzVCfoyzV9PhqbpTKwqR688IkVLbCG/9OfV/Z3FmWl27QFTTUtsVG5NCNyAWUQ48aZXU25zJKCN9+V5Klm/zmbUkWn2b0NPjDfTCwHh74++i6XSKXQgWI2hl0y9B3PB/etVedTyxR5fLi/9S+jqrFYllVrAlBd8Mms6LMMAsQNBnwfJDwuHsKbDpXHRQ0KEVCVcuPUak9mvRuGRiFLRd0v0C8UHQk6H5TZeCLwpi4QyKm5qzNrOZcWnyLw12u1yU/1wysV1/b+rpkRC76PPWZ7ElFoB5eWefLwiskVTwWi2XV0VcduhDiKiHEbiHEI0KIG7sc8zohxINCiAeEEDct7232xg1bLIpokNFvMOCoTFt4RWMFoSYFodxls7BeibBf774Ys0m6J3oQRS66PW2QWhHIM9x1W3OuMIlclgMzIslqbatXW6rN9DeYabspWixPa5b8yRVCuMDHgZcC+4G7hBC3SikfNI45A/gPwPOklEeEEBuP1w1n4ckmdXcAAiBoMOAGEIDj5Y01PhsUI9EPSpEjbcwrQS8vcbvpyMVvqvPGza9SkUu6r4pjOnR/+SpFzIgky13r+64d6R7htJ1viUFRi8WyqunHoV8OPCKl3CelbAI3A9eljnkL8HEp5REAKeVTy3ubPQhDXALqTpRF+00GHOWYnVwxEVe/wYbIxFfWRwN7jbmjdOhGhu7mjfa0qchFO3S9/Fxbcy6/e0Z9tGRFLh0ZOlHkchQO3XZTtFielvQj6NuAJ43v90fbTHYBu4QQPxRC/FgIcVXWiYQQbxVC3C2EuHtycvLY7jhN1Ju84Q7G35d05JIrJuIaNBmMtq/fEFWWNKrHJuh+o92hpyMX/RDRvx2km3MtV6TRVraY0QkxFvS5/hpqLdUP3WKxrGqWq5eLB5wBvAh4A/BpIcRI+iAp5aeklJdKKS/dsGHD8lw5muTT8hKHXoocupvLtzn0uH58YEx97VfQdR267vUSNLMdejpD1zGHSNWhL1ekkdWcK0vQkX0K+hJT/y0Wy6qmH0E/AJxkfL892mayH7hVStn6/9s7/1i3zrOOfx7b1743adI0abZmaZMmJS2aEEtDNIJoK2CjayqWMCahtLCtAlQhqKCqALWqVE37A2kgEEKbGAMqumnQbrCyIAW1AyYmTWw0LUnb9GeWFS1d2qbptjTJvf51H/5432O/PvccX/vGx8fHez6S5ePXJ8dPXvt+/fXzPu/7qup3gJdwAp89XqSbFS+67TqzEgl6MCjabnRneK72gj7/fWjNp0/iiahUXU13Z1A0JYcer3KJHHpYhz7KlEs49T+xbHHt0nMHup45dMMoIoMI+hPADhHZJiJV4ABwMHbOv+DcOSJyOS4Fc2KEcabjRbrVcehO0OtacWuhl0pOoFr17tZxkUN/+5S7X86hR+eEg6KRQ+9ZnCuoQ4ful8mSQdEROeDEpXRDh37J0nP7Xs9SLoZRZJYVdFVtAXcBjwHPA19U1WMi8gkR2edPeww4IyLPAV8D/lBVz2QVdA/eoS9GJXrtBjVp0WCmu7lFxTvpaHPnyKGf9T80hhX0dpBD79mzM5gpGr0uBA5dR1y2GFWllJNTLpXa0i+Vvtdbpg7dMIyJZqC/XFU9BByKtT0QHCtwj7+NF+/QJRLlVp2atGhS7q60WI479A3u/uz33P2KHHqt+0XRSbmkOfTYeugjq3JZZnGuKO4LZ4ascjFBN4wiUvgNLlpNN82/I8rtBtW4Q4/WXYkc+uylzjVHgl6N7VCURLiEbrvu0i3lanetFAhENe7Qw0HRPrsHDUvPFnQpYhz1iwzwVos5dMMoMoUX9IX5eQAqs6sAgVadKi0aUQ4duluxtetOtEplJ3Qdh77MoCg40Q/LFste0MOp/5Hz7jj0hEHRkZYtBlUpSSkX6Ar6UDNFLYduGEWk+IK+cAGAmdqcT4HUqdJ0Dr0SpVwih17vim1t7fCDoo3IoYdli42lZYsdhx7LX2c2sahfymVt77l9r2czRQ2jyBRe0OsLzqFXa3OdDR2quJRLLRwUbTW6QgxOoFuxdE0/enLo4cSiaEegUlcQy8FMUUhfnOtiKSWULcavHaWTLIduGFNP8QW97kR5dna2s6HDDE0aVKhVAoFd4tADER9G0BfbLnVSrgWpnNhAZyfl4tt6FudqjS7lEubQkxbniuIOYxj0eoZhFI7CC3qjvtShz2iTerxsMZopWk4Q9IEGRde4fx9NLqpUex166Grjg6JZLc6V6NBjqZVODt3KFg1j2pkCQXcOfW5uVcehV7TZOyharnZnilaClAu4JWbT1goPiXLR5315fWf5XJ9DD133kkFRAcTn0LMoW0xZnAtWNihqgm4YhaTwgt5q+JTLXOTQ61S04csW/X+vn0MfJN0SnnfhTX/NoMolnnKJD4qCc7/a9imXEU/9j7a0g4SUy9rec/teL6jbNwyjcBRe0Jte0Odm57xDbzqHToXZSujQm34NlphDrw2QbgnPvxBz6G0/2BqfoRmdEyGlDKb+D5NyMYduGNNO4QW97QV91arVnZx2ebGZMPW/3t3cGVbg0L3wR4IeTqtvLfS62s7iXIFDl3IGM0WXWQ8dAkEf4K0OyyANwygcUyDobjq/c+gup13WBg2tUJsJ6tCjssUlDn3IlMt5n3IpB7shNS7EHHqsbBGcWC76OvSRb0HXr8plBWWLtgWdYRSS4gt60wl6KVoqt12nKm12bN7Ae670S7JXqiNw6D4X3ePQI0E/F6tySXPovmxx5FvQVdIrVDpli8Msn2uCbhhFpPB/uYvNBVqUqZRKHYdeatfZufUdENahdwZF4w59gGn/4fk9Dt1fq3mh13UnOXSRIOUy6qn/YcolLuhre8/tez1LuRhGkSm8Q9d2nSbBsrXRIlyhO+5XtrjiKpfQoZ/vHYyMb3ABPuUy6jr0YKr+cotzWR26YUw9hRd0WnWaEiyKFS2TG7rjpPbIuQ4q6DOrXKVKx6HXul8OjXMpM0VjKZfFpku7jDWHPoygWw7dMIrMFAh6g5YEDr0570QzdMflmkt3NOdX7tBF3Lnnwzp0/xr1c7Eql9hMUXCCGi3fO/Ic+jLroYfnDno9wzAKR/EFvV2nXQoENFpAK3THkYjXz3VFeO4yf79+8NeauwzOnnTHM6uh6re9m3/L7Tka0ZmFGtS4S7m7wcaoBL26qns/44+j+4gZX84Zb1/ueoZhFI7C/7aWdoPFcHPmllvbZYlDB/dc1L7mCvj1L8OWnxn8xT7013DqqPsS2HANrNsC+z7lBkW3/mz3vA3XwO1fhO0/HwRaggW/Dkz0RXCxrN8Ot38Jtv+c+9L6yKNw1Z7ec0ol+OhX4PJrl7/e1hvgtodh087RxGcYxlgpvKCXFhssziRMtU9y6PH2H3vfcC+2ZY+7hdfd9ZHkc6/9QCzQEiz8wB0PWlkzCNfe3D2+5heSz9k64JdWqQTX7b34mAzDyIXCp1zKi000Srn0iHiCQ4+3jxMpw8IP3fGgeXvDMIwhKLSgLy4q5cUGWk4Q9DQRD88ZJ6UyzEcO3QTdMIzRU2hBP99oUZVWN6USCndq+iUvhx6mXEzQDcMYPYUW9AuNNlWayasbTppDl3J371ETdMMwMqDQgn6u3mKGFpK0dsqkOfRwtcNRDooahmF4Ci3o5+stqrQozQzj0HMcFI0wh24YRgYUXNDb1KTZFfQ0V95znNMsSAnWXcnrV4JhGFNNwQXdOfTyjJ+lWZ7gQdFoLZXaGr/HqGEYxmgprKB//3yDE2+eo0qTctUL+iC153mnXCzdYhhGRgwk6CJyi4i8KCLHReTehOfvEJHTInLE335r9KF2abQWufkvvs4fH3qBKi2qtcihDzIommMdOtiAqGEYmbHs1H8RKQOfBn4ROAk8ISIHVfW52KmPqOpdGcS4hG8cf5PTb9e5+/07mPtGC5nzi0lNtEP3353m0A3DyIhBHPp7geOqekJVG8DDwP5sw+rPwaPf49K5GX7npm2ItrsiPdDU/7zq0E3QDcPIlkEEfTPw3eDxSd8W58Mi8rSI/JOIXJV0IRG5U0QOi8jh06dPryBcWGi2efzYa+z9iSvcpCJInimamn6ZgEFRwzCMDBjVoOi/Aler6k8CXwUeSjpJVT+rqrtVdffGjRtX9EL/+cIbnG+0+eB73tVdX7ycUIc+iYtzgQm6YRiZMYigvwqEjvtK39ZBVc+oqldX/hb4qdGEt5Rme5Hrt6xjz/YN3R2AOg59kDr0nAdFw00vDMMwRsgg66E/AewQkW04IT8A3B6eICKbVPWUf7gPeH6kUQbs37mZ/Tt9xifNoZervbXepZLbVm2xme/iXGBVLoZhZMaygq6qLRG5C3gMKAMPquoxEfkEcFhVDwK/JyL7gBbwFnBHhjF36Tj02EzRpDx5pQaNZo6Lc9mgqGEY2TLQjkWqegg4FGt7IDi+D7hvtKENQMehB5tEQ3IlS9LGzePEBkUNw8iYws4UBaCVlnJJceiQ7/K5YIJuGEZmFFvQ27FB0XLFpTaSFuAqzzhRLZWXPjcOLOViGEbGFFvQ4w49Ok5Kq6S1jwub+m8YRsYUW9DbsUFRcG49LeWSV7oFLOViGEbmTIegxzeHThsUzdWhRykXq0M3DCMbii3oUcqlEpsJmurQcxR0y6EbhpExxRb0RIde7ePQJyDlYjNFDcPIiIHq0CeKpz4P//0pdzz/A3ff49BnUxx6Svu4KFVgZnV+VTaGYUw9xRP0Veth43Xdx5dcAWve1X184z0wt27pv/vpO7tfAHmw8zZ4x4/n9/qGYUw9oqq5vPDu3bv18OHDuby2YRhGURGRJ1V1d9Jzxc6hG4ZhGB1M0A3DMKYEE3TDMIwpwQTdMAxjSjBBNwzDmBJM0A3DMKYEE3TDMIwpwQTdMAxjSshtYpGInAb+b4X//HLgzRGGM0omNTaLazgsruGZ1NimLa6tqrox6YncBP1iEJHDaTOl8mZSY7O4hsPiGp5Jje1HKS5LuRiGYUwJJuiGYRhTQlEF/bN5B9CHSY3N4hoOi2t4JjW2H5m4CplDNwzDMJZSVIduGIZhxDBBNwzDmBIKJ+gicouIvCgix0Xk3hzjuEpEviYiz4nIMRH5fd/+cRF5VUSO+NutOcT2iog841//sG9bLyJfFZGX/f1lY47puqBPjojIWRG5O6/+EpEHReQNEXk2aEvsI3H8pf/MPS0iu8Yc15+KyAv+tR8VkXW+/WoRmQ/67jNjjiv1vROR+3x/vSgiH8gqrj6xPRLE9YqIHPHtY+mzPvqQ7WdMVQtzA8rAt4HtQBU4Crw7p1g2Abv88RrgJeDdwMeBP8i5n14BLo+1/Qlwrz++F/hkzu/ja8DWvPoLuAnYBTy7XB8BtwL/BgiwB/jWmOO6Gaj4408GcV0dnpdDfyW+d/7v4ChQA7b5v9nyOGOLPf9nwAPj7LM++pDpZ6xoDv29wHFVPaGqDeBhYH8egajqKVV9yh+/DTwPbM4jlgHZDzzkjx8CfjnHWN4HfFtVVzpT+KJR1a8Db8Wa0/poP/A5dXwTWCcim8YVl6o+rqot//CbwJVZvPawcfVhP/CwqtZV9TvAcdzf7thjExEBfhX4x6xePyWmNH3I9DNWNEHfDHw3eHySCRBREbkauB74lm+6y/9senDcqQ2PAo+LyJMicqdve6eqnvLHrwHvzCGuiAP0/oHl3V8RaX00SZ+738A5uYhtIvK/IvJfInJjDvEkvXeT1F83Aq+r6stB21j7LKYPmX7GiiboE4eIXAL8M3C3qp4F/gq4BtgJnML93Bs3N6jqLmAv8LsiclP4pLrfeLnUq4pIFdgHfMk3TUJ/LSHPPkpDRO4HWsAXfNMpYIuqXg/cA/yDiKwdY0gT+d7FuI1e8zDWPkvQhw5ZfMaKJuivAlcFj6/0bbkgIjO4N+sLqvplAFV9XVXbqroI/A0Z/tRMQ1Vf9fdvAI/6GF6PfsL5+zfGHZdnL/CUqr7uY8y9vwLS+ij3z52I3AH8EvBrXgjwKY0z/vhJXK762nHF1Oe9y72/AESkAvwK8EjUNs4+S9IHMv6MFU3QnwB2iMg27/QOAAfzCMTn5v4OeF5V/zxoD/NeHwKejf/bjONaLSJromPcgNqzuH76mD/tY8BXxhlXQI9jyru/YqT10UHgo74SYQ/ww+Bnc+aIyC3AHwH7VPVC0L5RRMr+eDuwAzgxxrjS3ruDwAERqYnINh/X/4wrroD3Ay+o6smoYVx9lqYPZP0Zy3q0d9Q33GjwS7hv1vtzjOMG3M+lp4Ej/nYr8HngGd9+ENg05ri24yoMjgLHoj4CNgD/AbwM/DuwPoc+Ww2cAS4N2nLpL9yXyimgictX/mZaH+EqDz7tP3PPALvHHNdxXH41+px9xp/7Yf8eHwGeAj445rhS3zvgft9fLwJ7x/1e+va/B347du5Y+qyPPmT6GbOp/4ZhGFNC0VIuhmEYRgom6IZhGFOCCbphGMaUYIJuGIYxJZigG4ZhTAkm6IZhGFOCCbphGMaU8P+/gIEg6QQuoQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 2s 114ms/step - loss: 0.3571 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.4162 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.3894 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h-NycpNu8OG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmJlt1IJro0L",
        "outputId": "ea9105b1-8520-44ec-ddd5-a713e92210d5"
      },
      "source": [
        "train_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.34151899814605713, 1.0],\n",
              " [0.3556908369064331, 1.0],\n",
              " [0.36260902881622314, 1.0],\n",
              " [0.3238179683685303, 1.0],\n",
              " [0.4550715982913971, 1.0],\n",
              " [0.32176896929740906, 1.0],\n",
              " [0.34186291694641113, 1.0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVEPB4XHLzU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgLtG6mjrqgo",
        "outputId": "74a9113b-1bbc-4670-b818-e93297eefcd3"
      },
      "source": [
        "test_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.6650466918945312, 0.9200000166893005],\n",
              " [0.6667777299880981, 0.8399999737739563],\n",
              " [0.6932337880134583, 0.800000011920929],\n",
              " [0.6768825054168701, 0.800000011920929],\n",
              " [0.9106686115264893, 0.7599999904632568],\n",
              " [0.8469778299331665, 0.8399999737739563],\n",
              " [0.6377021670341492, 0.8799999952316284]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35wg8d3jrs-f",
        "outputId": "72099307-6772-44a6-acd8-acf1c5e3f3dc"
      },
      "source": [
        "val_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.5196131467819214, 1.0],\n",
              " [0.4384850859642029, 1.0],\n",
              " [0.6274903416633606, 0.7857142686843872],\n",
              " [0.391350120306015, 1.0],\n",
              " [0.6051260828971863, 1.0],\n",
              " [0.5397868752479553, 0.8461538553237915],\n",
              " [0.44713324308395386, 0.9230769276618958]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KeWBKywEp6Ub",
        "outputId": "c02b0f88-13b9-405e-cb36-676bce9c4ec1"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(7):\n",
        "  y_test_pred=load_model(f'./mod{i}.h5')\n",
        "  y_test_pred=y_test_pred.predict(x_test)\n",
        "\n",
        "  y_pred=y_test_pred.flatten()\n",
        "\n",
        "  y_test_pred=np.where(y_pred<0.5, 0,1)\n",
        "\n",
        "  c_matrix=confusion_matrix(y_test,y_test_pred)\n",
        "  ax=sns.heatmap(c_matrix,annot=True, xticklabels=['No ADHD','ADHD'],yticklabels=['No ADHD','ADHD'],cbar=False,cmap='Blues')\n",
        "  ax.set_xlabel('Prediction')\n",
        "  ax.set_ylabel('Actual')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}